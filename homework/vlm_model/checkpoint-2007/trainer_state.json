{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.07003341155533843,
  "eval_steps": 500,
  "global_step": 2007,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 3.489457476598826e-05,
      "grad_norm": 28.450647354125977,
      "learning_rate": 0.0005,
      "loss": 7.5423,
      "step": 1
    },
    {
      "epoch": 6.978914953197651e-05,
      "grad_norm": 14.481511116027832,
      "learning_rate": 0.0004997508719481814,
      "loss": 6.0039,
      "step": 2
    },
    {
      "epoch": 0.00010468372429796477,
      "grad_norm": 11.351561546325684,
      "learning_rate": 0.0004995017438963628,
      "loss": 4.7707,
      "step": 3
    },
    {
      "epoch": 0.00013957829906395303,
      "grad_norm": 13.177013397216797,
      "learning_rate": 0.0004992526158445441,
      "loss": 4.3014,
      "step": 4
    },
    {
      "epoch": 0.0001744728738299413,
      "grad_norm": 10.996316909790039,
      "learning_rate": 0.0004990034877927255,
      "loss": 2.2629,
      "step": 5
    },
    {
      "epoch": 0.00020936744859592954,
      "grad_norm": 7.819504261016846,
      "learning_rate": 0.0004987543597409068,
      "loss": 2.71,
      "step": 6
    },
    {
      "epoch": 0.0002442620233619178,
      "grad_norm": 6.326552867889404,
      "learning_rate": 0.0004985052316890882,
      "loss": 1.9717,
      "step": 7
    },
    {
      "epoch": 0.00027915659812790606,
      "grad_norm": 6.132354736328125,
      "learning_rate": 0.0004982561036372695,
      "loss": 1.9766,
      "step": 8
    },
    {
      "epoch": 0.00031405117289389433,
      "grad_norm": 5.814479827880859,
      "learning_rate": 0.0004980069755854509,
      "loss": 2.057,
      "step": 9
    },
    {
      "epoch": 0.0003489457476598826,
      "grad_norm": 7.771047592163086,
      "learning_rate": 0.0004977578475336323,
      "loss": 2.1422,
      "step": 10
    },
    {
      "epoch": 0.0003838403224258708,
      "grad_norm": 6.143257141113281,
      "learning_rate": 0.0004975087194818137,
      "loss": 1.5464,
      "step": 11
    },
    {
      "epoch": 0.0004187348971918591,
      "grad_norm": 10.983332633972168,
      "learning_rate": 0.000497259591429995,
      "loss": 2.1703,
      "step": 12
    },
    {
      "epoch": 0.00045362947195784736,
      "grad_norm": 15.223211288452148,
      "learning_rate": 0.0004970104633781764,
      "loss": 2.1712,
      "step": 13
    },
    {
      "epoch": 0.0004885240467238356,
      "grad_norm": 9.888079643249512,
      "learning_rate": 0.0004967613353263578,
      "loss": 1.3955,
      "step": 14
    },
    {
      "epoch": 0.0005234186214898239,
      "grad_norm": 12.444092750549316,
      "learning_rate": 0.0004965122072745391,
      "loss": 2.6439,
      "step": 15
    },
    {
      "epoch": 0.0005583131962558121,
      "grad_norm": 11.364310264587402,
      "learning_rate": 0.0004962630792227205,
      "loss": 1.4854,
      "step": 16
    },
    {
      "epoch": 0.0005932077710218003,
      "grad_norm": 7.733478546142578,
      "learning_rate": 0.0004960139511709018,
      "loss": 0.4252,
      "step": 17
    },
    {
      "epoch": 0.0006281023457877887,
      "grad_norm": 6.005594730377197,
      "learning_rate": 0.0004957648231190832,
      "loss": 0.6658,
      "step": 18
    },
    {
      "epoch": 0.0006629969205537769,
      "grad_norm": 10.808453559875488,
      "learning_rate": 0.0004955156950672646,
      "loss": 1.4837,
      "step": 19
    },
    {
      "epoch": 0.0006978914953197652,
      "grad_norm": 7.639589786529541,
      "learning_rate": 0.000495266567015446,
      "loss": 2.1225,
      "step": 20
    },
    {
      "epoch": 0.0007327860700857534,
      "grad_norm": 7.911014080047607,
      "learning_rate": 0.0004950174389636273,
      "loss": 2.1809,
      "step": 21
    },
    {
      "epoch": 0.0007676806448517416,
      "grad_norm": 6.150979995727539,
      "learning_rate": 0.0004947683109118087,
      "loss": 1.397,
      "step": 22
    },
    {
      "epoch": 0.00080257521961773,
      "grad_norm": 5.664465427398682,
      "learning_rate": 0.0004945191828599901,
      "loss": 0.8365,
      "step": 23
    },
    {
      "epoch": 0.0008374697943837182,
      "grad_norm": 4.55019474029541,
      "learning_rate": 0.0004942700548081713,
      "loss": 1.2046,
      "step": 24
    },
    {
      "epoch": 0.0008723643691497065,
      "grad_norm": 5.059478282928467,
      "learning_rate": 0.0004940209267563528,
      "loss": 1.0263,
      "step": 25
    },
    {
      "epoch": 0.0009072589439156947,
      "grad_norm": 4.235340118408203,
      "learning_rate": 0.0004937717987045341,
      "loss": 1.3425,
      "step": 26
    },
    {
      "epoch": 0.0009421535186816829,
      "grad_norm": 5.211792469024658,
      "learning_rate": 0.0004935226706527155,
      "loss": 1.9797,
      "step": 27
    },
    {
      "epoch": 0.0009770480934476712,
      "grad_norm": 5.027712345123291,
      "learning_rate": 0.0004932735426008968,
      "loss": 1.2239,
      "step": 28
    },
    {
      "epoch": 0.0010119426682136596,
      "grad_norm": 4.795842170715332,
      "learning_rate": 0.0004930244145490782,
      "loss": 0.9827,
      "step": 29
    },
    {
      "epoch": 0.0010468372429796478,
      "grad_norm": 5.1328511238098145,
      "learning_rate": 0.0004927752864972596,
      "loss": 0.9831,
      "step": 30
    },
    {
      "epoch": 0.001081731817745636,
      "grad_norm": 3.9476873874664307,
      "learning_rate": 0.000492526158445441,
      "loss": 1.2474,
      "step": 31
    },
    {
      "epoch": 0.0011166263925116242,
      "grad_norm": 4.110507965087891,
      "learning_rate": 0.0004922770303936224,
      "loss": 0.822,
      "step": 32
    },
    {
      "epoch": 0.0011515209672776125,
      "grad_norm": 4.482431411743164,
      "learning_rate": 0.0004920279023418037,
      "loss": 1.6808,
      "step": 33
    },
    {
      "epoch": 0.0011864155420436007,
      "grad_norm": 4.665486812591553,
      "learning_rate": 0.0004917787742899851,
      "loss": 1.786,
      "step": 34
    },
    {
      "epoch": 0.001221310116809589,
      "grad_norm": 7.49329948425293,
      "learning_rate": 0.0004915296462381664,
      "loss": 1.5139,
      "step": 35
    },
    {
      "epoch": 0.0012562046915755773,
      "grad_norm": 4.326588153839111,
      "learning_rate": 0.0004912805181863478,
      "loss": 0.7662,
      "step": 36
    },
    {
      "epoch": 0.0012910992663415655,
      "grad_norm": 5.138260841369629,
      "learning_rate": 0.0004910313901345291,
      "loss": 0.8414,
      "step": 37
    },
    {
      "epoch": 0.0013259938411075538,
      "grad_norm": 5.883663654327393,
      "learning_rate": 0.0004907822620827105,
      "loss": 1.2155,
      "step": 38
    },
    {
      "epoch": 0.001360888415873542,
      "grad_norm": 12.173378944396973,
      "learning_rate": 0.0004905331340308919,
      "loss": 1.5621,
      "step": 39
    },
    {
      "epoch": 0.0013957829906395304,
      "grad_norm": 4.939220428466797,
      "learning_rate": 0.0004902840059790733,
      "loss": 1.0858,
      "step": 40
    },
    {
      "epoch": 0.0014306775654055186,
      "grad_norm": 8.680706024169922,
      "learning_rate": 0.0004900348779272546,
      "loss": 0.9004,
      "step": 41
    },
    {
      "epoch": 0.0014655721401715068,
      "grad_norm": 4.991660118103027,
      "learning_rate": 0.000489785749875436,
      "loss": 0.7759,
      "step": 42
    },
    {
      "epoch": 0.001500466714937495,
      "grad_norm": 8.443506240844727,
      "learning_rate": 0.0004895366218236174,
      "loss": 1.0251,
      "step": 43
    },
    {
      "epoch": 0.0015353612897034833,
      "grad_norm": 4.165907382965088,
      "learning_rate": 0.0004892874937717987,
      "loss": 0.7349,
      "step": 44
    },
    {
      "epoch": 0.0015702558644694717,
      "grad_norm": 4.9132843017578125,
      "learning_rate": 0.00048903836571998,
      "loss": 0.7732,
      "step": 45
    },
    {
      "epoch": 0.00160515043923546,
      "grad_norm": 7.4204792976379395,
      "learning_rate": 0.0004887892376681615,
      "loss": 0.9826,
      "step": 46
    },
    {
      "epoch": 0.0016400450140014481,
      "grad_norm": 4.165923595428467,
      "learning_rate": 0.0004885401096163428,
      "loss": 1.1153,
      "step": 47
    },
    {
      "epoch": 0.0016749395887674364,
      "grad_norm": 2.6413283348083496,
      "learning_rate": 0.0004882909815645242,
      "loss": 0.5764,
      "step": 48
    },
    {
      "epoch": 0.0017098341635334246,
      "grad_norm": 2.4502172470092773,
      "learning_rate": 0.00048804185351270554,
      "loss": 0.422,
      "step": 49
    },
    {
      "epoch": 0.001744728738299413,
      "grad_norm": 5.019986629486084,
      "learning_rate": 0.0004877927254608869,
      "loss": 1.2111,
      "step": 50
    },
    {
      "epoch": 0.0017796233130654012,
      "grad_norm": 4.1562018394470215,
      "learning_rate": 0.0004875435974090683,
      "loss": 1.0323,
      "step": 51
    },
    {
      "epoch": 0.0018145178878313894,
      "grad_norm": 2.757084369659424,
      "learning_rate": 0.0004872944693572496,
      "loss": 0.4224,
      "step": 52
    },
    {
      "epoch": 0.0018494124625973777,
      "grad_norm": 3.9546468257904053,
      "learning_rate": 0.000487045341305431,
      "loss": 0.6465,
      "step": 53
    },
    {
      "epoch": 0.0018843070373633659,
      "grad_norm": 8.379056930541992,
      "learning_rate": 0.00048679621325361237,
      "loss": 1.4443,
      "step": 54
    },
    {
      "epoch": 0.001919201612129354,
      "grad_norm": 7.732920169830322,
      "learning_rate": 0.00048654708520179374,
      "loss": 1.1115,
      "step": 55
    },
    {
      "epoch": 0.0019540961868953423,
      "grad_norm": 5.383084297180176,
      "learning_rate": 0.0004862979571499751,
      "loss": 0.9715,
      "step": 56
    },
    {
      "epoch": 0.0019889907616613307,
      "grad_norm": 4.995002746582031,
      "learning_rate": 0.00048604882909815644,
      "loss": 0.9887,
      "step": 57
    },
    {
      "epoch": 0.002023885336427319,
      "grad_norm": 4.2283406257629395,
      "learning_rate": 0.0004857997010463378,
      "loss": 0.897,
      "step": 58
    },
    {
      "epoch": 0.002058779911193307,
      "grad_norm": 4.6674275398254395,
      "learning_rate": 0.0004855505729945192,
      "loss": 0.8092,
      "step": 59
    },
    {
      "epoch": 0.0020936744859592956,
      "grad_norm": 4.466358184814453,
      "learning_rate": 0.00048530144494270057,
      "loss": 0.9344,
      "step": 60
    },
    {
      "epoch": 0.0021285690607252836,
      "grad_norm": 4.352532863616943,
      "learning_rate": 0.0004850523168908819,
      "loss": 0.7167,
      "step": 61
    },
    {
      "epoch": 0.002163463635491272,
      "grad_norm": 3.6744449138641357,
      "learning_rate": 0.00048480318883906327,
      "loss": 0.7088,
      "step": 62
    },
    {
      "epoch": 0.00219835821025726,
      "grad_norm": 5.159687519073486,
      "learning_rate": 0.0004845540607872447,
      "loss": 0.9396,
      "step": 63
    },
    {
      "epoch": 0.0022332527850232485,
      "grad_norm": 4.992852210998535,
      "learning_rate": 0.000484304932735426,
      "loss": 0.617,
      "step": 64
    },
    {
      "epoch": 0.002268147359789237,
      "grad_norm": 4.158852577209473,
      "learning_rate": 0.0004840558046836074,
      "loss": 0.5255,
      "step": 65
    },
    {
      "epoch": 0.002303041934555225,
      "grad_norm": 6.507327079772949,
      "learning_rate": 0.0004838066766317887,
      "loss": 0.7489,
      "step": 66
    },
    {
      "epoch": 0.0023379365093212133,
      "grad_norm": 5.984500885009766,
      "learning_rate": 0.00048355754857997015,
      "loss": 0.4807,
      "step": 67
    },
    {
      "epoch": 0.0023728310840872013,
      "grad_norm": 4.616611003875732,
      "learning_rate": 0.0004833084205281515,
      "loss": 0.7151,
      "step": 68
    },
    {
      "epoch": 0.0024077256588531898,
      "grad_norm": 5.785494804382324,
      "learning_rate": 0.00048305929247633285,
      "loss": 0.5012,
      "step": 69
    },
    {
      "epoch": 0.002442620233619178,
      "grad_norm": 5.023387432098389,
      "learning_rate": 0.00048281016442451417,
      "loss": 0.6241,
      "step": 70
    },
    {
      "epoch": 0.002477514808385166,
      "grad_norm": 4.399099826812744,
      "learning_rate": 0.00048256103637269555,
      "loss": 0.4757,
      "step": 71
    },
    {
      "epoch": 0.0025124093831511546,
      "grad_norm": 5.465885162353516,
      "learning_rate": 0.000482311908320877,
      "loss": 0.4015,
      "step": 72
    },
    {
      "epoch": 0.0025473039579171426,
      "grad_norm": 6.670832633972168,
      "learning_rate": 0.0004820627802690583,
      "loss": 0.644,
      "step": 73
    },
    {
      "epoch": 0.002582198532683131,
      "grad_norm": 8.91059398651123,
      "learning_rate": 0.0004818136522172397,
      "loss": 0.8488,
      "step": 74
    },
    {
      "epoch": 0.0026170931074491195,
      "grad_norm": 4.830768585205078,
      "learning_rate": 0.000481564524165421,
      "loss": 0.4202,
      "step": 75
    },
    {
      "epoch": 0.0026519876822151075,
      "grad_norm": 6.572054862976074,
      "learning_rate": 0.00048131539611360243,
      "loss": 0.9529,
      "step": 76
    },
    {
      "epoch": 0.002686882256981096,
      "grad_norm": 6.0848894119262695,
      "learning_rate": 0.00048106626806178375,
      "loss": 0.6569,
      "step": 77
    },
    {
      "epoch": 0.002721776831747084,
      "grad_norm": 4.388816833496094,
      "learning_rate": 0.00048081714000996513,
      "loss": 0.4761,
      "step": 78
    },
    {
      "epoch": 0.0027566714065130724,
      "grad_norm": 5.99564790725708,
      "learning_rate": 0.00048056801195814645,
      "loss": 0.3909,
      "step": 79
    },
    {
      "epoch": 0.002791565981279061,
      "grad_norm": 4.115144729614258,
      "learning_rate": 0.0004803188839063279,
      "loss": 0.6504,
      "step": 80
    },
    {
      "epoch": 0.002826460556045049,
      "grad_norm": 3.6055164337158203,
      "learning_rate": 0.00048006975585450926,
      "loss": 0.3857,
      "step": 81
    },
    {
      "epoch": 0.0028613551308110372,
      "grad_norm": 5.779383182525635,
      "learning_rate": 0.0004798206278026906,
      "loss": 0.4023,
      "step": 82
    },
    {
      "epoch": 0.0028962497055770252,
      "grad_norm": 5.454442024230957,
      "learning_rate": 0.00047957149975087196,
      "loss": 0.4484,
      "step": 83
    },
    {
      "epoch": 0.0029311442803430137,
      "grad_norm": 5.149089813232422,
      "learning_rate": 0.00047932237169905333,
      "loss": 0.792,
      "step": 84
    },
    {
      "epoch": 0.002966038855109002,
      "grad_norm": 3.4856338500976562,
      "learning_rate": 0.0004790732436472347,
      "loss": 0.3031,
      "step": 85
    },
    {
      "epoch": 0.00300093342987499,
      "grad_norm": 3.9765641689300537,
      "learning_rate": 0.00047882411559541603,
      "loss": 0.4046,
      "step": 86
    },
    {
      "epoch": 0.0030358280046409785,
      "grad_norm": 4.554452419281006,
      "learning_rate": 0.0004785749875435974,
      "loss": 0.9452,
      "step": 87
    },
    {
      "epoch": 0.0030707225794069665,
      "grad_norm": 3.302476167678833,
      "learning_rate": 0.00047832585949177873,
      "loss": 0.6332,
      "step": 88
    },
    {
      "epoch": 0.003105617154172955,
      "grad_norm": 4.558747291564941,
      "learning_rate": 0.00047807673143996016,
      "loss": 0.4767,
      "step": 89
    },
    {
      "epoch": 0.0031405117289389434,
      "grad_norm": 4.212779998779297,
      "learning_rate": 0.00047782760338814154,
      "loss": 0.599,
      "step": 90
    },
    {
      "epoch": 0.0031754063037049314,
      "grad_norm": 3.166602611541748,
      "learning_rate": 0.00047757847533632286,
      "loss": 0.4415,
      "step": 91
    },
    {
      "epoch": 0.00321030087847092,
      "grad_norm": 5.626041412353516,
      "learning_rate": 0.00047732934728450424,
      "loss": 0.709,
      "step": 92
    },
    {
      "epoch": 0.003245195453236908,
      "grad_norm": 2.4112765789031982,
      "learning_rate": 0.0004770802192326856,
      "loss": 0.4478,
      "step": 93
    },
    {
      "epoch": 0.0032800900280028963,
      "grad_norm": 3.784104347229004,
      "learning_rate": 0.000476831091180867,
      "loss": 0.6875,
      "step": 94
    },
    {
      "epoch": 0.0033149846027688847,
      "grad_norm": 6.486544132232666,
      "learning_rate": 0.0004765819631290483,
      "loss": 0.5979,
      "step": 95
    },
    {
      "epoch": 0.0033498791775348727,
      "grad_norm": 3.9533932209014893,
      "learning_rate": 0.0004763328350772297,
      "loss": 0.4002,
      "step": 96
    },
    {
      "epoch": 0.003384773752300861,
      "grad_norm": 5.128205299377441,
      "learning_rate": 0.0004760837070254111,
      "loss": 0.6641,
      "step": 97
    },
    {
      "epoch": 0.003419668327066849,
      "grad_norm": 4.542031764984131,
      "learning_rate": 0.00047583457897359244,
      "loss": 0.6843,
      "step": 98
    },
    {
      "epoch": 0.0034545629018328376,
      "grad_norm": 4.739355087280273,
      "learning_rate": 0.0004755854509217738,
      "loss": 0.712,
      "step": 99
    },
    {
      "epoch": 0.003489457476598826,
      "grad_norm": 4.328557968139648,
      "learning_rate": 0.00047533632286995514,
      "loss": 0.3449,
      "step": 100
    },
    {
      "epoch": 0.003524352051364814,
      "grad_norm": 3.6899819374084473,
      "learning_rate": 0.00047508719481813657,
      "loss": 0.4556,
      "step": 101
    },
    {
      "epoch": 0.0035592466261308024,
      "grad_norm": 4.269426345825195,
      "learning_rate": 0.0004748380667663179,
      "loss": 0.4965,
      "step": 102
    },
    {
      "epoch": 0.0035941412008967904,
      "grad_norm": 2.91923189163208,
      "learning_rate": 0.00047458893871449927,
      "loss": 0.5772,
      "step": 103
    },
    {
      "epoch": 0.003629035775662779,
      "grad_norm": 2.5374391078948975,
      "learning_rate": 0.0004743398106626806,
      "loss": 0.4345,
      "step": 104
    },
    {
      "epoch": 0.003663930350428767,
      "grad_norm": 6.120459079742432,
      "learning_rate": 0.00047409068261086197,
      "loss": 0.6008,
      "step": 105
    },
    {
      "epoch": 0.0036988249251947553,
      "grad_norm": 3.4010274410247803,
      "learning_rate": 0.0004738415545590434,
      "loss": 0.509,
      "step": 106
    },
    {
      "epoch": 0.0037337194999607437,
      "grad_norm": 8.0973539352417,
      "learning_rate": 0.0004735924265072247,
      "loss": 0.7293,
      "step": 107
    },
    {
      "epoch": 0.0037686140747267317,
      "grad_norm": 3.3099887371063232,
      "learning_rate": 0.0004733432984554061,
      "loss": 0.4356,
      "step": 108
    },
    {
      "epoch": 0.00380350864949272,
      "grad_norm": 6.317104339599609,
      "learning_rate": 0.0004730941704035874,
      "loss": 0.4928,
      "step": 109
    },
    {
      "epoch": 0.003838403224258708,
      "grad_norm": 2.5514118671417236,
      "learning_rate": 0.00047284504235176885,
      "loss": 0.5602,
      "step": 110
    },
    {
      "epoch": 0.0038732977990246966,
      "grad_norm": 2.0278637409210205,
      "learning_rate": 0.0004725959142999502,
      "loss": 0.4692,
      "step": 111
    },
    {
      "epoch": 0.003908192373790685,
      "grad_norm": 4.491654396057129,
      "learning_rate": 0.00047234678624813155,
      "loss": 0.6521,
      "step": 112
    },
    {
      "epoch": 0.0039430869485566735,
      "grad_norm": 2.2400240898132324,
      "learning_rate": 0.00047209765819631287,
      "loss": 0.3315,
      "step": 113
    },
    {
      "epoch": 0.0039779815233226615,
      "grad_norm": 2.88393235206604,
      "learning_rate": 0.0004718485301444943,
      "loss": 0.5546,
      "step": 114
    },
    {
      "epoch": 0.0040128760980886495,
      "grad_norm": 6.284704685211182,
      "learning_rate": 0.0004715994020926757,
      "loss": 0.5839,
      "step": 115
    },
    {
      "epoch": 0.004047770672854638,
      "grad_norm": 9.000625610351562,
      "learning_rate": 0.000471350274040857,
      "loss": 0.6164,
      "step": 116
    },
    {
      "epoch": 0.004082665247620626,
      "grad_norm": 9.20932388305664,
      "learning_rate": 0.0004711011459890384,
      "loss": 0.7645,
      "step": 117
    },
    {
      "epoch": 0.004117559822386614,
      "grad_norm": 12.134368896484375,
      "learning_rate": 0.00047085201793721975,
      "loss": 0.7666,
      "step": 118
    },
    {
      "epoch": 0.004152454397152602,
      "grad_norm": 6.2442145347595215,
      "learning_rate": 0.00047060288988540113,
      "loss": 0.7563,
      "step": 119
    },
    {
      "epoch": 0.004187348971918591,
      "grad_norm": 9.213313102722168,
      "learning_rate": 0.00047035376183358245,
      "loss": 0.8867,
      "step": 120
    },
    {
      "epoch": 0.004222243546684579,
      "grad_norm": 4.905455589294434,
      "learning_rate": 0.00047010463378176383,
      "loss": 0.6964,
      "step": 121
    },
    {
      "epoch": 0.004257138121450567,
      "grad_norm": 4.810601234436035,
      "learning_rate": 0.00046985550572994515,
      "loss": 0.8251,
      "step": 122
    },
    {
      "epoch": 0.004292032696216556,
      "grad_norm": 2.471174478530884,
      "learning_rate": 0.0004696063776781266,
      "loss": 0.7389,
      "step": 123
    },
    {
      "epoch": 0.004326927270982544,
      "grad_norm": 5.860949516296387,
      "learning_rate": 0.00046935724962630796,
      "loss": 0.4181,
      "step": 124
    },
    {
      "epoch": 0.004361821845748532,
      "grad_norm": 4.554804801940918,
      "learning_rate": 0.0004691081215744893,
      "loss": 0.5695,
      "step": 125
    },
    {
      "epoch": 0.00439671642051452,
      "grad_norm": 2.5328774452209473,
      "learning_rate": 0.00046885899352267066,
      "loss": 0.8064,
      "step": 126
    },
    {
      "epoch": 0.004431610995280509,
      "grad_norm": 4.576289653778076,
      "learning_rate": 0.00046860986547085203,
      "loss": 0.4917,
      "step": 127
    },
    {
      "epoch": 0.004466505570046497,
      "grad_norm": 3.655006170272827,
      "learning_rate": 0.0004683607374190334,
      "loss": 0.3945,
      "step": 128
    },
    {
      "epoch": 0.004501400144812485,
      "grad_norm": 6.895676612854004,
      "learning_rate": 0.00046811160936721473,
      "loss": 0.4763,
      "step": 129
    },
    {
      "epoch": 0.004536294719578474,
      "grad_norm": 4.013714790344238,
      "learning_rate": 0.0004678624813153961,
      "loss": 0.615,
      "step": 130
    },
    {
      "epoch": 0.004571189294344462,
      "grad_norm": 5.884292125701904,
      "learning_rate": 0.0004676133532635775,
      "loss": 0.5725,
      "step": 131
    },
    {
      "epoch": 0.00460608386911045,
      "grad_norm": 10.301098823547363,
      "learning_rate": 0.00046736422521175886,
      "loss": 0.5814,
      "step": 132
    },
    {
      "epoch": 0.004640978443876439,
      "grad_norm": 6.455746650695801,
      "learning_rate": 0.00046711509715994024,
      "loss": 0.5085,
      "step": 133
    },
    {
      "epoch": 0.004675873018642427,
      "grad_norm": 8.60205078125,
      "learning_rate": 0.00046686596910812156,
      "loss": 0.2494,
      "step": 134
    },
    {
      "epoch": 0.004710767593408415,
      "grad_norm": 7.675626277923584,
      "learning_rate": 0.000466616841056303,
      "loss": 0.4379,
      "step": 135
    },
    {
      "epoch": 0.004745662168174403,
      "grad_norm": 5.432530879974365,
      "learning_rate": 0.0004663677130044843,
      "loss": 0.4104,
      "step": 136
    },
    {
      "epoch": 0.0047805567429403915,
      "grad_norm": 7.0333356857299805,
      "learning_rate": 0.0004661185849526657,
      "loss": 0.7177,
      "step": 137
    },
    {
      "epoch": 0.0048154513177063795,
      "grad_norm": 2.904719352722168,
      "learning_rate": 0.000465869456900847,
      "loss": 0.4149,
      "step": 138
    },
    {
      "epoch": 0.0048503458924723675,
      "grad_norm": 2.497433662414551,
      "learning_rate": 0.0004656203288490284,
      "loss": 0.361,
      "step": 139
    },
    {
      "epoch": 0.004885240467238356,
      "grad_norm": 17.868417739868164,
      "learning_rate": 0.0004653712007972098,
      "loss": 0.6148,
      "step": 140
    },
    {
      "epoch": 0.004920135042004344,
      "grad_norm": 4.529391765594482,
      "learning_rate": 0.00046512207274539114,
      "loss": 0.5523,
      "step": 141
    },
    {
      "epoch": 0.004955029616770332,
      "grad_norm": 6.303463459014893,
      "learning_rate": 0.0004648729446935725,
      "loss": 0.2814,
      "step": 142
    },
    {
      "epoch": 0.004989924191536321,
      "grad_norm": 4.003666400909424,
      "learning_rate": 0.00046462381664175384,
      "loss": 0.4978,
      "step": 143
    },
    {
      "epoch": 0.005024818766302309,
      "grad_norm": 2.383483409881592,
      "learning_rate": 0.00046437468858993527,
      "loss": 0.4477,
      "step": 144
    },
    {
      "epoch": 0.005059713341068297,
      "grad_norm": 4.666818141937256,
      "learning_rate": 0.0004641255605381166,
      "loss": 0.7471,
      "step": 145
    },
    {
      "epoch": 0.005094607915834285,
      "grad_norm": 3.408939838409424,
      "learning_rate": 0.00046387643248629797,
      "loss": 0.6049,
      "step": 146
    },
    {
      "epoch": 0.005129502490600274,
      "grad_norm": 2.7218916416168213,
      "learning_rate": 0.0004636273044344793,
      "loss": 0.2515,
      "step": 147
    },
    {
      "epoch": 0.005164397065366262,
      "grad_norm": 5.01939058303833,
      "learning_rate": 0.0004633781763826607,
      "loss": 0.5361,
      "step": 148
    },
    {
      "epoch": 0.00519929164013225,
      "grad_norm": 2.8118577003479004,
      "learning_rate": 0.0004631290483308421,
      "loss": 0.4957,
      "step": 149
    },
    {
      "epoch": 0.005234186214898239,
      "grad_norm": 3.408773899078369,
      "learning_rate": 0.0004628799202790234,
      "loss": 0.3437,
      "step": 150
    },
    {
      "epoch": 0.005269080789664227,
      "grad_norm": 3.789778709411621,
      "learning_rate": 0.0004626307922272048,
      "loss": 0.6044,
      "step": 151
    },
    {
      "epoch": 0.005303975364430215,
      "grad_norm": 5.5164008140563965,
      "learning_rate": 0.0004623816641753862,
      "loss": 0.7435,
      "step": 152
    },
    {
      "epoch": 0.005338869939196204,
      "grad_norm": 7.906383037567139,
      "learning_rate": 0.00046213253612356755,
      "loss": 0.3161,
      "step": 153
    },
    {
      "epoch": 0.005373764513962192,
      "grad_norm": 4.132258415222168,
      "learning_rate": 0.0004618834080717489,
      "loss": 0.6252,
      "step": 154
    },
    {
      "epoch": 0.00540865908872818,
      "grad_norm": 8.626569747924805,
      "learning_rate": 0.00046163428001993025,
      "loss": 0.4359,
      "step": 155
    },
    {
      "epoch": 0.005443553663494168,
      "grad_norm": 6.910851001739502,
      "learning_rate": 0.00046138515196811157,
      "loss": 0.4672,
      "step": 156
    },
    {
      "epoch": 0.005478448238260157,
      "grad_norm": 7.003218173980713,
      "learning_rate": 0.000461136023916293,
      "loss": 0.5908,
      "step": 157
    },
    {
      "epoch": 0.005513342813026145,
      "grad_norm": 3.771320104598999,
      "learning_rate": 0.0004608868958644744,
      "loss": 0.444,
      "step": 158
    },
    {
      "epoch": 0.005548237387792133,
      "grad_norm": 4.468625068664551,
      "learning_rate": 0.0004606377678126557,
      "loss": 0.4905,
      "step": 159
    },
    {
      "epoch": 0.005583131962558122,
      "grad_norm": 3.453885316848755,
      "learning_rate": 0.0004603886397608371,
      "loss": 0.2888,
      "step": 160
    },
    {
      "epoch": 0.00561802653732411,
      "grad_norm": 2.8732011318206787,
      "learning_rate": 0.00046013951170901845,
      "loss": 0.3401,
      "step": 161
    },
    {
      "epoch": 0.005652921112090098,
      "grad_norm": 4.697991847991943,
      "learning_rate": 0.00045989038365719983,
      "loss": 0.4632,
      "step": 162
    },
    {
      "epoch": 0.0056878156868560865,
      "grad_norm": 3.0242550373077393,
      "learning_rate": 0.00045964125560538115,
      "loss": 0.4959,
      "step": 163
    },
    {
      "epoch": 0.0057227102616220745,
      "grad_norm": 5.483144283294678,
      "learning_rate": 0.00045939212755356253,
      "loss": 0.6918,
      "step": 164
    },
    {
      "epoch": 0.0057576048363880625,
      "grad_norm": 3.41015625,
      "learning_rate": 0.0004591429995017439,
      "loss": 0.4362,
      "step": 165
    },
    {
      "epoch": 0.0057924994111540505,
      "grad_norm": 2.876286029815674,
      "learning_rate": 0.0004588938714499253,
      "loss": 0.6043,
      "step": 166
    },
    {
      "epoch": 0.005827393985920039,
      "grad_norm": 3.32312273979187,
      "learning_rate": 0.00045864474339810666,
      "loss": 0.3029,
      "step": 167
    },
    {
      "epoch": 0.005862288560686027,
      "grad_norm": 2.376347303390503,
      "learning_rate": 0.000458395615346288,
      "loss": 0.4156,
      "step": 168
    },
    {
      "epoch": 0.005897183135452015,
      "grad_norm": 2.9692606925964355,
      "learning_rate": 0.0004581464872944694,
      "loss": 0.4561,
      "step": 169
    },
    {
      "epoch": 0.005932077710218004,
      "grad_norm": 4.183121681213379,
      "learning_rate": 0.00045789735924265073,
      "loss": 0.4554,
      "step": 170
    },
    {
      "epoch": 0.005966972284983992,
      "grad_norm": 5.311225414276123,
      "learning_rate": 0.0004576482311908321,
      "loss": 0.5823,
      "step": 171
    },
    {
      "epoch": 0.00600186685974998,
      "grad_norm": 4.964428901672363,
      "learning_rate": 0.00045739910313901343,
      "loss": 0.4357,
      "step": 172
    },
    {
      "epoch": 0.006036761434515968,
      "grad_norm": 2.3446717262268066,
      "learning_rate": 0.0004571499750871948,
      "loss": 0.4969,
      "step": 173
    },
    {
      "epoch": 0.006071656009281957,
      "grad_norm": 4.932802200317383,
      "learning_rate": 0.0004569008470353762,
      "loss": 0.5294,
      "step": 174
    },
    {
      "epoch": 0.006106550584047945,
      "grad_norm": 4.570998668670654,
      "learning_rate": 0.00045665171898355756,
      "loss": 0.3772,
      "step": 175
    },
    {
      "epoch": 0.006141445158813933,
      "grad_norm": 5.973507881164551,
      "learning_rate": 0.00045640259093173894,
      "loss": 0.5949,
      "step": 176
    },
    {
      "epoch": 0.006176339733579922,
      "grad_norm": 6.562748432159424,
      "learning_rate": 0.00045615346287992026,
      "loss": 0.3632,
      "step": 177
    },
    {
      "epoch": 0.00621123430834591,
      "grad_norm": 3.3465213775634766,
      "learning_rate": 0.0004559043348281017,
      "loss": 0.4249,
      "step": 178
    },
    {
      "epoch": 0.006246128883111898,
      "grad_norm": 6.249157428741455,
      "learning_rate": 0.000455655206776283,
      "loss": 0.6096,
      "step": 179
    },
    {
      "epoch": 0.006281023457877887,
      "grad_norm": 7.020566463470459,
      "learning_rate": 0.0004554060787244644,
      "loss": 0.2365,
      "step": 180
    },
    {
      "epoch": 0.006315918032643875,
      "grad_norm": 14.231945991516113,
      "learning_rate": 0.0004551569506726457,
      "loss": 0.567,
      "step": 181
    },
    {
      "epoch": 0.006350812607409863,
      "grad_norm": 7.75086784362793,
      "learning_rate": 0.00045490782262082714,
      "loss": 0.4293,
      "step": 182
    },
    {
      "epoch": 0.006385707182175851,
      "grad_norm": 5.761453151702881,
      "learning_rate": 0.00045465869456900846,
      "loss": 0.3295,
      "step": 183
    },
    {
      "epoch": 0.00642060175694184,
      "grad_norm": 7.618579864501953,
      "learning_rate": 0.00045440956651718984,
      "loss": 0.521,
      "step": 184
    },
    {
      "epoch": 0.006455496331707828,
      "grad_norm": 3.642460346221924,
      "learning_rate": 0.0004541604384653712,
      "loss": 0.5773,
      "step": 185
    },
    {
      "epoch": 0.006490390906473816,
      "grad_norm": 2.6925058364868164,
      "learning_rate": 0.0004539113104135526,
      "loss": 0.4134,
      "step": 186
    },
    {
      "epoch": 0.0065252854812398045,
      "grad_norm": 5.6538519859313965,
      "learning_rate": 0.00045366218236173397,
      "loss": 0.5491,
      "step": 187
    },
    {
      "epoch": 0.0065601800560057925,
      "grad_norm": 5.971572399139404,
      "learning_rate": 0.0004534130543099153,
      "loss": 0.4732,
      "step": 188
    },
    {
      "epoch": 0.0065950746307717805,
      "grad_norm": 9.808980941772461,
      "learning_rate": 0.00045316392625809667,
      "loss": 0.5921,
      "step": 189
    },
    {
      "epoch": 0.006629969205537769,
      "grad_norm": 4.005667209625244,
      "learning_rate": 0.000452914798206278,
      "loss": 0.4916,
      "step": 190
    },
    {
      "epoch": 0.006664863780303757,
      "grad_norm": 2.6640374660491943,
      "learning_rate": 0.0004526656701544594,
      "loss": 0.4992,
      "step": 191
    },
    {
      "epoch": 0.006699758355069745,
      "grad_norm": 5.2160186767578125,
      "learning_rate": 0.00045241654210264074,
      "loss": 0.5519,
      "step": 192
    },
    {
      "epoch": 0.006734652929835733,
      "grad_norm": 2.392328977584839,
      "learning_rate": 0.0004521674140508221,
      "loss": 0.4452,
      "step": 193
    },
    {
      "epoch": 0.006769547504601722,
      "grad_norm": 3.684330701828003,
      "learning_rate": 0.0004519182859990035,
      "loss": 0.3322,
      "step": 194
    },
    {
      "epoch": 0.00680444207936771,
      "grad_norm": 5.533759593963623,
      "learning_rate": 0.0004516691579471849,
      "loss": 0.3445,
      "step": 195
    },
    {
      "epoch": 0.006839336654133698,
      "grad_norm": 2.748739242553711,
      "learning_rate": 0.00045142002989536625,
      "loss": 0.3598,
      "step": 196
    },
    {
      "epoch": 0.006874231228899687,
      "grad_norm": 3.2070744037628174,
      "learning_rate": 0.00045117090184354757,
      "loss": 0.301,
      "step": 197
    },
    {
      "epoch": 0.006909125803665675,
      "grad_norm": 4.843875408172607,
      "learning_rate": 0.00045092177379172895,
      "loss": 0.3516,
      "step": 198
    },
    {
      "epoch": 0.006944020378431663,
      "grad_norm": 3.4647748470306396,
      "learning_rate": 0.0004506726457399103,
      "loss": 0.3898,
      "step": 199
    },
    {
      "epoch": 0.006978914953197652,
      "grad_norm": 4.528743743896484,
      "learning_rate": 0.0004504235176880917,
      "loss": 0.5129,
      "step": 200
    },
    {
      "epoch": 0.00701380952796364,
      "grad_norm": 9.13222885131836,
      "learning_rate": 0.0004501743896362731,
      "loss": 0.7452,
      "step": 201
    },
    {
      "epoch": 0.007048704102729628,
      "grad_norm": 8.887048721313477,
      "learning_rate": 0.0004499252615844544,
      "loss": 0.4188,
      "step": 202
    },
    {
      "epoch": 0.007083598677495616,
      "grad_norm": 5.840224742889404,
      "learning_rate": 0.00044967613353263583,
      "loss": 0.6684,
      "step": 203
    },
    {
      "epoch": 0.007118493252261605,
      "grad_norm": 3.947303533554077,
      "learning_rate": 0.00044942700548081715,
      "loss": 0.371,
      "step": 204
    },
    {
      "epoch": 0.007153387827027593,
      "grad_norm": 2.5601823329925537,
      "learning_rate": 0.00044917787742899853,
      "loss": 0.5295,
      "step": 205
    },
    {
      "epoch": 0.007188282401793581,
      "grad_norm": 1.8601516485214233,
      "learning_rate": 0.00044892874937717985,
      "loss": 0.4915,
      "step": 206
    },
    {
      "epoch": 0.00722317697655957,
      "grad_norm": 5.662216663360596,
      "learning_rate": 0.00044867962132536123,
      "loss": 0.6268,
      "step": 207
    },
    {
      "epoch": 0.007258071551325558,
      "grad_norm": 3.8027286529541016,
      "learning_rate": 0.0004484304932735426,
      "loss": 0.495,
      "step": 208
    },
    {
      "epoch": 0.007292966126091546,
      "grad_norm": 4.030114650726318,
      "learning_rate": 0.000448181365221724,
      "loss": 0.4809,
      "step": 209
    },
    {
      "epoch": 0.007327860700857534,
      "grad_norm": 1.564842939376831,
      "learning_rate": 0.00044793223716990536,
      "loss": 0.432,
      "step": 210
    },
    {
      "epoch": 0.007362755275623523,
      "grad_norm": 3.3509438037872314,
      "learning_rate": 0.0004476831091180867,
      "loss": 0.3364,
      "step": 211
    },
    {
      "epoch": 0.007397649850389511,
      "grad_norm": 4.1627116203308105,
      "learning_rate": 0.0004474339810662681,
      "loss": 0.489,
      "step": 212
    },
    {
      "epoch": 0.007432544425155499,
      "grad_norm": 1.645795226097107,
      "learning_rate": 0.00044718485301444943,
      "loss": 0.2194,
      "step": 213
    },
    {
      "epoch": 0.0074674389999214875,
      "grad_norm": 2.07328200340271,
      "learning_rate": 0.0004469357249626308,
      "loss": 0.4235,
      "step": 214
    },
    {
      "epoch": 0.0075023335746874755,
      "grad_norm": 6.218151569366455,
      "learning_rate": 0.00044668659691081213,
      "loss": 0.51,
      "step": 215
    },
    {
      "epoch": 0.0075372281494534635,
      "grad_norm": 3.5234122276306152,
      "learning_rate": 0.00044643746885899356,
      "loss": 0.2653,
      "step": 216
    },
    {
      "epoch": 0.007572122724219452,
      "grad_norm": 4.643627643585205,
      "learning_rate": 0.0004461883408071749,
      "loss": 0.4877,
      "step": 217
    },
    {
      "epoch": 0.00760701729898544,
      "grad_norm": 2.5045876502990723,
      "learning_rate": 0.00044593921275535626,
      "loss": 0.1862,
      "step": 218
    },
    {
      "epoch": 0.007641911873751428,
      "grad_norm": 4.3351054191589355,
      "learning_rate": 0.00044569008470353764,
      "loss": 0.566,
      "step": 219
    },
    {
      "epoch": 0.007676806448517416,
      "grad_norm": 4.105373859405518,
      "learning_rate": 0.000445440956651719,
      "loss": 0.4459,
      "step": 220
    },
    {
      "epoch": 0.007711701023283405,
      "grad_norm": 6.438356876373291,
      "learning_rate": 0.0004451918285999004,
      "loss": 0.4155,
      "step": 221
    },
    {
      "epoch": 0.007746595598049393,
      "grad_norm": 3.113896369934082,
      "learning_rate": 0.0004449427005480817,
      "loss": 0.5701,
      "step": 222
    },
    {
      "epoch": 0.007781490172815381,
      "grad_norm": 5.664767742156982,
      "learning_rate": 0.0004446935724962631,
      "loss": 0.6239,
      "step": 223
    },
    {
      "epoch": 0.00781638474758137,
      "grad_norm": 7.0937180519104,
      "learning_rate": 0.0004444444444444444,
      "loss": 0.5253,
      "step": 224
    },
    {
      "epoch": 0.007851279322347358,
      "grad_norm": 9.05324649810791,
      "learning_rate": 0.00044419531639262584,
      "loss": 0.7309,
      "step": 225
    },
    {
      "epoch": 0.007886173897113347,
      "grad_norm": 3.385918617248535,
      "learning_rate": 0.00044394618834080716,
      "loss": 0.3329,
      "step": 226
    },
    {
      "epoch": 0.007921068471879334,
      "grad_norm": 1.733538269996643,
      "learning_rate": 0.00044369706028898854,
      "loss": 0.2668,
      "step": 227
    },
    {
      "epoch": 0.007955963046645323,
      "grad_norm": 6.609569072723389,
      "learning_rate": 0.0004434479322371699,
      "loss": 0.5792,
      "step": 228
    },
    {
      "epoch": 0.007990857621411312,
      "grad_norm": 4.771899700164795,
      "learning_rate": 0.0004431988041853513,
      "loss": 0.475,
      "step": 229
    },
    {
      "epoch": 0.008025752196177299,
      "grad_norm": 3.5788989067077637,
      "learning_rate": 0.00044294967613353267,
      "loss": 0.3046,
      "step": 230
    },
    {
      "epoch": 0.008060646770943288,
      "grad_norm": 1.8831599950790405,
      "learning_rate": 0.000442700548081714,
      "loss": 0.2915,
      "step": 231
    },
    {
      "epoch": 0.008095541345709277,
      "grad_norm": 6.831310749053955,
      "learning_rate": 0.00044245142002989537,
      "loss": 0.5466,
      "step": 232
    },
    {
      "epoch": 0.008130435920475264,
      "grad_norm": 3.2426488399505615,
      "learning_rate": 0.00044220229197807675,
      "loss": 0.3831,
      "step": 233
    },
    {
      "epoch": 0.008165330495241253,
      "grad_norm": 2.358731746673584,
      "learning_rate": 0.0004419531639262581,
      "loss": 0.4223,
      "step": 234
    },
    {
      "epoch": 0.00820022507000724,
      "grad_norm": 4.0409464836120605,
      "learning_rate": 0.00044170403587443944,
      "loss": 0.5637,
      "step": 235
    },
    {
      "epoch": 0.008235119644773229,
      "grad_norm": 5.097674369812012,
      "learning_rate": 0.0004414549078226208,
      "loss": 0.4497,
      "step": 236
    },
    {
      "epoch": 0.008270014219539218,
      "grad_norm": 5.187782287597656,
      "learning_rate": 0.00044120577977080225,
      "loss": 0.6849,
      "step": 237
    },
    {
      "epoch": 0.008304908794305205,
      "grad_norm": 2.169788122177124,
      "learning_rate": 0.0004409566517189836,
      "loss": 0.4267,
      "step": 238
    },
    {
      "epoch": 0.008339803369071194,
      "grad_norm": 1.6536530256271362,
      "learning_rate": 0.00044070752366716495,
      "loss": 0.4734,
      "step": 239
    },
    {
      "epoch": 0.008374697943837182,
      "grad_norm": 6.9722065925598145,
      "learning_rate": 0.00044045839561534627,
      "loss": 0.3028,
      "step": 240
    },
    {
      "epoch": 0.00840959251860317,
      "grad_norm": 2.509368419647217,
      "learning_rate": 0.00044020926756352765,
      "loss": 0.3968,
      "step": 241
    },
    {
      "epoch": 0.008444487093369158,
      "grad_norm": 2.8096835613250732,
      "learning_rate": 0.000439960139511709,
      "loss": 0.6112,
      "step": 242
    },
    {
      "epoch": 0.008479381668135147,
      "grad_norm": 5.294726848602295,
      "learning_rate": 0.0004397110114598904,
      "loss": 0.5374,
      "step": 243
    },
    {
      "epoch": 0.008514276242901134,
      "grad_norm": 6.68627405166626,
      "learning_rate": 0.0004394618834080717,
      "loss": 0.3933,
      "step": 244
    },
    {
      "epoch": 0.008549170817667123,
      "grad_norm": 3.847761631011963,
      "learning_rate": 0.0004392127553562531,
      "loss": 0.2421,
      "step": 245
    },
    {
      "epoch": 0.008584065392433112,
      "grad_norm": 2.4541547298431396,
      "learning_rate": 0.00043896362730443453,
      "loss": 0.2996,
      "step": 246
    },
    {
      "epoch": 0.0086189599671991,
      "grad_norm": 3.3341314792633057,
      "learning_rate": 0.00043871449925261585,
      "loss": 0.3803,
      "step": 247
    },
    {
      "epoch": 0.008653854541965088,
      "grad_norm": 3.6863253116607666,
      "learning_rate": 0.00043846537120079723,
      "loss": 0.3719,
      "step": 248
    },
    {
      "epoch": 0.008688749116731077,
      "grad_norm": 5.726950645446777,
      "learning_rate": 0.00043821624314897855,
      "loss": 0.4272,
      "step": 249
    },
    {
      "epoch": 0.008723643691497064,
      "grad_norm": 1.6389575004577637,
      "learning_rate": 0.00043796711509716,
      "loss": 0.4954,
      "step": 250
    },
    {
      "epoch": 0.008758538266263053,
      "grad_norm": 7.990952014923096,
      "learning_rate": 0.0004377179870453413,
      "loss": 0.8895,
      "step": 251
    },
    {
      "epoch": 0.00879343284102904,
      "grad_norm": 1.973973035812378,
      "learning_rate": 0.0004374688589935227,
      "loss": 0.2873,
      "step": 252
    },
    {
      "epoch": 0.008828327415795029,
      "grad_norm": 2.367307424545288,
      "learning_rate": 0.000437219730941704,
      "loss": 0.2017,
      "step": 253
    },
    {
      "epoch": 0.008863221990561018,
      "grad_norm": 5.4470720291137695,
      "learning_rate": 0.0004369706028898854,
      "loss": 0.6114,
      "step": 254
    },
    {
      "epoch": 0.008898116565327005,
      "grad_norm": 2.849903106689453,
      "learning_rate": 0.0004367214748380668,
      "loss": 0.3501,
      "step": 255
    },
    {
      "epoch": 0.008933011140092994,
      "grad_norm": 3.3946869373321533,
      "learning_rate": 0.00043647234678624813,
      "loss": 0.3524,
      "step": 256
    },
    {
      "epoch": 0.008967905714858983,
      "grad_norm": 2.354367256164551,
      "learning_rate": 0.0004362232187344295,
      "loss": 0.5148,
      "step": 257
    },
    {
      "epoch": 0.00900280028962497,
      "grad_norm": 2.776526689529419,
      "learning_rate": 0.00043597409068261083,
      "loss": 0.3318,
      "step": 258
    },
    {
      "epoch": 0.009037694864390959,
      "grad_norm": 5.0001726150512695,
      "learning_rate": 0.00043572496263079226,
      "loss": 0.4183,
      "step": 259
    },
    {
      "epoch": 0.009072589439156948,
      "grad_norm": 8.05362606048584,
      "learning_rate": 0.0004354758345789736,
      "loss": 0.3749,
      "step": 260
    },
    {
      "epoch": 0.009107484013922935,
      "grad_norm": 4.781121730804443,
      "learning_rate": 0.00043522670652715496,
      "loss": 0.572,
      "step": 261
    },
    {
      "epoch": 0.009142378588688924,
      "grad_norm": 4.387824058532715,
      "learning_rate": 0.00043497757847533634,
      "loss": 0.2486,
      "step": 262
    },
    {
      "epoch": 0.009177273163454912,
      "grad_norm": 6.740039825439453,
      "learning_rate": 0.0004347284504235177,
      "loss": 0.4398,
      "step": 263
    },
    {
      "epoch": 0.0092121677382209,
      "grad_norm": 2.931262254714966,
      "learning_rate": 0.0004344793223716991,
      "loss": 0.4488,
      "step": 264
    },
    {
      "epoch": 0.009247062312986888,
      "grad_norm": 2.344970464706421,
      "learning_rate": 0.0004342301943198804,
      "loss": 0.4446,
      "step": 265
    },
    {
      "epoch": 0.009281956887752877,
      "grad_norm": 2.344207763671875,
      "learning_rate": 0.0004339810662680618,
      "loss": 0.5839,
      "step": 266
    },
    {
      "epoch": 0.009316851462518864,
      "grad_norm": 2.530064582824707,
      "learning_rate": 0.00043373193821624317,
      "loss": 0.3308,
      "step": 267
    },
    {
      "epoch": 0.009351746037284853,
      "grad_norm": 2.956998109817505,
      "learning_rate": 0.00043348281016442454,
      "loss": 0.2913,
      "step": 268
    },
    {
      "epoch": 0.009386640612050842,
      "grad_norm": 1.2404779195785522,
      "learning_rate": 0.00043323368211260586,
      "loss": 0.3027,
      "step": 269
    },
    {
      "epoch": 0.00942153518681683,
      "grad_norm": 3.4780516624450684,
      "learning_rate": 0.00043298455406078724,
      "loss": 0.2746,
      "step": 270
    },
    {
      "epoch": 0.009456429761582818,
      "grad_norm": 3.7945404052734375,
      "learning_rate": 0.0004327354260089686,
      "loss": 0.3529,
      "step": 271
    },
    {
      "epoch": 0.009491324336348805,
      "grad_norm": 5.433294296264648,
      "learning_rate": 0.00043248629795715,
      "loss": 0.718,
      "step": 272
    },
    {
      "epoch": 0.009526218911114794,
      "grad_norm": 4.150812149047852,
      "learning_rate": 0.00043223716990533137,
      "loss": 0.4682,
      "step": 273
    },
    {
      "epoch": 0.009561113485880783,
      "grad_norm": 1.7946490049362183,
      "learning_rate": 0.0004319880418535127,
      "loss": 0.21,
      "step": 274
    },
    {
      "epoch": 0.00959600806064677,
      "grad_norm": 2.255284070968628,
      "learning_rate": 0.00043173891380169407,
      "loss": 0.3696,
      "step": 275
    },
    {
      "epoch": 0.009630902635412759,
      "grad_norm": 3.431877851486206,
      "learning_rate": 0.00043148978574987544,
      "loss": 0.2918,
      "step": 276
    },
    {
      "epoch": 0.009665797210178748,
      "grad_norm": 2.6171600818634033,
      "learning_rate": 0.0004312406576980568,
      "loss": 0.4535,
      "step": 277
    },
    {
      "epoch": 0.009700691784944735,
      "grad_norm": 6.2097272872924805,
      "learning_rate": 0.00043099152964623814,
      "loss": 0.4726,
      "step": 278
    },
    {
      "epoch": 0.009735586359710724,
      "grad_norm": 6.454467296600342,
      "learning_rate": 0.0004307424015944195,
      "loss": 0.4441,
      "step": 279
    },
    {
      "epoch": 0.009770480934476713,
      "grad_norm": 5.085592269897461,
      "learning_rate": 0.00043049327354260095,
      "loss": 0.5415,
      "step": 280
    },
    {
      "epoch": 0.0098053755092427,
      "grad_norm": 5.672427654266357,
      "learning_rate": 0.0004302441454907823,
      "loss": 0.5464,
      "step": 281
    },
    {
      "epoch": 0.009840270084008689,
      "grad_norm": 8.203423500061035,
      "learning_rate": 0.00042999501743896365,
      "loss": 0.4584,
      "step": 282
    },
    {
      "epoch": 0.009875164658774678,
      "grad_norm": 2.4791488647460938,
      "learning_rate": 0.00042974588938714497,
      "loss": 0.2637,
      "step": 283
    },
    {
      "epoch": 0.009910059233540665,
      "grad_norm": 1.4547913074493408,
      "learning_rate": 0.0004294967613353264,
      "loss": 0.3684,
      "step": 284
    },
    {
      "epoch": 0.009944953808306654,
      "grad_norm": 2.002784252166748,
      "learning_rate": 0.0004292476332835077,
      "loss": 0.3405,
      "step": 285
    },
    {
      "epoch": 0.009979848383072643,
      "grad_norm": 1.9521069526672363,
      "learning_rate": 0.0004289985052316891,
      "loss": 0.519,
      "step": 286
    },
    {
      "epoch": 0.01001474295783863,
      "grad_norm": 4.492144584655762,
      "learning_rate": 0.0004287493771798704,
      "loss": 0.4941,
      "step": 287
    },
    {
      "epoch": 0.010049637532604619,
      "grad_norm": 4.8328657150268555,
      "learning_rate": 0.0004285002491280518,
      "loss": 0.5809,
      "step": 288
    },
    {
      "epoch": 0.010084532107370607,
      "grad_norm": 2.1249587535858154,
      "learning_rate": 0.00042825112107623323,
      "loss": 0.3679,
      "step": 289
    },
    {
      "epoch": 0.010119426682136595,
      "grad_norm": 3.299175500869751,
      "learning_rate": 0.00042800199302441455,
      "loss": 0.4299,
      "step": 290
    },
    {
      "epoch": 0.010154321256902583,
      "grad_norm": 4.902045726776123,
      "learning_rate": 0.00042775286497259593,
      "loss": 0.509,
      "step": 291
    },
    {
      "epoch": 0.01018921583166857,
      "grad_norm": 9.517048835754395,
      "learning_rate": 0.00042750373692077725,
      "loss": 0.492,
      "step": 292
    },
    {
      "epoch": 0.01022411040643456,
      "grad_norm": 1.1202301979064941,
      "learning_rate": 0.0004272546088689587,
      "loss": 0.1303,
      "step": 293
    },
    {
      "epoch": 0.010259004981200548,
      "grad_norm": 4.743006706237793,
      "learning_rate": 0.00042700548081714,
      "loss": 0.49,
      "step": 294
    },
    {
      "epoch": 0.010293899555966535,
      "grad_norm": 5.881464958190918,
      "learning_rate": 0.0004267563527653214,
      "loss": 0.3819,
      "step": 295
    },
    {
      "epoch": 0.010328794130732524,
      "grad_norm": 2.4761765003204346,
      "learning_rate": 0.0004265072247135027,
      "loss": 0.2075,
      "step": 296
    },
    {
      "epoch": 0.010363688705498513,
      "grad_norm": 3.748422861099243,
      "learning_rate": 0.00042625809666168413,
      "loss": 0.4077,
      "step": 297
    },
    {
      "epoch": 0.0103985832802645,
      "grad_norm": 1.6829403638839722,
      "learning_rate": 0.0004260089686098655,
      "loss": 0.2815,
      "step": 298
    },
    {
      "epoch": 0.01043347785503049,
      "grad_norm": 3.976104974746704,
      "learning_rate": 0.00042575984055804683,
      "loss": 0.3888,
      "step": 299
    },
    {
      "epoch": 0.010468372429796478,
      "grad_norm": 3.858619213104248,
      "learning_rate": 0.0004255107125062282,
      "loss": 0.3775,
      "step": 300
    },
    {
      "epoch": 0.010503267004562465,
      "grad_norm": 2.279083490371704,
      "learning_rate": 0.0004252615844544096,
      "loss": 0.4383,
      "step": 301
    },
    {
      "epoch": 0.010538161579328454,
      "grad_norm": 1.7227927446365356,
      "learning_rate": 0.00042501245640259096,
      "loss": 0.4152,
      "step": 302
    },
    {
      "epoch": 0.010573056154094443,
      "grad_norm": 4.498323440551758,
      "learning_rate": 0.0004247633283507723,
      "loss": 0.4942,
      "step": 303
    },
    {
      "epoch": 0.01060795072886043,
      "grad_norm": 3.196117639541626,
      "learning_rate": 0.00042451420029895366,
      "loss": 0.5166,
      "step": 304
    },
    {
      "epoch": 0.010642845303626419,
      "grad_norm": 1.8816494941711426,
      "learning_rate": 0.000424265072247135,
      "loss": 0.3222,
      "step": 305
    },
    {
      "epoch": 0.010677739878392408,
      "grad_norm": 3.7242257595062256,
      "learning_rate": 0.0004240159441953164,
      "loss": 0.3459,
      "step": 306
    },
    {
      "epoch": 0.010712634453158395,
      "grad_norm": 3.630566120147705,
      "learning_rate": 0.0004237668161434978,
      "loss": 0.1803,
      "step": 307
    },
    {
      "epoch": 0.010747529027924384,
      "grad_norm": 9.551702499389648,
      "learning_rate": 0.0004235176880916791,
      "loss": 0.4501,
      "step": 308
    },
    {
      "epoch": 0.010782423602690371,
      "grad_norm": 2.774725914001465,
      "learning_rate": 0.0004232685600398605,
      "loss": 0.3284,
      "step": 309
    },
    {
      "epoch": 0.01081731817745636,
      "grad_norm": 4.514040470123291,
      "learning_rate": 0.00042301943198804186,
      "loss": 0.2856,
      "step": 310
    },
    {
      "epoch": 0.010852212752222349,
      "grad_norm": 2.196784496307373,
      "learning_rate": 0.00042277030393622324,
      "loss": 0.4882,
      "step": 311
    },
    {
      "epoch": 0.010887107326988336,
      "grad_norm": 1.827577829360962,
      "learning_rate": 0.00042252117588440456,
      "loss": 0.3027,
      "step": 312
    },
    {
      "epoch": 0.010922001901754325,
      "grad_norm": 1.6714074611663818,
      "learning_rate": 0.00042227204783258594,
      "loss": 0.4901,
      "step": 313
    },
    {
      "epoch": 0.010956896476520313,
      "grad_norm": 1.5393011569976807,
      "learning_rate": 0.00042202291978076737,
      "loss": 0.3639,
      "step": 314
    },
    {
      "epoch": 0.0109917910512863,
      "grad_norm": 5.618338584899902,
      "learning_rate": 0.0004217737917289487,
      "loss": 0.3176,
      "step": 315
    },
    {
      "epoch": 0.01102668562605229,
      "grad_norm": 4.420034408569336,
      "learning_rate": 0.00042152466367713007,
      "loss": 0.3909,
      "step": 316
    },
    {
      "epoch": 0.011061580200818278,
      "grad_norm": 10.293754577636719,
      "learning_rate": 0.0004212755356253114,
      "loss": 0.6557,
      "step": 317
    },
    {
      "epoch": 0.011096474775584265,
      "grad_norm": 2.9170074462890625,
      "learning_rate": 0.0004210264075734928,
      "loss": 0.4586,
      "step": 318
    },
    {
      "epoch": 0.011131369350350254,
      "grad_norm": 1.3407671451568604,
      "learning_rate": 0.00042077727952167414,
      "loss": 0.2559,
      "step": 319
    },
    {
      "epoch": 0.011166263925116243,
      "grad_norm": 1.9488496780395508,
      "learning_rate": 0.0004205281514698555,
      "loss": 0.2154,
      "step": 320
    },
    {
      "epoch": 0.01120115849988223,
      "grad_norm": 3.8182475566864014,
      "learning_rate": 0.00042027902341803684,
      "loss": 0.3792,
      "step": 321
    },
    {
      "epoch": 0.01123605307464822,
      "grad_norm": 3.5125298500061035,
      "learning_rate": 0.0004200298953662182,
      "loss": 0.525,
      "step": 322
    },
    {
      "epoch": 0.011270947649414208,
      "grad_norm": 1.6968598365783691,
      "learning_rate": 0.00041978076731439965,
      "loss": 0.3886,
      "step": 323
    },
    {
      "epoch": 0.011305842224180195,
      "grad_norm": 2.91359806060791,
      "learning_rate": 0.00041953163926258097,
      "loss": 0.2772,
      "step": 324
    },
    {
      "epoch": 0.011340736798946184,
      "grad_norm": 4.736337184906006,
      "learning_rate": 0.00041928251121076235,
      "loss": 0.3686,
      "step": 325
    },
    {
      "epoch": 0.011375631373712173,
      "grad_norm": 7.711532115936279,
      "learning_rate": 0.00041903338315894367,
      "loss": 0.4311,
      "step": 326
    },
    {
      "epoch": 0.01141052594847816,
      "grad_norm": 4.513404369354248,
      "learning_rate": 0.0004187842551071251,
      "loss": 0.3357,
      "step": 327
    },
    {
      "epoch": 0.011445420523244149,
      "grad_norm": 4.4024977684021,
      "learning_rate": 0.0004185351270553064,
      "loss": 0.4239,
      "step": 328
    },
    {
      "epoch": 0.011480315098010136,
      "grad_norm": 2.825946569442749,
      "learning_rate": 0.0004182859990034878,
      "loss": 0.2592,
      "step": 329
    },
    {
      "epoch": 0.011515209672776125,
      "grad_norm": 5.241634368896484,
      "learning_rate": 0.0004180368709516691,
      "loss": 0.5336,
      "step": 330
    },
    {
      "epoch": 0.011550104247542114,
      "grad_norm": 9.312636375427246,
      "learning_rate": 0.00041778774289985055,
      "loss": 0.5398,
      "step": 331
    },
    {
      "epoch": 0.011584998822308101,
      "grad_norm": 6.863613128662109,
      "learning_rate": 0.00041753861484803193,
      "loss": 0.5672,
      "step": 332
    },
    {
      "epoch": 0.01161989339707409,
      "grad_norm": 4.433732509613037,
      "learning_rate": 0.00041728948679621325,
      "loss": 0.3494,
      "step": 333
    },
    {
      "epoch": 0.011654787971840079,
      "grad_norm": 2.758436679840088,
      "learning_rate": 0.00041704035874439463,
      "loss": 0.2769,
      "step": 334
    },
    {
      "epoch": 0.011689682546606066,
      "grad_norm": 4.456250190734863,
      "learning_rate": 0.000416791230692576,
      "loss": 0.593,
      "step": 335
    },
    {
      "epoch": 0.011724577121372055,
      "grad_norm": 1.7079931497573853,
      "learning_rate": 0.0004165421026407574,
      "loss": 0.5373,
      "step": 336
    },
    {
      "epoch": 0.011759471696138044,
      "grad_norm": 5.578596115112305,
      "learning_rate": 0.0004162929745889387,
      "loss": 0.5661,
      "step": 337
    },
    {
      "epoch": 0.01179436627090403,
      "grad_norm": 1.89806067943573,
      "learning_rate": 0.0004160438465371201,
      "loss": 0.2507,
      "step": 338
    },
    {
      "epoch": 0.01182926084567002,
      "grad_norm": 2.4524943828582764,
      "learning_rate": 0.0004157947184853014,
      "loss": 0.4231,
      "step": 339
    },
    {
      "epoch": 0.011864155420436008,
      "grad_norm": 12.574894905090332,
      "learning_rate": 0.00041554559043348283,
      "loss": 0.5585,
      "step": 340
    },
    {
      "epoch": 0.011899049995201996,
      "grad_norm": 2.279693126678467,
      "learning_rate": 0.0004152964623816642,
      "loss": 0.6268,
      "step": 341
    },
    {
      "epoch": 0.011933944569967984,
      "grad_norm": 10.141854286193848,
      "learning_rate": 0.00041504733432984553,
      "loss": 0.4929,
      "step": 342
    },
    {
      "epoch": 0.011968839144733973,
      "grad_norm": 2.8105947971343994,
      "learning_rate": 0.0004147982062780269,
      "loss": 0.5208,
      "step": 343
    },
    {
      "epoch": 0.01200373371949996,
      "grad_norm": 5.037689208984375,
      "learning_rate": 0.0004145490782262083,
      "loss": 0.4392,
      "step": 344
    },
    {
      "epoch": 0.01203862829426595,
      "grad_norm": 6.045564651489258,
      "learning_rate": 0.00041429995017438966,
      "loss": 0.5345,
      "step": 345
    },
    {
      "epoch": 0.012073522869031936,
      "grad_norm": 5.244930744171143,
      "learning_rate": 0.000414050822122571,
      "loss": 0.6162,
      "step": 346
    },
    {
      "epoch": 0.012108417443797925,
      "grad_norm": 3.3194215297698975,
      "learning_rate": 0.00041380169407075236,
      "loss": 0.4586,
      "step": 347
    },
    {
      "epoch": 0.012143312018563914,
      "grad_norm": 6.057051181793213,
      "learning_rate": 0.00041355256601893374,
      "loss": 0.4463,
      "step": 348
    },
    {
      "epoch": 0.012178206593329901,
      "grad_norm": 1.3101407289505005,
      "learning_rate": 0.0004133034379671151,
      "loss": 0.448,
      "step": 349
    },
    {
      "epoch": 0.01221310116809589,
      "grad_norm": 3.170398473739624,
      "learning_rate": 0.0004130543099152965,
      "loss": 0.3408,
      "step": 350
    },
    {
      "epoch": 0.012247995742861879,
      "grad_norm": 3.937359571456909,
      "learning_rate": 0.0004128051818634778,
      "loss": 0.4857,
      "step": 351
    },
    {
      "epoch": 0.012282890317627866,
      "grad_norm": 1.7371827363967896,
      "learning_rate": 0.00041255605381165924,
      "loss": 0.3475,
      "step": 352
    },
    {
      "epoch": 0.012317784892393855,
      "grad_norm": 2.799729347229004,
      "learning_rate": 0.00041230692575984056,
      "loss": 0.4482,
      "step": 353
    },
    {
      "epoch": 0.012352679467159844,
      "grad_norm": 5.464942932128906,
      "learning_rate": 0.00041205779770802194,
      "loss": 0.5345,
      "step": 354
    },
    {
      "epoch": 0.012387574041925831,
      "grad_norm": 2.144134998321533,
      "learning_rate": 0.00041180866965620326,
      "loss": 0.5432,
      "step": 355
    },
    {
      "epoch": 0.01242246861669182,
      "grad_norm": 1.9100829362869263,
      "learning_rate": 0.00041155954160438464,
      "loss": 0.4795,
      "step": 356
    },
    {
      "epoch": 0.012457363191457809,
      "grad_norm": 3.414881467819214,
      "learning_rate": 0.000411310413552566,
      "loss": 0.5009,
      "step": 357
    },
    {
      "epoch": 0.012492257766223796,
      "grad_norm": 1.9327346086502075,
      "learning_rate": 0.0004110612855007474,
      "loss": 0.2328,
      "step": 358
    },
    {
      "epoch": 0.012527152340989785,
      "grad_norm": 1.7375929355621338,
      "learning_rate": 0.00041081215744892877,
      "loss": 0.5448,
      "step": 359
    },
    {
      "epoch": 0.012562046915755774,
      "grad_norm": 1.449977159500122,
      "learning_rate": 0.0004105630293971101,
      "loss": 0.3644,
      "step": 360
    },
    {
      "epoch": 0.01259694149052176,
      "grad_norm": 1.6328673362731934,
      "learning_rate": 0.0004103139013452915,
      "loss": 0.333,
      "step": 361
    },
    {
      "epoch": 0.01263183606528775,
      "grad_norm": 4.853394508361816,
      "learning_rate": 0.00041006477329347284,
      "loss": 0.53,
      "step": 362
    },
    {
      "epoch": 0.012666730640053738,
      "grad_norm": 1.429169774055481,
      "learning_rate": 0.0004098156452416542,
      "loss": 0.3647,
      "step": 363
    },
    {
      "epoch": 0.012701625214819726,
      "grad_norm": 2.3874785900115967,
      "learning_rate": 0.00040956651718983554,
      "loss": 0.3233,
      "step": 364
    },
    {
      "epoch": 0.012736519789585714,
      "grad_norm": 2.7261037826538086,
      "learning_rate": 0.000409317389138017,
      "loss": 0.4179,
      "step": 365
    },
    {
      "epoch": 0.012771414364351702,
      "grad_norm": 1.945633888244629,
      "learning_rate": 0.00040906826108619835,
      "loss": 0.2998,
      "step": 366
    },
    {
      "epoch": 0.01280630893911769,
      "grad_norm": 8.603433609008789,
      "learning_rate": 0.00040881913303437967,
      "loss": 0.3611,
      "step": 367
    },
    {
      "epoch": 0.01284120351388368,
      "grad_norm": 3.2039761543273926,
      "learning_rate": 0.00040857000498256105,
      "loss": 0.2986,
      "step": 368
    },
    {
      "epoch": 0.012876098088649666,
      "grad_norm": 4.154094696044922,
      "learning_rate": 0.0004083208769307424,
      "loss": 0.2874,
      "step": 369
    },
    {
      "epoch": 0.012910992663415655,
      "grad_norm": 1.6088875532150269,
      "learning_rate": 0.0004080717488789238,
      "loss": 0.1932,
      "step": 370
    },
    {
      "epoch": 0.012945887238181644,
      "grad_norm": 3.3621840476989746,
      "learning_rate": 0.0004078226208271051,
      "loss": 0.2536,
      "step": 371
    },
    {
      "epoch": 0.012980781812947631,
      "grad_norm": 2.6412699222564697,
      "learning_rate": 0.0004075734927752865,
      "loss": 0.3009,
      "step": 372
    },
    {
      "epoch": 0.01301567638771362,
      "grad_norm": 1.7183481454849243,
      "learning_rate": 0.0004073243647234678,
      "loss": 0.2127,
      "step": 373
    },
    {
      "epoch": 0.013050570962479609,
      "grad_norm": 9.494209289550781,
      "learning_rate": 0.00040707523667164925,
      "loss": 0.4471,
      "step": 374
    },
    {
      "epoch": 0.013085465537245596,
      "grad_norm": 5.7355146408081055,
      "learning_rate": 0.00040682610861983063,
      "loss": 0.6407,
      "step": 375
    },
    {
      "epoch": 0.013120360112011585,
      "grad_norm": 4.6648335456848145,
      "learning_rate": 0.00040657698056801195,
      "loss": 0.3607,
      "step": 376
    },
    {
      "epoch": 0.013155254686777574,
      "grad_norm": 7.412822723388672,
      "learning_rate": 0.00040632785251619333,
      "loss": 0.595,
      "step": 377
    },
    {
      "epoch": 0.013190149261543561,
      "grad_norm": 4.463756084442139,
      "learning_rate": 0.0004060787244643747,
      "loss": 0.6948,
      "step": 378
    },
    {
      "epoch": 0.01322504383630955,
      "grad_norm": 2.650115966796875,
      "learning_rate": 0.0004058295964125561,
      "loss": 0.5,
      "step": 379
    },
    {
      "epoch": 0.013259938411075539,
      "grad_norm": 4.9498372077941895,
      "learning_rate": 0.0004055804683607374,
      "loss": 0.5002,
      "step": 380
    },
    {
      "epoch": 0.013294832985841526,
      "grad_norm": 3.3909144401550293,
      "learning_rate": 0.0004053313403089188,
      "loss": 0.5566,
      "step": 381
    },
    {
      "epoch": 0.013329727560607515,
      "grad_norm": 6.6416168212890625,
      "learning_rate": 0.00040508221225710016,
      "loss": 0.2543,
      "step": 382
    },
    {
      "epoch": 0.013364622135373502,
      "grad_norm": 6.128859996795654,
      "learning_rate": 0.00040483308420528153,
      "loss": 0.5162,
      "step": 383
    },
    {
      "epoch": 0.01339951671013949,
      "grad_norm": 3.5571987628936768,
      "learning_rate": 0.0004045839561534629,
      "loss": 0.2708,
      "step": 384
    },
    {
      "epoch": 0.01343441128490548,
      "grad_norm": 10.294866561889648,
      "learning_rate": 0.00040433482810164423,
      "loss": 0.6314,
      "step": 385
    },
    {
      "epoch": 0.013469305859671467,
      "grad_norm": 4.232209205627441,
      "learning_rate": 0.00040408570004982566,
      "loss": 0.3799,
      "step": 386
    },
    {
      "epoch": 0.013504200434437456,
      "grad_norm": 4.2343034744262695,
      "learning_rate": 0.000403836571998007,
      "loss": 0.2985,
      "step": 387
    },
    {
      "epoch": 0.013539095009203445,
      "grad_norm": 2.4900310039520264,
      "learning_rate": 0.00040358744394618836,
      "loss": 0.5895,
      "step": 388
    },
    {
      "epoch": 0.013573989583969432,
      "grad_norm": 6.1201491355896,
      "learning_rate": 0.0004033383158943697,
      "loss": 0.3548,
      "step": 389
    },
    {
      "epoch": 0.01360888415873542,
      "grad_norm": 3.0130321979522705,
      "learning_rate": 0.00040308918784255106,
      "loss": 0.3805,
      "step": 390
    },
    {
      "epoch": 0.01364377873350141,
      "grad_norm": 5.290499210357666,
      "learning_rate": 0.00040284005979073244,
      "loss": 0.5031,
      "step": 391
    },
    {
      "epoch": 0.013678673308267397,
      "grad_norm": 8.532734870910645,
      "learning_rate": 0.0004025909317389138,
      "loss": 0.4354,
      "step": 392
    },
    {
      "epoch": 0.013713567883033385,
      "grad_norm": 3.090378999710083,
      "learning_rate": 0.0004023418036870952,
      "loss": 0.4922,
      "step": 393
    },
    {
      "epoch": 0.013748462457799374,
      "grad_norm": 5.993404865264893,
      "learning_rate": 0.0004020926756352765,
      "loss": 0.3877,
      "step": 394
    },
    {
      "epoch": 0.013783357032565361,
      "grad_norm": 4.076908588409424,
      "learning_rate": 0.00040184354758345794,
      "loss": 0.3225,
      "step": 395
    },
    {
      "epoch": 0.01381825160733135,
      "grad_norm": 6.655969142913818,
      "learning_rate": 0.00040159441953163926,
      "loss": 0.4125,
      "step": 396
    },
    {
      "epoch": 0.01385314618209734,
      "grad_norm": 1.9876683950424194,
      "learning_rate": 0.00040134529147982064,
      "loss": 0.4426,
      "step": 397
    },
    {
      "epoch": 0.013888040756863326,
      "grad_norm": 1.8007781505584717,
      "learning_rate": 0.00040109616342800196,
      "loss": 0.3473,
      "step": 398
    },
    {
      "epoch": 0.013922935331629315,
      "grad_norm": 7.1828532218933105,
      "learning_rate": 0.0004008470353761834,
      "loss": 0.2599,
      "step": 399
    },
    {
      "epoch": 0.013957829906395304,
      "grad_norm": 5.708067893981934,
      "learning_rate": 0.0004005979073243647,
      "loss": 0.4418,
      "step": 400
    },
    {
      "epoch": 0.013992724481161291,
      "grad_norm": 4.7407307624816895,
      "learning_rate": 0.0004003487792725461,
      "loss": 0.3556,
      "step": 401
    },
    {
      "epoch": 0.01402761905592728,
      "grad_norm": 6.04172420501709,
      "learning_rate": 0.00040009965122072747,
      "loss": 0.3358,
      "step": 402
    },
    {
      "epoch": 0.014062513630693267,
      "grad_norm": 3.0200066566467285,
      "learning_rate": 0.00039985052316890885,
      "loss": 0.6082,
      "step": 403
    },
    {
      "epoch": 0.014097408205459256,
      "grad_norm": 3.4921669960021973,
      "learning_rate": 0.0003996013951170902,
      "loss": 0.4086,
      "step": 404
    },
    {
      "epoch": 0.014132302780225245,
      "grad_norm": 4.707747936248779,
      "learning_rate": 0.00039935226706527154,
      "loss": 0.5182,
      "step": 405
    },
    {
      "epoch": 0.014167197354991232,
      "grad_norm": 2.5089476108551025,
      "learning_rate": 0.0003991031390134529,
      "loss": 0.2382,
      "step": 406
    },
    {
      "epoch": 0.014202091929757221,
      "grad_norm": 2.718796968460083,
      "learning_rate": 0.00039885401096163424,
      "loss": 0.508,
      "step": 407
    },
    {
      "epoch": 0.01423698650452321,
      "grad_norm": 2.756762742996216,
      "learning_rate": 0.0003986048829098157,
      "loss": 0.5771,
      "step": 408
    },
    {
      "epoch": 0.014271881079289197,
      "grad_norm": 4.128317832946777,
      "learning_rate": 0.000398355754857997,
      "loss": 0.34,
      "step": 409
    },
    {
      "epoch": 0.014306775654055186,
      "grad_norm": 2.8592920303344727,
      "learning_rate": 0.00039810662680617837,
      "loss": 0.3185,
      "step": 410
    },
    {
      "epoch": 0.014341670228821175,
      "grad_norm": 2.1640994548797607,
      "learning_rate": 0.00039785749875435975,
      "loss": 0.505,
      "step": 411
    },
    {
      "epoch": 0.014376564803587162,
      "grad_norm": 4.245242595672607,
      "learning_rate": 0.0003976083707025411,
      "loss": 0.2835,
      "step": 412
    },
    {
      "epoch": 0.01441145937835315,
      "grad_norm": 5.996641635894775,
      "learning_rate": 0.0003973592426507225,
      "loss": 0.5112,
      "step": 413
    },
    {
      "epoch": 0.01444635395311914,
      "grad_norm": 2.2332465648651123,
      "learning_rate": 0.0003971101145989038,
      "loss": 0.3167,
      "step": 414
    },
    {
      "epoch": 0.014481248527885127,
      "grad_norm": 2.866523504257202,
      "learning_rate": 0.0003968609865470852,
      "loss": 0.5232,
      "step": 415
    },
    {
      "epoch": 0.014516143102651115,
      "grad_norm": 1.251052737236023,
      "learning_rate": 0.0003966118584952666,
      "loss": 0.2792,
      "step": 416
    },
    {
      "epoch": 0.014551037677417104,
      "grad_norm": 3.6669809818267822,
      "learning_rate": 0.00039636273044344795,
      "loss": 0.3953,
      "step": 417
    },
    {
      "epoch": 0.014585932252183091,
      "grad_norm": 2.5719187259674072,
      "learning_rate": 0.0003961136023916293,
      "loss": 0.476,
      "step": 418
    },
    {
      "epoch": 0.01462082682694908,
      "grad_norm": 3.2878198623657227,
      "learning_rate": 0.00039586447433981065,
      "loss": 0.4516,
      "step": 419
    },
    {
      "epoch": 0.014655721401715067,
      "grad_norm": 4.712095737457275,
      "learning_rate": 0.0003956153462879921,
      "loss": 0.314,
      "step": 420
    },
    {
      "epoch": 0.014690615976481056,
      "grad_norm": 2.760484457015991,
      "learning_rate": 0.0003953662182361734,
      "loss": 0.2569,
      "step": 421
    },
    {
      "epoch": 0.014725510551247045,
      "grad_norm": 4.890283584594727,
      "learning_rate": 0.0003951170901843548,
      "loss": 0.4142,
      "step": 422
    },
    {
      "epoch": 0.014760405126013032,
      "grad_norm": 4.460312366485596,
      "learning_rate": 0.0003948679621325361,
      "loss": 0.3839,
      "step": 423
    },
    {
      "epoch": 0.014795299700779021,
      "grad_norm": 3.3774921894073486,
      "learning_rate": 0.0003946188340807175,
      "loss": 0.4186,
      "step": 424
    },
    {
      "epoch": 0.01483019427554501,
      "grad_norm": 6.802966594696045,
      "learning_rate": 0.00039436970602889886,
      "loss": 0.2832,
      "step": 425
    },
    {
      "epoch": 0.014865088850310997,
      "grad_norm": 1.2026853561401367,
      "learning_rate": 0.00039412057797708023,
      "loss": 0.3843,
      "step": 426
    },
    {
      "epoch": 0.014899983425076986,
      "grad_norm": 5.730804443359375,
      "learning_rate": 0.0003938714499252616,
      "loss": 0.5166,
      "step": 427
    },
    {
      "epoch": 0.014934877999842975,
      "grad_norm": 5.229554653167725,
      "learning_rate": 0.00039362232187344293,
      "loss": 0.6382,
      "step": 428
    },
    {
      "epoch": 0.014969772574608962,
      "grad_norm": 4.070996284484863,
      "learning_rate": 0.00039337319382162436,
      "loss": 0.3347,
      "step": 429
    },
    {
      "epoch": 0.015004667149374951,
      "grad_norm": 2.4073164463043213,
      "learning_rate": 0.0003931240657698057,
      "loss": 0.4229,
      "step": 430
    },
    {
      "epoch": 0.01503956172414094,
      "grad_norm": 1.5973265171051025,
      "learning_rate": 0.00039287493771798706,
      "loss": 0.2081,
      "step": 431
    },
    {
      "epoch": 0.015074456298906927,
      "grad_norm": 4.545201778411865,
      "learning_rate": 0.0003926258096661684,
      "loss": 0.2001,
      "step": 432
    },
    {
      "epoch": 0.015109350873672916,
      "grad_norm": 2.2834415435791016,
      "learning_rate": 0.0003923766816143498,
      "loss": 0.2136,
      "step": 433
    },
    {
      "epoch": 0.015144245448438905,
      "grad_norm": 3.206861734390259,
      "learning_rate": 0.00039212755356253114,
      "loss": 0.3142,
      "step": 434
    },
    {
      "epoch": 0.015179140023204892,
      "grad_norm": 2.0471031665802,
      "learning_rate": 0.0003918784255107125,
      "loss": 0.4465,
      "step": 435
    },
    {
      "epoch": 0.01521403459797088,
      "grad_norm": 8.141973495483398,
      "learning_rate": 0.0003916292974588939,
      "loss": 0.5664,
      "step": 436
    },
    {
      "epoch": 0.01524892917273687,
      "grad_norm": 6.920530796051025,
      "learning_rate": 0.00039138016940707526,
      "loss": 0.4322,
      "step": 437
    },
    {
      "epoch": 0.015283823747502857,
      "grad_norm": 3.3813459873199463,
      "learning_rate": 0.00039113104135525664,
      "loss": 0.316,
      "step": 438
    },
    {
      "epoch": 0.015318718322268846,
      "grad_norm": 4.473339557647705,
      "learning_rate": 0.00039088191330343796,
      "loss": 0.6106,
      "step": 439
    },
    {
      "epoch": 0.015353612897034833,
      "grad_norm": 3.3097336292266846,
      "learning_rate": 0.00039063278525161934,
      "loss": 0.5572,
      "step": 440
    },
    {
      "epoch": 0.015388507471800822,
      "grad_norm": 3.2097251415252686,
      "learning_rate": 0.00039038365719980066,
      "loss": 0.3172,
      "step": 441
    },
    {
      "epoch": 0.01542340204656681,
      "grad_norm": 4.328136444091797,
      "learning_rate": 0.0003901345291479821,
      "loss": 0.2762,
      "step": 442
    },
    {
      "epoch": 0.015458296621332798,
      "grad_norm": 2.7838268280029297,
      "learning_rate": 0.0003898854010961634,
      "loss": 0.4062,
      "step": 443
    },
    {
      "epoch": 0.015493191196098786,
      "grad_norm": 3.5299313068389893,
      "learning_rate": 0.0003896362730443448,
      "loss": 0.6501,
      "step": 444
    },
    {
      "epoch": 0.015528085770864775,
      "grad_norm": 2.375081777572632,
      "learning_rate": 0.00038938714499252617,
      "loss": 0.5042,
      "step": 445
    },
    {
      "epoch": 0.015562980345630762,
      "grad_norm": 4.893121242523193,
      "learning_rate": 0.00038913801694070754,
      "loss": 0.5562,
      "step": 446
    },
    {
      "epoch": 0.015597874920396751,
      "grad_norm": 2.760937213897705,
      "learning_rate": 0.0003888888888888889,
      "loss": 0.2636,
      "step": 447
    },
    {
      "epoch": 0.01563276949516274,
      "grad_norm": 1.5170586109161377,
      "learning_rate": 0.00038863976083707024,
      "loss": 0.2432,
      "step": 448
    },
    {
      "epoch": 0.015667664069928727,
      "grad_norm": 3.464463472366333,
      "learning_rate": 0.0003883906327852516,
      "loss": 0.3676,
      "step": 449
    },
    {
      "epoch": 0.015702558644694716,
      "grad_norm": 11.062788009643555,
      "learning_rate": 0.000388141504733433,
      "loss": 0.393,
      "step": 450
    },
    {
      "epoch": 0.015737453219460705,
      "grad_norm": 2.3907718658447266,
      "learning_rate": 0.00038789237668161437,
      "loss": 0.2056,
      "step": 451
    },
    {
      "epoch": 0.015772347794226694,
      "grad_norm": 1.6080018281936646,
      "learning_rate": 0.0003876432486297957,
      "loss": 0.323,
      "step": 452
    },
    {
      "epoch": 0.01580724236899268,
      "grad_norm": 1.7710040807724,
      "learning_rate": 0.00038739412057797707,
      "loss": 0.4627,
      "step": 453
    },
    {
      "epoch": 0.015842136943758668,
      "grad_norm": 4.159505367279053,
      "learning_rate": 0.0003871449925261585,
      "loss": 0.4466,
      "step": 454
    },
    {
      "epoch": 0.015877031518524657,
      "grad_norm": 3.5498852729797363,
      "learning_rate": 0.0003868958644743398,
      "loss": 0.2932,
      "step": 455
    },
    {
      "epoch": 0.015911926093290646,
      "grad_norm": 3.494032621383667,
      "learning_rate": 0.0003866467364225212,
      "loss": 0.3863,
      "step": 456
    },
    {
      "epoch": 0.015946820668056635,
      "grad_norm": 2.4732587337493896,
      "learning_rate": 0.0003863976083707025,
      "loss": 0.3631,
      "step": 457
    },
    {
      "epoch": 0.015981715242822624,
      "grad_norm": 11.434162139892578,
      "learning_rate": 0.0003861484803188839,
      "loss": 0.2795,
      "step": 458
    },
    {
      "epoch": 0.01601660981758861,
      "grad_norm": 4.0894670486450195,
      "learning_rate": 0.0003858993522670653,
      "loss": 0.3901,
      "step": 459
    },
    {
      "epoch": 0.016051504392354598,
      "grad_norm": 1.9035097360610962,
      "learning_rate": 0.00038565022421524665,
      "loss": 0.2526,
      "step": 460
    },
    {
      "epoch": 0.016086398967120587,
      "grad_norm": 3.4629430770874023,
      "learning_rate": 0.000385401096163428,
      "loss": 0.6303,
      "step": 461
    },
    {
      "epoch": 0.016121293541886576,
      "grad_norm": 3.298886299133301,
      "learning_rate": 0.00038515196811160935,
      "loss": 0.5399,
      "step": 462
    },
    {
      "epoch": 0.016156188116652564,
      "grad_norm": 6.333906173706055,
      "learning_rate": 0.0003849028400597908,
      "loss": 0.7012,
      "step": 463
    },
    {
      "epoch": 0.016191082691418553,
      "grad_norm": 2.211949586868286,
      "learning_rate": 0.0003846537120079721,
      "loss": 0.3629,
      "step": 464
    },
    {
      "epoch": 0.01622597726618454,
      "grad_norm": 2.680621862411499,
      "learning_rate": 0.0003844045839561535,
      "loss": 0.2609,
      "step": 465
    },
    {
      "epoch": 0.016260871840950528,
      "grad_norm": 11.58118724822998,
      "learning_rate": 0.0003841554559043348,
      "loss": 0.5951,
      "step": 466
    },
    {
      "epoch": 0.016295766415716516,
      "grad_norm": 3.8026742935180664,
      "learning_rate": 0.00038390632785251623,
      "loss": 0.2777,
      "step": 467
    },
    {
      "epoch": 0.016330660990482505,
      "grad_norm": 1.5137187242507935,
      "learning_rate": 0.00038365719980069756,
      "loss": 0.3665,
      "step": 468
    },
    {
      "epoch": 0.016365555565248494,
      "grad_norm": 4.040452480316162,
      "learning_rate": 0.00038340807174887893,
      "loss": 0.2913,
      "step": 469
    },
    {
      "epoch": 0.01640045014001448,
      "grad_norm": 2.426311492919922,
      "learning_rate": 0.00038315894369706025,
      "loss": 0.4184,
      "step": 470
    },
    {
      "epoch": 0.01643534471478047,
      "grad_norm": 2.710510015487671,
      "learning_rate": 0.0003829098156452417,
      "loss": 0.3586,
      "step": 471
    },
    {
      "epoch": 0.016470239289546457,
      "grad_norm": 4.231709003448486,
      "learning_rate": 0.00038266068759342306,
      "loss": 0.4185,
      "step": 472
    },
    {
      "epoch": 0.016505133864312446,
      "grad_norm": 3.062171697616577,
      "learning_rate": 0.0003824115595416044,
      "loss": 0.4731,
      "step": 473
    },
    {
      "epoch": 0.016540028439078435,
      "grad_norm": 1.1834050416946411,
      "learning_rate": 0.00038216243148978576,
      "loss": 0.1452,
      "step": 474
    },
    {
      "epoch": 0.016574923013844424,
      "grad_norm": 2.752499580383301,
      "learning_rate": 0.0003819133034379671,
      "loss": 0.1796,
      "step": 475
    },
    {
      "epoch": 0.01660981758861041,
      "grad_norm": 4.293211936950684,
      "learning_rate": 0.0003816641753861485,
      "loss": 0.3707,
      "step": 476
    },
    {
      "epoch": 0.016644712163376398,
      "grad_norm": 1.7624311447143555,
      "learning_rate": 0.00038141504733432984,
      "loss": 0.2172,
      "step": 477
    },
    {
      "epoch": 0.016679606738142387,
      "grad_norm": 3.723328113555908,
      "learning_rate": 0.0003811659192825112,
      "loss": 0.5373,
      "step": 478
    },
    {
      "epoch": 0.016714501312908376,
      "grad_norm": 3.0376555919647217,
      "learning_rate": 0.00038091679123069253,
      "loss": 0.348,
      "step": 479
    },
    {
      "epoch": 0.016749395887674365,
      "grad_norm": 3.875135898590088,
      "learning_rate": 0.00038066766317887396,
      "loss": 0.555,
      "step": 480
    },
    {
      "epoch": 0.016784290462440354,
      "grad_norm": 1.1340278387069702,
      "learning_rate": 0.00038041853512705534,
      "loss": 0.2576,
      "step": 481
    },
    {
      "epoch": 0.01681918503720634,
      "grad_norm": 1.2017040252685547,
      "learning_rate": 0.00038016940707523666,
      "loss": 0.3647,
      "step": 482
    },
    {
      "epoch": 0.016854079611972328,
      "grad_norm": 5.686746597290039,
      "learning_rate": 0.00037992027902341804,
      "loss": 0.3058,
      "step": 483
    },
    {
      "epoch": 0.016888974186738317,
      "grad_norm": 1.8839224576950073,
      "learning_rate": 0.0003796711509715994,
      "loss": 0.4196,
      "step": 484
    },
    {
      "epoch": 0.016923868761504306,
      "grad_norm": 7.010015964508057,
      "learning_rate": 0.0003794220229197808,
      "loss": 0.2579,
      "step": 485
    },
    {
      "epoch": 0.016958763336270295,
      "grad_norm": 2.0599441528320312,
      "learning_rate": 0.0003791728948679621,
      "loss": 0.2538,
      "step": 486
    },
    {
      "epoch": 0.01699365791103628,
      "grad_norm": 4.075503349304199,
      "learning_rate": 0.0003789237668161435,
      "loss": 0.42,
      "step": 487
    },
    {
      "epoch": 0.01702855248580227,
      "grad_norm": 1.5996294021606445,
      "learning_rate": 0.0003786746387643249,
      "loss": 0.3508,
      "step": 488
    },
    {
      "epoch": 0.017063447060568258,
      "grad_norm": 6.385864734649658,
      "learning_rate": 0.00037842551071250624,
      "loss": 0.306,
      "step": 489
    },
    {
      "epoch": 0.017098341635334247,
      "grad_norm": 4.847731590270996,
      "learning_rate": 0.0003781763826606876,
      "loss": 0.2273,
      "step": 490
    },
    {
      "epoch": 0.017133236210100235,
      "grad_norm": 6.463296413421631,
      "learning_rate": 0.00037792725460886894,
      "loss": 0.354,
      "step": 491
    },
    {
      "epoch": 0.017168130784866224,
      "grad_norm": 9.370889663696289,
      "learning_rate": 0.0003776781265570503,
      "loss": 0.6502,
      "step": 492
    },
    {
      "epoch": 0.01720302535963221,
      "grad_norm": 2.6976311206817627,
      "learning_rate": 0.0003774289985052317,
      "loss": 0.3478,
      "step": 493
    },
    {
      "epoch": 0.0172379199343982,
      "grad_norm": 7.216363906860352,
      "learning_rate": 0.00037717987045341307,
      "loss": 0.3526,
      "step": 494
    },
    {
      "epoch": 0.017272814509164187,
      "grad_norm": 7.5299906730651855,
      "learning_rate": 0.0003769307424015944,
      "loss": 0.5758,
      "step": 495
    },
    {
      "epoch": 0.017307709083930176,
      "grad_norm": 3.6009953022003174,
      "learning_rate": 0.00037668161434977577,
      "loss": 0.3921,
      "step": 496
    },
    {
      "epoch": 0.017342603658696165,
      "grad_norm": 2.7375190258026123,
      "learning_rate": 0.0003764324862979572,
      "loss": 0.2527,
      "step": 497
    },
    {
      "epoch": 0.017377498233462154,
      "grad_norm": 1.8508142232894897,
      "learning_rate": 0.0003761833582461385,
      "loss": 0.5293,
      "step": 498
    },
    {
      "epoch": 0.01741239280822814,
      "grad_norm": 3.8924639225006104,
      "learning_rate": 0.0003759342301943199,
      "loss": 0.4203,
      "step": 499
    },
    {
      "epoch": 0.01744728738299413,
      "grad_norm": 2.1733126640319824,
      "learning_rate": 0.0003756851021425012,
      "loss": 0.3382,
      "step": 500
    },
    {
      "epoch": 0.017482181957760117,
      "grad_norm": 4.95046329498291,
      "learning_rate": 0.00037543597409068265,
      "loss": 0.4271,
      "step": 501
    },
    {
      "epoch": 0.017517076532526106,
      "grad_norm": 4.939902305603027,
      "learning_rate": 0.000375186846038864,
      "loss": 0.4863,
      "step": 502
    },
    {
      "epoch": 0.017551971107292095,
      "grad_norm": 2.9170939922332764,
      "learning_rate": 0.00037493771798704535,
      "loss": 0.3189,
      "step": 503
    },
    {
      "epoch": 0.01758686568205808,
      "grad_norm": 2.245849847793579,
      "learning_rate": 0.0003746885899352267,
      "loss": 0.3654,
      "step": 504
    },
    {
      "epoch": 0.01762176025682407,
      "grad_norm": 3.414686441421509,
      "learning_rate": 0.0003744394618834081,
      "loss": 0.5095,
      "step": 505
    },
    {
      "epoch": 0.017656654831590058,
      "grad_norm": 2.8701095581054688,
      "learning_rate": 0.0003741903338315895,
      "loss": 0.3373,
      "step": 506
    },
    {
      "epoch": 0.017691549406356047,
      "grad_norm": 2.1708528995513916,
      "learning_rate": 0.0003739412057797708,
      "loss": 0.3118,
      "step": 507
    },
    {
      "epoch": 0.017726443981122036,
      "grad_norm": 1.5658648014068604,
      "learning_rate": 0.0003736920777279522,
      "loss": 0.4329,
      "step": 508
    },
    {
      "epoch": 0.017761338555888025,
      "grad_norm": 1.6754039525985718,
      "learning_rate": 0.0003734429496761335,
      "loss": 0.1198,
      "step": 509
    },
    {
      "epoch": 0.01779623313065401,
      "grad_norm": 1.0512155294418335,
      "learning_rate": 0.00037319382162431493,
      "loss": 0.2699,
      "step": 510
    },
    {
      "epoch": 0.01783112770542,
      "grad_norm": 4.513897895812988,
      "learning_rate": 0.00037294469357249626,
      "loss": 0.37,
      "step": 511
    },
    {
      "epoch": 0.017866022280185988,
      "grad_norm": 3.78256893157959,
      "learning_rate": 0.00037269556552067763,
      "loss": 0.4071,
      "step": 512
    },
    {
      "epoch": 0.017900916854951977,
      "grad_norm": 2.9695582389831543,
      "learning_rate": 0.00037244643746885895,
      "loss": 0.4266,
      "step": 513
    },
    {
      "epoch": 0.017935811429717965,
      "grad_norm": 1.4517648220062256,
      "learning_rate": 0.0003721973094170404,
      "loss": 0.3566,
      "step": 514
    },
    {
      "epoch": 0.017970706004483954,
      "grad_norm": 1.034108281135559,
      "learning_rate": 0.00037194818136522176,
      "loss": 0.3215,
      "step": 515
    },
    {
      "epoch": 0.01800560057924994,
      "grad_norm": 2.2136735916137695,
      "learning_rate": 0.0003716990533134031,
      "loss": 0.2818,
      "step": 516
    },
    {
      "epoch": 0.01804049515401593,
      "grad_norm": 2.9867193698883057,
      "learning_rate": 0.00037144992526158446,
      "loss": 0.4589,
      "step": 517
    },
    {
      "epoch": 0.018075389728781917,
      "grad_norm": 7.94342565536499,
      "learning_rate": 0.00037120079720976584,
      "loss": 0.3643,
      "step": 518
    },
    {
      "epoch": 0.018110284303547906,
      "grad_norm": 2.780332326889038,
      "learning_rate": 0.0003709516691579472,
      "loss": 0.3318,
      "step": 519
    },
    {
      "epoch": 0.018145178878313895,
      "grad_norm": 2.9223012924194336,
      "learning_rate": 0.00037070254110612853,
      "loss": 0.3984,
      "step": 520
    },
    {
      "epoch": 0.018180073453079884,
      "grad_norm": 1.3285354375839233,
      "learning_rate": 0.0003704534130543099,
      "loss": 0.4201,
      "step": 521
    },
    {
      "epoch": 0.01821496802784587,
      "grad_norm": 5.67521333694458,
      "learning_rate": 0.0003702042850024913,
      "loss": 0.4933,
      "step": 522
    },
    {
      "epoch": 0.01824986260261186,
      "grad_norm": 1.4424078464508057,
      "learning_rate": 0.00036995515695067266,
      "loss": 0.1742,
      "step": 523
    },
    {
      "epoch": 0.018284757177377847,
      "grad_norm": 3.599738836288452,
      "learning_rate": 0.00036970602889885404,
      "loss": 0.2874,
      "step": 524
    },
    {
      "epoch": 0.018319651752143836,
      "grad_norm": 2.90382719039917,
      "learning_rate": 0.00036945690084703536,
      "loss": 0.4286,
      "step": 525
    },
    {
      "epoch": 0.018354546326909825,
      "grad_norm": 4.8912177085876465,
      "learning_rate": 0.00036920777279521674,
      "loss": 0.2317,
      "step": 526
    },
    {
      "epoch": 0.01838944090167581,
      "grad_norm": 3.280059576034546,
      "learning_rate": 0.0003689586447433981,
      "loss": 0.5017,
      "step": 527
    },
    {
      "epoch": 0.0184243354764418,
      "grad_norm": 2.5154871940612793,
      "learning_rate": 0.0003687095166915795,
      "loss": 0.4749,
      "step": 528
    },
    {
      "epoch": 0.018459230051207788,
      "grad_norm": 2.1360836029052734,
      "learning_rate": 0.0003684603886397608,
      "loss": 0.2202,
      "step": 529
    },
    {
      "epoch": 0.018494124625973777,
      "grad_norm": 4.074864864349365,
      "learning_rate": 0.0003682112605879422,
      "loss": 0.3216,
      "step": 530
    },
    {
      "epoch": 0.018529019200739766,
      "grad_norm": 1.5990421772003174,
      "learning_rate": 0.0003679621325361236,
      "loss": 0.404,
      "step": 531
    },
    {
      "epoch": 0.018563913775505755,
      "grad_norm": 4.061786651611328,
      "learning_rate": 0.00036771300448430494,
      "loss": 0.1694,
      "step": 532
    },
    {
      "epoch": 0.01859880835027174,
      "grad_norm": 3.276765823364258,
      "learning_rate": 0.0003674638764324863,
      "loss": 0.4735,
      "step": 533
    },
    {
      "epoch": 0.01863370292503773,
      "grad_norm": 2.675656795501709,
      "learning_rate": 0.00036721474838066764,
      "loss": 0.4598,
      "step": 534
    },
    {
      "epoch": 0.018668597499803718,
      "grad_norm": 1.9363526105880737,
      "learning_rate": 0.0003669656203288491,
      "loss": 0.3217,
      "step": 535
    },
    {
      "epoch": 0.018703492074569707,
      "grad_norm": 2.8643674850463867,
      "learning_rate": 0.0003667164922770304,
      "loss": 0.4866,
      "step": 536
    },
    {
      "epoch": 0.018738386649335696,
      "grad_norm": 1.275941252708435,
      "learning_rate": 0.00036646736422521177,
      "loss": 0.4827,
      "step": 537
    },
    {
      "epoch": 0.018773281224101684,
      "grad_norm": 1.2644636631011963,
      "learning_rate": 0.0003662182361733931,
      "loss": 0.2996,
      "step": 538
    },
    {
      "epoch": 0.01880817579886767,
      "grad_norm": 2.6594481468200684,
      "learning_rate": 0.0003659691081215745,
      "loss": 0.2397,
      "step": 539
    },
    {
      "epoch": 0.01884307037363366,
      "grad_norm": 9.784626007080078,
      "learning_rate": 0.0003657199800697559,
      "loss": 0.4029,
      "step": 540
    },
    {
      "epoch": 0.018877964948399648,
      "grad_norm": 2.027665138244629,
      "learning_rate": 0.0003654708520179372,
      "loss": 0.3655,
      "step": 541
    },
    {
      "epoch": 0.018912859523165636,
      "grad_norm": 6.650440216064453,
      "learning_rate": 0.0003652217239661186,
      "loss": 0.3016,
      "step": 542
    },
    {
      "epoch": 0.018947754097931625,
      "grad_norm": 1.0808123350143433,
      "learning_rate": 0.0003649725959142999,
      "loss": 0.3458,
      "step": 543
    },
    {
      "epoch": 0.01898264867269761,
      "grad_norm": 2.46064829826355,
      "learning_rate": 0.00036472346786248135,
      "loss": 0.3773,
      "step": 544
    },
    {
      "epoch": 0.0190175432474636,
      "grad_norm": 8.419904708862305,
      "learning_rate": 0.0003644743398106627,
      "loss": 0.4689,
      "step": 545
    },
    {
      "epoch": 0.01905243782222959,
      "grad_norm": 6.3061394691467285,
      "learning_rate": 0.00036422521175884405,
      "loss": 0.5421,
      "step": 546
    },
    {
      "epoch": 0.019087332396995577,
      "grad_norm": 5.042564392089844,
      "learning_rate": 0.0003639760837070254,
      "loss": 0.3498,
      "step": 547
    },
    {
      "epoch": 0.019122226971761566,
      "grad_norm": 1.4820951223373413,
      "learning_rate": 0.0003637269556552068,
      "loss": 0.3804,
      "step": 548
    },
    {
      "epoch": 0.019157121546527555,
      "grad_norm": 1.6030627489089966,
      "learning_rate": 0.0003634778276033882,
      "loss": 0.3904,
      "step": 549
    },
    {
      "epoch": 0.01919201612129354,
      "grad_norm": 0.8531241416931152,
      "learning_rate": 0.0003632286995515695,
      "loss": 0.2708,
      "step": 550
    },
    {
      "epoch": 0.01922691069605953,
      "grad_norm": 4.995585918426514,
      "learning_rate": 0.0003629795714997509,
      "loss": 0.3923,
      "step": 551
    },
    {
      "epoch": 0.019261805270825518,
      "grad_norm": 6.852991104125977,
      "learning_rate": 0.00036273044344793226,
      "loss": 0.2833,
      "step": 552
    },
    {
      "epoch": 0.019296699845591507,
      "grad_norm": 2.097395896911621,
      "learning_rate": 0.00036248131539611363,
      "loss": 0.2997,
      "step": 553
    },
    {
      "epoch": 0.019331594420357496,
      "grad_norm": 2.6634960174560547,
      "learning_rate": 0.00036223218734429495,
      "loss": 0.5793,
      "step": 554
    },
    {
      "epoch": 0.019366488995123485,
      "grad_norm": 1.7528094053268433,
      "learning_rate": 0.00036198305929247633,
      "loss": 0.3012,
      "step": 555
    },
    {
      "epoch": 0.01940138356988947,
      "grad_norm": 1.4701555967330933,
      "learning_rate": 0.0003617339312406577,
      "loss": 0.2612,
      "step": 556
    },
    {
      "epoch": 0.01943627814465546,
      "grad_norm": 1.5463733673095703,
      "learning_rate": 0.0003614848031888391,
      "loss": 0.4682,
      "step": 557
    },
    {
      "epoch": 0.019471172719421448,
      "grad_norm": 1.8911653757095337,
      "learning_rate": 0.00036123567513702046,
      "loss": 0.2653,
      "step": 558
    },
    {
      "epoch": 0.019506067294187437,
      "grad_norm": 2.6411941051483154,
      "learning_rate": 0.0003609865470852018,
      "loss": 0.1695,
      "step": 559
    },
    {
      "epoch": 0.019540961868953426,
      "grad_norm": 10.14411449432373,
      "learning_rate": 0.00036073741903338316,
      "loss": 0.5714,
      "step": 560
    },
    {
      "epoch": 0.01957585644371941,
      "grad_norm": 1.4560235738754272,
      "learning_rate": 0.00036048829098156454,
      "loss": 0.2729,
      "step": 561
    },
    {
      "epoch": 0.0196107510184854,
      "grad_norm": 18.315885543823242,
      "learning_rate": 0.0003602391629297459,
      "loss": 0.9921,
      "step": 562
    },
    {
      "epoch": 0.01964564559325139,
      "grad_norm": 22.55074691772461,
      "learning_rate": 0.00035999003487792723,
      "loss": 0.5194,
      "step": 563
    },
    {
      "epoch": 0.019680540168017378,
      "grad_norm": 6.679548263549805,
      "learning_rate": 0.0003597409068261086,
      "loss": 0.4594,
      "step": 564
    },
    {
      "epoch": 0.019715434742783366,
      "grad_norm": 8.823063850402832,
      "learning_rate": 0.00035949177877429,
      "loss": 0.5161,
      "step": 565
    },
    {
      "epoch": 0.019750329317549355,
      "grad_norm": 1.6586267948150635,
      "learning_rate": 0.00035924265072247136,
      "loss": 0.3619,
      "step": 566
    },
    {
      "epoch": 0.01978522389231534,
      "grad_norm": 6.6055402755737305,
      "learning_rate": 0.00035899352267065274,
      "loss": 0.2368,
      "step": 567
    },
    {
      "epoch": 0.01982011846708133,
      "grad_norm": 6.083635330200195,
      "learning_rate": 0.00035874439461883406,
      "loss": 0.3082,
      "step": 568
    },
    {
      "epoch": 0.01985501304184732,
      "grad_norm": 3.518932580947876,
      "learning_rate": 0.0003584952665670155,
      "loss": 0.3893,
      "step": 569
    },
    {
      "epoch": 0.019889907616613307,
      "grad_norm": 1.739997148513794,
      "learning_rate": 0.0003582461385151968,
      "loss": 0.3448,
      "step": 570
    },
    {
      "epoch": 0.019924802191379296,
      "grad_norm": 1.5811350345611572,
      "learning_rate": 0.0003579970104633782,
      "loss": 0.4479,
      "step": 571
    },
    {
      "epoch": 0.019959696766145285,
      "grad_norm": 1.639626383781433,
      "learning_rate": 0.0003577478824115595,
      "loss": 0.2133,
      "step": 572
    },
    {
      "epoch": 0.01999459134091127,
      "grad_norm": 4.702783107757568,
      "learning_rate": 0.00035749875435974094,
      "loss": 0.2894,
      "step": 573
    },
    {
      "epoch": 0.02002948591567726,
      "grad_norm": 1.774538516998291,
      "learning_rate": 0.00035724962630792227,
      "loss": 0.4323,
      "step": 574
    },
    {
      "epoch": 0.020064380490443248,
      "grad_norm": 2.3338375091552734,
      "learning_rate": 0.00035700049825610364,
      "loss": 0.2291,
      "step": 575
    },
    {
      "epoch": 0.020099275065209237,
      "grad_norm": 3.557434558868408,
      "learning_rate": 0.000356751370204285,
      "loss": 0.3731,
      "step": 576
    },
    {
      "epoch": 0.020134169639975226,
      "grad_norm": 2.6993727684020996,
      "learning_rate": 0.00035650224215246634,
      "loss": 0.4519,
      "step": 577
    },
    {
      "epoch": 0.020169064214741215,
      "grad_norm": 2.6500656604766846,
      "learning_rate": 0.00035625311410064777,
      "loss": 0.4974,
      "step": 578
    },
    {
      "epoch": 0.0202039587895072,
      "grad_norm": 9.244566917419434,
      "learning_rate": 0.0003560039860488291,
      "loss": 0.5257,
      "step": 579
    },
    {
      "epoch": 0.02023885336427319,
      "grad_norm": 2.099393844604492,
      "learning_rate": 0.00035575485799701047,
      "loss": 0.3696,
      "step": 580
    },
    {
      "epoch": 0.020273747939039178,
      "grad_norm": 0.9486695528030396,
      "learning_rate": 0.0003555057299451918,
      "loss": 0.1574,
      "step": 581
    },
    {
      "epoch": 0.020308642513805167,
      "grad_norm": 1.361087679862976,
      "learning_rate": 0.0003552566018933732,
      "loss": 0.5092,
      "step": 582
    },
    {
      "epoch": 0.020343537088571156,
      "grad_norm": 1.1747695207595825,
      "learning_rate": 0.00035500747384155455,
      "loss": 0.1126,
      "step": 583
    },
    {
      "epoch": 0.02037843166333714,
      "grad_norm": 5.386180400848389,
      "learning_rate": 0.0003547583457897359,
      "loss": 0.4218,
      "step": 584
    },
    {
      "epoch": 0.02041332623810313,
      "grad_norm": 1.1232084035873413,
      "learning_rate": 0.0003545092177379173,
      "loss": 0.2966,
      "step": 585
    },
    {
      "epoch": 0.02044822081286912,
      "grad_norm": 1.2572822570800781,
      "learning_rate": 0.0003542600896860987,
      "loss": 0.2854,
      "step": 586
    },
    {
      "epoch": 0.020483115387635108,
      "grad_norm": 2.812361001968384,
      "learning_rate": 0.00035401096163428005,
      "loss": 0.316,
      "step": 587
    },
    {
      "epoch": 0.020518009962401097,
      "grad_norm": 3.126120090484619,
      "learning_rate": 0.0003537618335824614,
      "loss": 0.231,
      "step": 588
    },
    {
      "epoch": 0.020552904537167085,
      "grad_norm": 1.2670437097549438,
      "learning_rate": 0.00035351270553064275,
      "loss": 0.3461,
      "step": 589
    },
    {
      "epoch": 0.02058779911193307,
      "grad_norm": 1.8279285430908203,
      "learning_rate": 0.00035326357747882413,
      "loss": 0.2657,
      "step": 590
    },
    {
      "epoch": 0.02062269368669906,
      "grad_norm": 3.7893598079681396,
      "learning_rate": 0.0003530144494270055,
      "loss": 0.2225,
      "step": 591
    },
    {
      "epoch": 0.02065758826146505,
      "grad_norm": 2.9121196269989014,
      "learning_rate": 0.0003527653213751869,
      "loss": 0.3331,
      "step": 592
    },
    {
      "epoch": 0.020692482836231037,
      "grad_norm": 3.0076980590820312,
      "learning_rate": 0.0003525161933233682,
      "loss": 0.387,
      "step": 593
    },
    {
      "epoch": 0.020727377410997026,
      "grad_norm": 2.24845552444458,
      "learning_rate": 0.0003522670652715496,
      "loss": 0.3084,
      "step": 594
    },
    {
      "epoch": 0.020762271985763015,
      "grad_norm": 5.069072723388672,
      "learning_rate": 0.00035201793721973096,
      "loss": 0.3093,
      "step": 595
    },
    {
      "epoch": 0.020797166560529,
      "grad_norm": 4.257030487060547,
      "learning_rate": 0.00035176880916791233,
      "loss": 0.5636,
      "step": 596
    },
    {
      "epoch": 0.02083206113529499,
      "grad_norm": 1.3279060125350952,
      "learning_rate": 0.00035151968111609365,
      "loss": 0.2325,
      "step": 597
    },
    {
      "epoch": 0.02086695571006098,
      "grad_norm": 3.989525556564331,
      "learning_rate": 0.00035127055306427503,
      "loss": 0.5195,
      "step": 598
    },
    {
      "epoch": 0.020901850284826967,
      "grad_norm": 3.0642383098602295,
      "learning_rate": 0.0003510214250124564,
      "loss": 0.3601,
      "step": 599
    },
    {
      "epoch": 0.020936744859592956,
      "grad_norm": 3.1349477767944336,
      "learning_rate": 0.0003507722969606378,
      "loss": 0.3331,
      "step": 600
    },
    {
      "epoch": 0.02097163943435894,
      "grad_norm": 4.23461389541626,
      "learning_rate": 0.00035052316890881916,
      "loss": 0.3673,
      "step": 601
    },
    {
      "epoch": 0.02100653400912493,
      "grad_norm": 4.17230749130249,
      "learning_rate": 0.0003502740408570005,
      "loss": 0.2815,
      "step": 602
    },
    {
      "epoch": 0.02104142858389092,
      "grad_norm": 4.237703800201416,
      "learning_rate": 0.0003500249128051819,
      "loss": 0.3833,
      "step": 603
    },
    {
      "epoch": 0.021076323158656908,
      "grad_norm": 7.730746746063232,
      "learning_rate": 0.00034977578475336324,
      "loss": 0.5077,
      "step": 604
    },
    {
      "epoch": 0.021111217733422897,
      "grad_norm": 2.7680208683013916,
      "learning_rate": 0.0003495266567015446,
      "loss": 0.4548,
      "step": 605
    },
    {
      "epoch": 0.021146112308188886,
      "grad_norm": 5.246023178100586,
      "learning_rate": 0.00034927752864972593,
      "loss": 0.4049,
      "step": 606
    },
    {
      "epoch": 0.02118100688295487,
      "grad_norm": 5.873373985290527,
      "learning_rate": 0.00034902840059790736,
      "loss": 0.2588,
      "step": 607
    },
    {
      "epoch": 0.02121590145772086,
      "grad_norm": 10.189082145690918,
      "learning_rate": 0.0003487792725460887,
      "loss": 0.4616,
      "step": 608
    },
    {
      "epoch": 0.02125079603248685,
      "grad_norm": 1.8615832328796387,
      "learning_rate": 0.00034853014449427006,
      "loss": 0.3026,
      "step": 609
    },
    {
      "epoch": 0.021285690607252838,
      "grad_norm": 1.792844533920288,
      "learning_rate": 0.00034828101644245144,
      "loss": 0.3096,
      "step": 610
    },
    {
      "epoch": 0.021320585182018827,
      "grad_norm": 5.108480930328369,
      "learning_rate": 0.00034803188839063276,
      "loss": 0.3379,
      "step": 611
    },
    {
      "epoch": 0.021355479756784815,
      "grad_norm": 2.0777156352996826,
      "learning_rate": 0.0003477827603388142,
      "loss": 0.512,
      "step": 612
    },
    {
      "epoch": 0.0213903743315508,
      "grad_norm": 3.9187166690826416,
      "learning_rate": 0.0003475336322869955,
      "loss": 0.255,
      "step": 613
    },
    {
      "epoch": 0.02142526890631679,
      "grad_norm": 6.158716201782227,
      "learning_rate": 0.0003472845042351769,
      "loss": 0.1874,
      "step": 614
    },
    {
      "epoch": 0.02146016348108278,
      "grad_norm": 6.26355504989624,
      "learning_rate": 0.0003470353761833582,
      "loss": 0.3614,
      "step": 615
    },
    {
      "epoch": 0.021495058055848767,
      "grad_norm": 2.002490282058716,
      "learning_rate": 0.00034678624813153964,
      "loss": 0.2903,
      "step": 616
    },
    {
      "epoch": 0.021529952630614756,
      "grad_norm": 1.7439979314804077,
      "learning_rate": 0.00034653712007972097,
      "loss": 0.3606,
      "step": 617
    },
    {
      "epoch": 0.021564847205380742,
      "grad_norm": 1.0744338035583496,
      "learning_rate": 0.00034628799202790234,
      "loss": 0.2984,
      "step": 618
    },
    {
      "epoch": 0.02159974178014673,
      "grad_norm": 1.8973468542099,
      "learning_rate": 0.0003460388639760837,
      "loss": 0.42,
      "step": 619
    },
    {
      "epoch": 0.02163463635491272,
      "grad_norm": 2.934941053390503,
      "learning_rate": 0.0003457897359242651,
      "loss": 0.3937,
      "step": 620
    },
    {
      "epoch": 0.02166953092967871,
      "grad_norm": 2.9800667762756348,
      "learning_rate": 0.00034554060787244647,
      "loss": 0.542,
      "step": 621
    },
    {
      "epoch": 0.021704425504444697,
      "grad_norm": 4.167416572570801,
      "learning_rate": 0.0003452914798206278,
      "loss": 0.5108,
      "step": 622
    },
    {
      "epoch": 0.021739320079210686,
      "grad_norm": 6.18108606338501,
      "learning_rate": 0.00034504235176880917,
      "loss": 0.4058,
      "step": 623
    },
    {
      "epoch": 0.02177421465397667,
      "grad_norm": 5.296584606170654,
      "learning_rate": 0.00034479322371699055,
      "loss": 0.3315,
      "step": 624
    },
    {
      "epoch": 0.02180910922874266,
      "grad_norm": 3.470630645751953,
      "learning_rate": 0.0003445440956651719,
      "loss": 0.3699,
      "step": 625
    },
    {
      "epoch": 0.02184400380350865,
      "grad_norm": 3.2919859886169434,
      "learning_rate": 0.00034429496761335325,
      "loss": 0.4672,
      "step": 626
    },
    {
      "epoch": 0.021878898378274638,
      "grad_norm": 8.092644691467285,
      "learning_rate": 0.0003440458395615346,
      "loss": 0.4111,
      "step": 627
    },
    {
      "epoch": 0.021913792953040627,
      "grad_norm": 1.832289695739746,
      "learning_rate": 0.000343796711509716,
      "loss": 0.3749,
      "step": 628
    },
    {
      "epoch": 0.021948687527806616,
      "grad_norm": 1.2647342681884766,
      "learning_rate": 0.0003435475834578974,
      "loss": 0.1948,
      "step": 629
    },
    {
      "epoch": 0.0219835821025726,
      "grad_norm": 1.331463098526001,
      "learning_rate": 0.00034329845540607875,
      "loss": 0.2464,
      "step": 630
    },
    {
      "epoch": 0.02201847667733859,
      "grad_norm": 2.453634738922119,
      "learning_rate": 0.0003430493273542601,
      "loss": 0.1983,
      "step": 631
    },
    {
      "epoch": 0.02205337125210458,
      "grad_norm": 3.6106700897216797,
      "learning_rate": 0.00034280019930244145,
      "loss": 0.3559,
      "step": 632
    },
    {
      "epoch": 0.022088265826870568,
      "grad_norm": 3.783931255340576,
      "learning_rate": 0.00034255107125062283,
      "loss": 0.278,
      "step": 633
    },
    {
      "epoch": 0.022123160401636557,
      "grad_norm": 2.1058461666107178,
      "learning_rate": 0.0003423019431988042,
      "loss": 0.3512,
      "step": 634
    },
    {
      "epoch": 0.022158054976402542,
      "grad_norm": 2.5307586193084717,
      "learning_rate": 0.0003420528151469855,
      "loss": 0.2731,
      "step": 635
    },
    {
      "epoch": 0.02219294955116853,
      "grad_norm": 3.54140043258667,
      "learning_rate": 0.0003418036870951669,
      "loss": 0.2806,
      "step": 636
    },
    {
      "epoch": 0.02222784412593452,
      "grad_norm": 7.110762596130371,
      "learning_rate": 0.00034155455904334833,
      "loss": 0.334,
      "step": 637
    },
    {
      "epoch": 0.02226273870070051,
      "grad_norm": 5.767597675323486,
      "learning_rate": 0.00034130543099152966,
      "loss": 0.3519,
      "step": 638
    },
    {
      "epoch": 0.022297633275466498,
      "grad_norm": 2.450974225997925,
      "learning_rate": 0.00034105630293971103,
      "loss": 0.2521,
      "step": 639
    },
    {
      "epoch": 0.022332527850232486,
      "grad_norm": 3.164889097213745,
      "learning_rate": 0.00034080717488789235,
      "loss": 0.3804,
      "step": 640
    },
    {
      "epoch": 0.022367422424998472,
      "grad_norm": 2.832798480987549,
      "learning_rate": 0.0003405580468360738,
      "loss": 0.3032,
      "step": 641
    },
    {
      "epoch": 0.02240231699976446,
      "grad_norm": 2.761931896209717,
      "learning_rate": 0.0003403089187842551,
      "loss": 0.539,
      "step": 642
    },
    {
      "epoch": 0.02243721157453045,
      "grad_norm": 1.3288538455963135,
      "learning_rate": 0.0003400597907324365,
      "loss": 0.2991,
      "step": 643
    },
    {
      "epoch": 0.02247210614929644,
      "grad_norm": 3.217686891555786,
      "learning_rate": 0.0003398106626806178,
      "loss": 0.4267,
      "step": 644
    },
    {
      "epoch": 0.022507000724062427,
      "grad_norm": 5.529422760009766,
      "learning_rate": 0.0003395615346287992,
      "loss": 0.3487,
      "step": 645
    },
    {
      "epoch": 0.022541895298828416,
      "grad_norm": 2.7222137451171875,
      "learning_rate": 0.0003393124065769806,
      "loss": 0.4373,
      "step": 646
    },
    {
      "epoch": 0.0225767898735944,
      "grad_norm": 3.7781622409820557,
      "learning_rate": 0.00033906327852516193,
      "loss": 0.3722,
      "step": 647
    },
    {
      "epoch": 0.02261168444836039,
      "grad_norm": 2.7221267223358154,
      "learning_rate": 0.0003388141504733433,
      "loss": 0.2385,
      "step": 648
    },
    {
      "epoch": 0.02264657902312638,
      "grad_norm": 2.927757740020752,
      "learning_rate": 0.00033856502242152463,
      "loss": 0.2473,
      "step": 649
    },
    {
      "epoch": 0.022681473597892368,
      "grad_norm": 2.1373894214630127,
      "learning_rate": 0.00033831589436970606,
      "loss": 0.3076,
      "step": 650
    },
    {
      "epoch": 0.022716368172658357,
      "grad_norm": 2.4302000999450684,
      "learning_rate": 0.0003380667663178874,
      "loss": 0.3533,
      "step": 651
    },
    {
      "epoch": 0.022751262747424346,
      "grad_norm": 2.575313091278076,
      "learning_rate": 0.00033781763826606876,
      "loss": 0.2842,
      "step": 652
    },
    {
      "epoch": 0.02278615732219033,
      "grad_norm": 2.6343297958374023,
      "learning_rate": 0.00033756851021425014,
      "loss": 0.0974,
      "step": 653
    },
    {
      "epoch": 0.02282105189695632,
      "grad_norm": 1.9511232376098633,
      "learning_rate": 0.0003373193821624315,
      "loss": 0.2082,
      "step": 654
    },
    {
      "epoch": 0.02285594647172231,
      "grad_norm": 1.9897713661193848,
      "learning_rate": 0.0003370702541106129,
      "loss": 0.2297,
      "step": 655
    },
    {
      "epoch": 0.022890841046488298,
      "grad_norm": 3.4663844108581543,
      "learning_rate": 0.0003368211260587942,
      "loss": 0.3206,
      "step": 656
    },
    {
      "epoch": 0.022925735621254287,
      "grad_norm": 3.2620162963867188,
      "learning_rate": 0.0003365719980069756,
      "loss": 0.4211,
      "step": 657
    },
    {
      "epoch": 0.022960630196020272,
      "grad_norm": 4.115172386169434,
      "learning_rate": 0.00033632286995515697,
      "loss": 0.4115,
      "step": 658
    },
    {
      "epoch": 0.02299552477078626,
      "grad_norm": 4.359607219696045,
      "learning_rate": 0.00033607374190333834,
      "loss": 0.442,
      "step": 659
    },
    {
      "epoch": 0.02303041934555225,
      "grad_norm": 3.5848984718322754,
      "learning_rate": 0.00033582461385151967,
      "loss": 0.3877,
      "step": 660
    },
    {
      "epoch": 0.02306531392031824,
      "grad_norm": 2.009974956512451,
      "learning_rate": 0.00033557548579970104,
      "loss": 0.4487,
      "step": 661
    },
    {
      "epoch": 0.023100208495084228,
      "grad_norm": 2.237539291381836,
      "learning_rate": 0.0003353263577478824,
      "loss": 0.1555,
      "step": 662
    },
    {
      "epoch": 0.023135103069850217,
      "grad_norm": 1.3831578493118286,
      "learning_rate": 0.0003350772296960638,
      "loss": 0.155,
      "step": 663
    },
    {
      "epoch": 0.023169997644616202,
      "grad_norm": 1.0900741815567017,
      "learning_rate": 0.00033482810164424517,
      "loss": 0.2392,
      "step": 664
    },
    {
      "epoch": 0.02320489221938219,
      "grad_norm": 1.1436994075775146,
      "learning_rate": 0.0003345789735924265,
      "loss": 0.3013,
      "step": 665
    },
    {
      "epoch": 0.02323978679414818,
      "grad_norm": 3.290844678878784,
      "learning_rate": 0.00033432984554060787,
      "loss": 0.2378,
      "step": 666
    },
    {
      "epoch": 0.02327468136891417,
      "grad_norm": 3.680243730545044,
      "learning_rate": 0.00033408071748878925,
      "loss": 0.3388,
      "step": 667
    },
    {
      "epoch": 0.023309575943680157,
      "grad_norm": 1.4331519603729248,
      "learning_rate": 0.0003338315894369706,
      "loss": 0.3085,
      "step": 668
    },
    {
      "epoch": 0.023344470518446146,
      "grad_norm": 2.1216087341308594,
      "learning_rate": 0.00033358246138515195,
      "loss": 0.3031,
      "step": 669
    },
    {
      "epoch": 0.02337936509321213,
      "grad_norm": 3.2374818325042725,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.196,
      "step": 670
    },
    {
      "epoch": 0.02341425966797812,
      "grad_norm": 1.752645492553711,
      "learning_rate": 0.00033308420528151475,
      "loss": 0.3334,
      "step": 671
    },
    {
      "epoch": 0.02344915424274411,
      "grad_norm": 2.039491653442383,
      "learning_rate": 0.0003328350772296961,
      "loss": 0.4033,
      "step": 672
    },
    {
      "epoch": 0.023484048817510098,
      "grad_norm": 3.7351670265197754,
      "learning_rate": 0.00033258594917787745,
      "loss": 0.3297,
      "step": 673
    },
    {
      "epoch": 0.023518943392276087,
      "grad_norm": 5.18834114074707,
      "learning_rate": 0.0003323368211260588,
      "loss": 0.5929,
      "step": 674
    },
    {
      "epoch": 0.023553837967042072,
      "grad_norm": 2.298480749130249,
      "learning_rate": 0.0003320876930742402,
      "loss": 0.413,
      "step": 675
    },
    {
      "epoch": 0.02358873254180806,
      "grad_norm": 2.610377311706543,
      "learning_rate": 0.0003318385650224215,
      "loss": 0.5665,
      "step": 676
    },
    {
      "epoch": 0.02362362711657405,
      "grad_norm": 7.665168285369873,
      "learning_rate": 0.0003315894369706029,
      "loss": 0.287,
      "step": 677
    },
    {
      "epoch": 0.02365852169134004,
      "grad_norm": 2.6673760414123535,
      "learning_rate": 0.0003313403089187842,
      "loss": 0.2859,
      "step": 678
    },
    {
      "epoch": 0.023693416266106028,
      "grad_norm": 2.397425413131714,
      "learning_rate": 0.0003310911808669656,
      "loss": 0.3068,
      "step": 679
    },
    {
      "epoch": 0.023728310840872017,
      "grad_norm": 1.1271158456802368,
      "learning_rate": 0.00033084205281514703,
      "loss": 0.184,
      "step": 680
    },
    {
      "epoch": 0.023763205415638002,
      "grad_norm": 3.2987148761749268,
      "learning_rate": 0.00033059292476332835,
      "loss": 0.27,
      "step": 681
    },
    {
      "epoch": 0.02379809999040399,
      "grad_norm": 2.482330322265625,
      "learning_rate": 0.00033034379671150973,
      "loss": 0.2723,
      "step": 682
    },
    {
      "epoch": 0.02383299456516998,
      "grad_norm": 1.830000400543213,
      "learning_rate": 0.00033009466865969105,
      "loss": 0.2002,
      "step": 683
    },
    {
      "epoch": 0.02386788913993597,
      "grad_norm": 1.9116857051849365,
      "learning_rate": 0.0003298455406078725,
      "loss": 0.2225,
      "step": 684
    },
    {
      "epoch": 0.023902783714701958,
      "grad_norm": 2.050821304321289,
      "learning_rate": 0.0003295964125560538,
      "loss": 0.1415,
      "step": 685
    },
    {
      "epoch": 0.023937678289467947,
      "grad_norm": 1.4892690181732178,
      "learning_rate": 0.0003293472845042352,
      "loss": 0.1937,
      "step": 686
    },
    {
      "epoch": 0.023972572864233932,
      "grad_norm": 1.8834316730499268,
      "learning_rate": 0.0003290981564524165,
      "loss": 0.2467,
      "step": 687
    },
    {
      "epoch": 0.02400746743899992,
      "grad_norm": 2.2261910438537598,
      "learning_rate": 0.00032884902840059794,
      "loss": 0.306,
      "step": 688
    },
    {
      "epoch": 0.02404236201376591,
      "grad_norm": 8.456838607788086,
      "learning_rate": 0.0003285999003487793,
      "loss": 0.5407,
      "step": 689
    },
    {
      "epoch": 0.0240772565885319,
      "grad_norm": 6.827193260192871,
      "learning_rate": 0.00032835077229696063,
      "loss": 0.4609,
      "step": 690
    },
    {
      "epoch": 0.024112151163297887,
      "grad_norm": 2.821904182434082,
      "learning_rate": 0.000328101644245142,
      "loss": 0.3268,
      "step": 691
    },
    {
      "epoch": 0.024147045738063873,
      "grad_norm": 2.367281675338745,
      "learning_rate": 0.0003278525161933234,
      "loss": 0.2047,
      "step": 692
    },
    {
      "epoch": 0.02418194031282986,
      "grad_norm": 2.701221227645874,
      "learning_rate": 0.00032760338814150476,
      "loss": 0.2072,
      "step": 693
    },
    {
      "epoch": 0.02421683488759585,
      "grad_norm": 4.728122234344482,
      "learning_rate": 0.0003273542600896861,
      "loss": 0.3989,
      "step": 694
    },
    {
      "epoch": 0.02425172946236184,
      "grad_norm": 3.844294548034668,
      "learning_rate": 0.00032710513203786746,
      "loss": 0.472,
      "step": 695
    },
    {
      "epoch": 0.02428662403712783,
      "grad_norm": 10.03265380859375,
      "learning_rate": 0.0003268560039860488,
      "loss": 0.6991,
      "step": 696
    },
    {
      "epoch": 0.024321518611893817,
      "grad_norm": 3.2868082523345947,
      "learning_rate": 0.0003266068759342302,
      "loss": 0.4735,
      "step": 697
    },
    {
      "epoch": 0.024356413186659803,
      "grad_norm": 1.5512157678604126,
      "learning_rate": 0.0003263577478824116,
      "loss": 0.2527,
      "step": 698
    },
    {
      "epoch": 0.02439130776142579,
      "grad_norm": 3.0630242824554443,
      "learning_rate": 0.0003261086198305929,
      "loss": 0.2969,
      "step": 699
    },
    {
      "epoch": 0.02442620233619178,
      "grad_norm": 2.5595312118530273,
      "learning_rate": 0.0003258594917787743,
      "loss": 0.2224,
      "step": 700
    },
    {
      "epoch": 0.02446109691095777,
      "grad_norm": 2.025938034057617,
      "learning_rate": 0.00032561036372695567,
      "loss": 0.2706,
      "step": 701
    },
    {
      "epoch": 0.024495991485723758,
      "grad_norm": 2.5602478981018066,
      "learning_rate": 0.00032536123567513704,
      "loss": 0.3434,
      "step": 702
    },
    {
      "epoch": 0.024530886060489747,
      "grad_norm": 1.5720250606536865,
      "learning_rate": 0.00032511210762331837,
      "loss": 0.2364,
      "step": 703
    },
    {
      "epoch": 0.024565780635255732,
      "grad_norm": 5.3969502449035645,
      "learning_rate": 0.00032486297957149974,
      "loss": 0.2766,
      "step": 704
    },
    {
      "epoch": 0.02460067521002172,
      "grad_norm": 5.6760735511779785,
      "learning_rate": 0.00032461385151968117,
      "loss": 0.4365,
      "step": 705
    },
    {
      "epoch": 0.02463556978478771,
      "grad_norm": 2.4132461547851562,
      "learning_rate": 0.0003243647234678625,
      "loss": 0.2241,
      "step": 706
    },
    {
      "epoch": 0.0246704643595537,
      "grad_norm": 2.185450792312622,
      "learning_rate": 0.00032411559541604387,
      "loss": 0.2974,
      "step": 707
    },
    {
      "epoch": 0.024705358934319688,
      "grad_norm": 4.397924900054932,
      "learning_rate": 0.0003238664673642252,
      "loss": 0.4041,
      "step": 708
    },
    {
      "epoch": 0.024740253509085677,
      "grad_norm": 11.328474044799805,
      "learning_rate": 0.0003236173393124066,
      "loss": 0.4979,
      "step": 709
    },
    {
      "epoch": 0.024775148083851662,
      "grad_norm": 5.490121841430664,
      "learning_rate": 0.00032336821126058795,
      "loss": 0.5423,
      "step": 710
    },
    {
      "epoch": 0.02481004265861765,
      "grad_norm": 7.070296764373779,
      "learning_rate": 0.0003231190832087693,
      "loss": 0.2643,
      "step": 711
    },
    {
      "epoch": 0.02484493723338364,
      "grad_norm": 2.1602678298950195,
      "learning_rate": 0.00032286995515695065,
      "loss": 0.3786,
      "step": 712
    },
    {
      "epoch": 0.02487983180814963,
      "grad_norm": 2.501885414123535,
      "learning_rate": 0.000322620827105132,
      "loss": 0.1682,
      "step": 713
    },
    {
      "epoch": 0.024914726382915618,
      "grad_norm": 2.143596649169922,
      "learning_rate": 0.00032237169905331345,
      "loss": 0.3445,
      "step": 714
    },
    {
      "epoch": 0.024949620957681603,
      "grad_norm": 9.687849998474121,
      "learning_rate": 0.0003221225710014948,
      "loss": 0.4659,
      "step": 715
    },
    {
      "epoch": 0.024984515532447592,
      "grad_norm": 1.5837788581848145,
      "learning_rate": 0.00032187344294967615,
      "loss": 0.2045,
      "step": 716
    },
    {
      "epoch": 0.02501941010721358,
      "grad_norm": 1.6571018695831299,
      "learning_rate": 0.0003216243148978575,
      "loss": 0.2454,
      "step": 717
    },
    {
      "epoch": 0.02505430468197957,
      "grad_norm": 1.713047742843628,
      "learning_rate": 0.0003213751868460389,
      "loss": 0.1901,
      "step": 718
    },
    {
      "epoch": 0.02508919925674556,
      "grad_norm": 2.330137252807617,
      "learning_rate": 0.0003211260587942202,
      "loss": 0.2617,
      "step": 719
    },
    {
      "epoch": 0.025124093831511547,
      "grad_norm": 4.050676345825195,
      "learning_rate": 0.0003208769307424016,
      "loss": 0.2623,
      "step": 720
    },
    {
      "epoch": 0.025158988406277533,
      "grad_norm": 2.835487127304077,
      "learning_rate": 0.0003206278026905829,
      "loss": 0.3401,
      "step": 721
    },
    {
      "epoch": 0.02519388298104352,
      "grad_norm": 2.535912275314331,
      "learning_rate": 0.00032037867463876436,
      "loss": 0.261,
      "step": 722
    },
    {
      "epoch": 0.02522877755580951,
      "grad_norm": 3.239821672439575,
      "learning_rate": 0.00032012954658694573,
      "loss": 0.3818,
      "step": 723
    },
    {
      "epoch": 0.0252636721305755,
      "grad_norm": 2.657660722732544,
      "learning_rate": 0.00031988041853512705,
      "loss": 0.2078,
      "step": 724
    },
    {
      "epoch": 0.025298566705341488,
      "grad_norm": 3.7979159355163574,
      "learning_rate": 0.00031963129048330843,
      "loss": 0.1896,
      "step": 725
    },
    {
      "epoch": 0.025333461280107477,
      "grad_norm": 3.931283473968506,
      "learning_rate": 0.0003193821624314898,
      "loss": 0.3688,
      "step": 726
    },
    {
      "epoch": 0.025368355854873462,
      "grad_norm": 1.7216384410858154,
      "learning_rate": 0.0003191330343796712,
      "loss": 0.3134,
      "step": 727
    },
    {
      "epoch": 0.02540325042963945,
      "grad_norm": 6.295055866241455,
      "learning_rate": 0.0003188839063278525,
      "loss": 0.3839,
      "step": 728
    },
    {
      "epoch": 0.02543814500440544,
      "grad_norm": 11.039401054382324,
      "learning_rate": 0.0003186347782760339,
      "loss": 0.3387,
      "step": 729
    },
    {
      "epoch": 0.02547303957917143,
      "grad_norm": 6.111503601074219,
      "learning_rate": 0.0003183856502242152,
      "loss": 0.1976,
      "step": 730
    },
    {
      "epoch": 0.025507934153937418,
      "grad_norm": 1.5562934875488281,
      "learning_rate": 0.00031813652217239664,
      "loss": 0.2089,
      "step": 731
    },
    {
      "epoch": 0.025542828728703403,
      "grad_norm": 3.9004955291748047,
      "learning_rate": 0.000317887394120578,
      "loss": 0.4721,
      "step": 732
    },
    {
      "epoch": 0.025577723303469392,
      "grad_norm": 6.961771488189697,
      "learning_rate": 0.00031763826606875933,
      "loss": 0.352,
      "step": 733
    },
    {
      "epoch": 0.02561261787823538,
      "grad_norm": 2.405611753463745,
      "learning_rate": 0.0003173891380169407,
      "loss": 0.2765,
      "step": 734
    },
    {
      "epoch": 0.02564751245300137,
      "grad_norm": 2.615900993347168,
      "learning_rate": 0.0003171400099651221,
      "loss": 0.2741,
      "step": 735
    },
    {
      "epoch": 0.02568240702776736,
      "grad_norm": 4.056253910064697,
      "learning_rate": 0.00031689088191330346,
      "loss": 0.2229,
      "step": 736
    },
    {
      "epoch": 0.025717301602533348,
      "grad_norm": 1.5085225105285645,
      "learning_rate": 0.0003166417538614848,
      "loss": 0.1791,
      "step": 737
    },
    {
      "epoch": 0.025752196177299333,
      "grad_norm": 4.149974822998047,
      "learning_rate": 0.00031639262580966616,
      "loss": 0.3389,
      "step": 738
    },
    {
      "epoch": 0.025787090752065322,
      "grad_norm": 2.1858229637145996,
      "learning_rate": 0.00031614349775784754,
      "loss": 0.1458,
      "step": 739
    },
    {
      "epoch": 0.02582198532683131,
      "grad_norm": 3.6772704124450684,
      "learning_rate": 0.0003158943697060289,
      "loss": 0.2523,
      "step": 740
    },
    {
      "epoch": 0.0258568799015973,
      "grad_norm": 3.7039852142333984,
      "learning_rate": 0.0003156452416542103,
      "loss": 0.5234,
      "step": 741
    },
    {
      "epoch": 0.02589177447636329,
      "grad_norm": 10.24194049835205,
      "learning_rate": 0.0003153961136023916,
      "loss": 0.8809,
      "step": 742
    },
    {
      "epoch": 0.025926669051129277,
      "grad_norm": 8.793864250183105,
      "learning_rate": 0.00031514698555057304,
      "loss": 0.5382,
      "step": 743
    },
    {
      "epoch": 0.025961563625895263,
      "grad_norm": 2.305143356323242,
      "learning_rate": 0.00031489785749875437,
      "loss": 0.3206,
      "step": 744
    },
    {
      "epoch": 0.02599645820066125,
      "grad_norm": 7.1719069480896,
      "learning_rate": 0.00031464872944693574,
      "loss": 0.4487,
      "step": 745
    },
    {
      "epoch": 0.02603135277542724,
      "grad_norm": 5.942006587982178,
      "learning_rate": 0.00031439960139511707,
      "loss": 0.3751,
      "step": 746
    },
    {
      "epoch": 0.02606624735019323,
      "grad_norm": 5.221052169799805,
      "learning_rate": 0.00031415047334329844,
      "loss": 0.5046,
      "step": 747
    },
    {
      "epoch": 0.026101141924959218,
      "grad_norm": 2.3374409675598145,
      "learning_rate": 0.0003139013452914798,
      "loss": 0.2544,
      "step": 748
    },
    {
      "epoch": 0.026136036499725204,
      "grad_norm": 7.759943962097168,
      "learning_rate": 0.0003136522172396612,
      "loss": 0.396,
      "step": 749
    },
    {
      "epoch": 0.026170931074491192,
      "grad_norm": 2.7929844856262207,
      "learning_rate": 0.00031340308918784257,
      "loss": 0.182,
      "step": 750
    },
    {
      "epoch": 0.02620582564925718,
      "grad_norm": 2.0696535110473633,
      "learning_rate": 0.0003131539611360239,
      "loss": 0.1564,
      "step": 751
    },
    {
      "epoch": 0.02624072022402317,
      "grad_norm": 2.85709285736084,
      "learning_rate": 0.0003129048330842053,
      "loss": 0.3114,
      "step": 752
    },
    {
      "epoch": 0.02627561479878916,
      "grad_norm": 4.839419364929199,
      "learning_rate": 0.00031265570503238665,
      "loss": 0.2684,
      "step": 753
    },
    {
      "epoch": 0.026310509373555148,
      "grad_norm": 4.353470325469971,
      "learning_rate": 0.000312406576980568,
      "loss": 0.5108,
      "step": 754
    },
    {
      "epoch": 0.026345403948321133,
      "grad_norm": 4.04958963394165,
      "learning_rate": 0.00031215744892874934,
      "loss": 0.3073,
      "step": 755
    },
    {
      "epoch": 0.026380298523087122,
      "grad_norm": 4.826739311218262,
      "learning_rate": 0.0003119083208769308,
      "loss": 0.4276,
      "step": 756
    },
    {
      "epoch": 0.02641519309785311,
      "grad_norm": 1.7615821361541748,
      "learning_rate": 0.00031165919282511215,
      "loss": 0.179,
      "step": 757
    },
    {
      "epoch": 0.0264500876726191,
      "grad_norm": 1.8376915454864502,
      "learning_rate": 0.0003114100647732935,
      "loss": 0.1939,
      "step": 758
    },
    {
      "epoch": 0.02648498224738509,
      "grad_norm": 2.325754404067993,
      "learning_rate": 0.00031116093672147485,
      "loss": 0.1409,
      "step": 759
    },
    {
      "epoch": 0.026519876822151078,
      "grad_norm": 1.6402508020401,
      "learning_rate": 0.0003109118086696562,
      "loss": 0.1547,
      "step": 760
    },
    {
      "epoch": 0.026554771396917063,
      "grad_norm": 2.645371437072754,
      "learning_rate": 0.0003106626806178376,
      "loss": 0.2248,
      "step": 761
    },
    {
      "epoch": 0.026589665971683052,
      "grad_norm": 2.483750581741333,
      "learning_rate": 0.0003104135525660189,
      "loss": 0.1515,
      "step": 762
    },
    {
      "epoch": 0.02662456054644904,
      "grad_norm": 1.9378080368041992,
      "learning_rate": 0.0003101644245142003,
      "loss": 0.2156,
      "step": 763
    },
    {
      "epoch": 0.02665945512121503,
      "grad_norm": 2.1537253856658936,
      "learning_rate": 0.0003099152964623816,
      "loss": 0.2839,
      "step": 764
    },
    {
      "epoch": 0.02669434969598102,
      "grad_norm": 4.8975443840026855,
      "learning_rate": 0.00030966616841056306,
      "loss": 0.3087,
      "step": 765
    },
    {
      "epoch": 0.026729244270747004,
      "grad_norm": 3.3810627460479736,
      "learning_rate": 0.00030941704035874443,
      "loss": 0.3562,
      "step": 766
    },
    {
      "epoch": 0.026764138845512993,
      "grad_norm": 1.7168638706207275,
      "learning_rate": 0.00030916791230692575,
      "loss": 0.2011,
      "step": 767
    },
    {
      "epoch": 0.02679903342027898,
      "grad_norm": 1.2877894639968872,
      "learning_rate": 0.00030891878425510713,
      "loss": 0.0908,
      "step": 768
    },
    {
      "epoch": 0.02683392799504497,
      "grad_norm": 2.7446696758270264,
      "learning_rate": 0.0003086696562032885,
      "loss": 0.2906,
      "step": 769
    },
    {
      "epoch": 0.02686882256981096,
      "grad_norm": 1.0014753341674805,
      "learning_rate": 0.0003084205281514699,
      "loss": 0.1298,
      "step": 770
    },
    {
      "epoch": 0.026903717144576948,
      "grad_norm": 3.5644888877868652,
      "learning_rate": 0.0003081714000996512,
      "loss": 0.3845,
      "step": 771
    },
    {
      "epoch": 0.026938611719342934,
      "grad_norm": 4.326469898223877,
      "learning_rate": 0.0003079222720478326,
      "loss": 0.2741,
      "step": 772
    },
    {
      "epoch": 0.026973506294108923,
      "grad_norm": 5.96442985534668,
      "learning_rate": 0.00030767314399601396,
      "loss": 0.3209,
      "step": 773
    },
    {
      "epoch": 0.02700840086887491,
      "grad_norm": 3.4287569522857666,
      "learning_rate": 0.00030742401594419533,
      "loss": 0.1701,
      "step": 774
    },
    {
      "epoch": 0.0270432954436409,
      "grad_norm": 6.134241580963135,
      "learning_rate": 0.0003071748878923767,
      "loss": 0.5162,
      "step": 775
    },
    {
      "epoch": 0.02707819001840689,
      "grad_norm": 3.284632921218872,
      "learning_rate": 0.00030692575984055803,
      "loss": 0.3217,
      "step": 776
    },
    {
      "epoch": 0.027113084593172878,
      "grad_norm": 2.321606159210205,
      "learning_rate": 0.0003066766317887394,
      "loss": 0.1911,
      "step": 777
    },
    {
      "epoch": 0.027147979167938863,
      "grad_norm": 6.016180038452148,
      "learning_rate": 0.0003064275037369208,
      "loss": 0.3533,
      "step": 778
    },
    {
      "epoch": 0.027182873742704852,
      "grad_norm": 5.074009418487549,
      "learning_rate": 0.00030617837568510216,
      "loss": 0.4354,
      "step": 779
    },
    {
      "epoch": 0.02721776831747084,
      "grad_norm": 2.0447781085968018,
      "learning_rate": 0.0003059292476332835,
      "loss": 0.1714,
      "step": 780
    },
    {
      "epoch": 0.02725266289223683,
      "grad_norm": 3.864290237426758,
      "learning_rate": 0.00030568011958146486,
      "loss": 0.3078,
      "step": 781
    },
    {
      "epoch": 0.02728755746700282,
      "grad_norm": 2.7540125846862793,
      "learning_rate": 0.00030543099152964624,
      "loss": 0.1946,
      "step": 782
    },
    {
      "epoch": 0.027322452041768808,
      "grad_norm": 1.9870729446411133,
      "learning_rate": 0.0003051818634778276,
      "loss": 0.2073,
      "step": 783
    },
    {
      "epoch": 0.027357346616534793,
      "grad_norm": 3.0809226036071777,
      "learning_rate": 0.000304932735426009,
      "loss": 0.2585,
      "step": 784
    },
    {
      "epoch": 0.027392241191300782,
      "grad_norm": 2.227219820022583,
      "learning_rate": 0.0003046836073741903,
      "loss": 0.1464,
      "step": 785
    },
    {
      "epoch": 0.02742713576606677,
      "grad_norm": 2.894806146621704,
      "learning_rate": 0.00030443447932237174,
      "loss": 0.3986,
      "step": 786
    },
    {
      "epoch": 0.02746203034083276,
      "grad_norm": 5.39146089553833,
      "learning_rate": 0.00030418535127055307,
      "loss": 0.1969,
      "step": 787
    },
    {
      "epoch": 0.02749692491559875,
      "grad_norm": 2.6692888736724854,
      "learning_rate": 0.00030393622321873444,
      "loss": 0.1016,
      "step": 788
    },
    {
      "epoch": 0.027531819490364734,
      "grad_norm": 2.9018771648406982,
      "learning_rate": 0.00030368709516691576,
      "loss": 0.2339,
      "step": 789
    },
    {
      "epoch": 0.027566714065130723,
      "grad_norm": 1.5505341291427612,
      "learning_rate": 0.0003034379671150972,
      "loss": 0.188,
      "step": 790
    },
    {
      "epoch": 0.02760160863989671,
      "grad_norm": 4.400171279907227,
      "learning_rate": 0.0003031888390632785,
      "loss": 0.3573,
      "step": 791
    },
    {
      "epoch": 0.0276365032146627,
      "grad_norm": 1.8474059104919434,
      "learning_rate": 0.0003029397110114599,
      "loss": 0.166,
      "step": 792
    },
    {
      "epoch": 0.02767139778942869,
      "grad_norm": 2.2197179794311523,
      "learning_rate": 0.00030269058295964127,
      "loss": 0.165,
      "step": 793
    },
    {
      "epoch": 0.02770629236419468,
      "grad_norm": 2.9597113132476807,
      "learning_rate": 0.0003024414549078226,
      "loss": 0.2819,
      "step": 794
    },
    {
      "epoch": 0.027741186938960664,
      "grad_norm": 2.1209657192230225,
      "learning_rate": 0.000302192326856004,
      "loss": 0.2224,
      "step": 795
    },
    {
      "epoch": 0.027776081513726653,
      "grad_norm": 4.878491401672363,
      "learning_rate": 0.00030194319880418535,
      "loss": 0.4716,
      "step": 796
    },
    {
      "epoch": 0.02781097608849264,
      "grad_norm": 2.292407751083374,
      "learning_rate": 0.0003016940707523667,
      "loss": 0.2718,
      "step": 797
    },
    {
      "epoch": 0.02784587066325863,
      "grad_norm": 2.9135079383850098,
      "learning_rate": 0.00030144494270054804,
      "loss": 0.4304,
      "step": 798
    },
    {
      "epoch": 0.02788076523802462,
      "grad_norm": 2.352809190750122,
      "learning_rate": 0.0003011958146487295,
      "loss": 0.2389,
      "step": 799
    },
    {
      "epoch": 0.027915659812790608,
      "grad_norm": 2.220975637435913,
      "learning_rate": 0.0003009466865969108,
      "loss": 0.2806,
      "step": 800
    },
    {
      "epoch": 0.027950554387556593,
      "grad_norm": 6.401760578155518,
      "learning_rate": 0.0003006975585450922,
      "loss": 0.4746,
      "step": 801
    },
    {
      "epoch": 0.027985448962322582,
      "grad_norm": 2.1579062938690186,
      "learning_rate": 0.00030044843049327355,
      "loss": 0.2176,
      "step": 802
    },
    {
      "epoch": 0.02802034353708857,
      "grad_norm": 2.789876699447632,
      "learning_rate": 0.0003001993024414549,
      "loss": 0.2703,
      "step": 803
    },
    {
      "epoch": 0.02805523811185456,
      "grad_norm": 2.6919567584991455,
      "learning_rate": 0.0002999501743896363,
      "loss": 0.2539,
      "step": 804
    },
    {
      "epoch": 0.02809013268662055,
      "grad_norm": 4.147339344024658,
      "learning_rate": 0.0002997010463378176,
      "loss": 0.2637,
      "step": 805
    },
    {
      "epoch": 0.028125027261386534,
      "grad_norm": 2.401573419570923,
      "learning_rate": 0.000299451918285999,
      "loss": 0.2551,
      "step": 806
    },
    {
      "epoch": 0.028159921836152523,
      "grad_norm": 4.39707612991333,
      "learning_rate": 0.0002992027902341804,
      "loss": 0.5054,
      "step": 807
    },
    {
      "epoch": 0.028194816410918512,
      "grad_norm": 1.3790735006332397,
      "learning_rate": 0.00029895366218236175,
      "loss": 0.0866,
      "step": 808
    },
    {
      "epoch": 0.0282297109856845,
      "grad_norm": 5.187991619110107,
      "learning_rate": 0.0002987045341305431,
      "loss": 0.3003,
      "step": 809
    },
    {
      "epoch": 0.02826460556045049,
      "grad_norm": 5.353060245513916,
      "learning_rate": 0.00029845540607872445,
      "loss": 0.2018,
      "step": 810
    },
    {
      "epoch": 0.02829950013521648,
      "grad_norm": 4.039761543273926,
      "learning_rate": 0.00029820627802690583,
      "loss": 0.3221,
      "step": 811
    },
    {
      "epoch": 0.028334394709982464,
      "grad_norm": 4.606817722320557,
      "learning_rate": 0.0002979571499750872,
      "loss": 0.2596,
      "step": 812
    },
    {
      "epoch": 0.028369289284748453,
      "grad_norm": 6.1632537841796875,
      "learning_rate": 0.0002977080219232686,
      "loss": 0.4722,
      "step": 813
    },
    {
      "epoch": 0.028404183859514442,
      "grad_norm": 3.325164794921875,
      "learning_rate": 0.0002974588938714499,
      "loss": 0.1901,
      "step": 814
    },
    {
      "epoch": 0.02843907843428043,
      "grad_norm": 2.7397072315216064,
      "learning_rate": 0.0002972097658196313,
      "loss": 0.2433,
      "step": 815
    },
    {
      "epoch": 0.02847397300904642,
      "grad_norm": 2.3919849395751953,
      "learning_rate": 0.00029696063776781266,
      "loss": 0.1557,
      "step": 816
    },
    {
      "epoch": 0.02850886758381241,
      "grad_norm": 1.413365364074707,
      "learning_rate": 0.00029671150971599403,
      "loss": 0.0741,
      "step": 817
    },
    {
      "epoch": 0.028543762158578394,
      "grad_norm": 3.1317098140716553,
      "learning_rate": 0.0002964623816641754,
      "loss": 0.1797,
      "step": 818
    },
    {
      "epoch": 0.028578656733344383,
      "grad_norm": 2.292738676071167,
      "learning_rate": 0.00029621325361235673,
      "loss": 0.1301,
      "step": 819
    },
    {
      "epoch": 0.02861355130811037,
      "grad_norm": 2.2308390140533447,
      "learning_rate": 0.00029596412556053816,
      "loss": 0.1446,
      "step": 820
    },
    {
      "epoch": 0.02864844588287636,
      "grad_norm": 4.037461757659912,
      "learning_rate": 0.0002957149975087195,
      "loss": 0.2782,
      "step": 821
    },
    {
      "epoch": 0.02868334045764235,
      "grad_norm": 1.9875390529632568,
      "learning_rate": 0.00029546586945690086,
      "loss": 0.1166,
      "step": 822
    },
    {
      "epoch": 0.028718235032408335,
      "grad_norm": 2.9019033908843994,
      "learning_rate": 0.0002952167414050822,
      "loss": 0.1657,
      "step": 823
    },
    {
      "epoch": 0.028753129607174324,
      "grad_norm": 6.419712543487549,
      "learning_rate": 0.0002949676133532636,
      "loss": 0.3312,
      "step": 824
    },
    {
      "epoch": 0.028788024181940312,
      "grad_norm": 2.907097578048706,
      "learning_rate": 0.00029471848530144494,
      "loss": 0.1218,
      "step": 825
    },
    {
      "epoch": 0.0288229187567063,
      "grad_norm": 3.902721643447876,
      "learning_rate": 0.0002944693572496263,
      "loss": 0.1028,
      "step": 826
    },
    {
      "epoch": 0.02885781333147229,
      "grad_norm": 5.968652725219727,
      "learning_rate": 0.0002942202291978077,
      "loss": 0.3637,
      "step": 827
    },
    {
      "epoch": 0.02889270790623828,
      "grad_norm": 3.825397253036499,
      "learning_rate": 0.000293971101145989,
      "loss": 0.2203,
      "step": 828
    },
    {
      "epoch": 0.028927602481004264,
      "grad_norm": 3.8462157249450684,
      "learning_rate": 0.00029372197309417044,
      "loss": 0.1994,
      "step": 829
    },
    {
      "epoch": 0.028962497055770253,
      "grad_norm": 5.624104976654053,
      "learning_rate": 0.00029347284504235177,
      "loss": 0.2953,
      "step": 830
    },
    {
      "epoch": 0.028997391630536242,
      "grad_norm": 2.1753244400024414,
      "learning_rate": 0.00029322371699053314,
      "loss": 0.3162,
      "step": 831
    },
    {
      "epoch": 0.02903228620530223,
      "grad_norm": 4.283011436462402,
      "learning_rate": 0.00029297458893871446,
      "loss": 0.1191,
      "step": 832
    },
    {
      "epoch": 0.02906718078006822,
      "grad_norm": 4.578165531158447,
      "learning_rate": 0.0002927254608868959,
      "loss": 0.3289,
      "step": 833
    },
    {
      "epoch": 0.02910207535483421,
      "grad_norm": 4.468640327453613,
      "learning_rate": 0.0002924763328350772,
      "loss": 0.3159,
      "step": 834
    },
    {
      "epoch": 0.029136969929600194,
      "grad_norm": 3.974701166152954,
      "learning_rate": 0.0002922272047832586,
      "loss": 0.3218,
      "step": 835
    },
    {
      "epoch": 0.029171864504366183,
      "grad_norm": 4.2350850105285645,
      "learning_rate": 0.00029197807673143997,
      "loss": 0.2458,
      "step": 836
    },
    {
      "epoch": 0.029206759079132172,
      "grad_norm": 4.936691761016846,
      "learning_rate": 0.00029172894867962135,
      "loss": 0.3598,
      "step": 837
    },
    {
      "epoch": 0.02924165365389816,
      "grad_norm": 4.092573642730713,
      "learning_rate": 0.0002914798206278027,
      "loss": 0.2041,
      "step": 838
    },
    {
      "epoch": 0.02927654822866415,
      "grad_norm": 1.9469197988510132,
      "learning_rate": 0.00029123069257598405,
      "loss": 0.1869,
      "step": 839
    },
    {
      "epoch": 0.029311442803430135,
      "grad_norm": 4.420064449310303,
      "learning_rate": 0.0002909815645241654,
      "loss": 0.3601,
      "step": 840
    },
    {
      "epoch": 0.029346337378196124,
      "grad_norm": 3.012151002883911,
      "learning_rate": 0.0002907324364723468,
      "loss": 0.3151,
      "step": 841
    },
    {
      "epoch": 0.029381231952962113,
      "grad_norm": 3.20874285697937,
      "learning_rate": 0.0002904833084205282,
      "loss": 0.249,
      "step": 842
    },
    {
      "epoch": 0.0294161265277281,
      "grad_norm": 5.756889343261719,
      "learning_rate": 0.0002902341803687095,
      "loss": 0.3682,
      "step": 843
    },
    {
      "epoch": 0.02945102110249409,
      "grad_norm": 4.019587516784668,
      "learning_rate": 0.0002899850523168909,
      "loss": 0.283,
      "step": 844
    },
    {
      "epoch": 0.02948591567726008,
      "grad_norm": 5.221162796020508,
      "learning_rate": 0.00028973592426507225,
      "loss": 0.2711,
      "step": 845
    },
    {
      "epoch": 0.029520810252026065,
      "grad_norm": 4.923125743865967,
      "learning_rate": 0.0002894867962132536,
      "loss": 0.3608,
      "step": 846
    },
    {
      "epoch": 0.029555704826792054,
      "grad_norm": 5.334845542907715,
      "learning_rate": 0.000289237668161435,
      "loss": 0.2875,
      "step": 847
    },
    {
      "epoch": 0.029590599401558042,
      "grad_norm": 2.4749152660369873,
      "learning_rate": 0.0002889885401096163,
      "loss": 0.1662,
      "step": 848
    },
    {
      "epoch": 0.02962549397632403,
      "grad_norm": 1.9123965501785278,
      "learning_rate": 0.0002887394120577977,
      "loss": 0.1742,
      "step": 849
    },
    {
      "epoch": 0.02966038855109002,
      "grad_norm": 4.829747200012207,
      "learning_rate": 0.0002884902840059791,
      "loss": 0.3362,
      "step": 850
    },
    {
      "epoch": 0.02969528312585601,
      "grad_norm": 6.510564804077148,
      "learning_rate": 0.00028824115595416045,
      "loss": 0.4603,
      "step": 851
    },
    {
      "epoch": 0.029730177700621994,
      "grad_norm": 3.197706699371338,
      "learning_rate": 0.0002879920279023418,
      "loss": 0.244,
      "step": 852
    },
    {
      "epoch": 0.029765072275387983,
      "grad_norm": 2.6206259727478027,
      "learning_rate": 0.00028774289985052315,
      "loss": 0.2281,
      "step": 853
    },
    {
      "epoch": 0.029799966850153972,
      "grad_norm": 5.837817668914795,
      "learning_rate": 0.0002874937717987046,
      "loss": 0.3987,
      "step": 854
    },
    {
      "epoch": 0.02983486142491996,
      "grad_norm": 1.4652819633483887,
      "learning_rate": 0.0002872446437468859,
      "loss": 0.162,
      "step": 855
    },
    {
      "epoch": 0.02986975599968595,
      "grad_norm": 3.826664447784424,
      "learning_rate": 0.0002869955156950673,
      "loss": 0.2645,
      "step": 856
    },
    {
      "epoch": 0.02990465057445194,
      "grad_norm": 3.2972402572631836,
      "learning_rate": 0.0002867463876432486,
      "loss": 0.2154,
      "step": 857
    },
    {
      "epoch": 0.029939545149217924,
      "grad_norm": 3.5130279064178467,
      "learning_rate": 0.00028649725959143004,
      "loss": 0.3397,
      "step": 858
    },
    {
      "epoch": 0.029974439723983913,
      "grad_norm": 8.167996406555176,
      "learning_rate": 0.00028624813153961136,
      "loss": 0.3319,
      "step": 859
    },
    {
      "epoch": 0.030009334298749902,
      "grad_norm": 2.798799991607666,
      "learning_rate": 0.00028599900348779273,
      "loss": 0.2735,
      "step": 860
    },
    {
      "epoch": 0.03004422887351589,
      "grad_norm": 1.9901691675186157,
      "learning_rate": 0.00028574987543597406,
      "loss": 0.2343,
      "step": 861
    },
    {
      "epoch": 0.03007912344828188,
      "grad_norm": 3.7077407836914062,
      "learning_rate": 0.00028550074738415543,
      "loss": 0.253,
      "step": 862
    },
    {
      "epoch": 0.030114018023047865,
      "grad_norm": 5.435625076293945,
      "learning_rate": 0.00028525161933233686,
      "loss": 0.3072,
      "step": 863
    },
    {
      "epoch": 0.030148912597813854,
      "grad_norm": 7.929316997528076,
      "learning_rate": 0.0002850024912805182,
      "loss": 0.3937,
      "step": 864
    },
    {
      "epoch": 0.030183807172579843,
      "grad_norm": 7.804164886474609,
      "learning_rate": 0.00028475336322869956,
      "loss": 0.5356,
      "step": 865
    },
    {
      "epoch": 0.03021870174734583,
      "grad_norm": 5.663996696472168,
      "learning_rate": 0.0002845042351768809,
      "loss": 0.1916,
      "step": 866
    },
    {
      "epoch": 0.03025359632211182,
      "grad_norm": 8.091291427612305,
      "learning_rate": 0.0002842551071250623,
      "loss": 0.088,
      "step": 867
    },
    {
      "epoch": 0.03028849089687781,
      "grad_norm": 3.0091092586517334,
      "learning_rate": 0.00028400597907324364,
      "loss": 0.0918,
      "step": 868
    },
    {
      "epoch": 0.030323385471643795,
      "grad_norm": 4.8800458908081055,
      "learning_rate": 0.000283756851021425,
      "loss": 0.1827,
      "step": 869
    },
    {
      "epoch": 0.030358280046409784,
      "grad_norm": 6.4183430671691895,
      "learning_rate": 0.0002835077229696064,
      "loss": 0.3066,
      "step": 870
    },
    {
      "epoch": 0.030393174621175773,
      "grad_norm": 6.667297840118408,
      "learning_rate": 0.00028325859491778777,
      "loss": 0.354,
      "step": 871
    },
    {
      "epoch": 0.03042806919594176,
      "grad_norm": 4.428797245025635,
      "learning_rate": 0.00028300946686596914,
      "loss": 0.2453,
      "step": 872
    },
    {
      "epoch": 0.03046296377070775,
      "grad_norm": 4.186694145202637,
      "learning_rate": 0.00028276033881415047,
      "loss": 0.1683,
      "step": 873
    },
    {
      "epoch": 0.03049785834547374,
      "grad_norm": 5.215307712554932,
      "learning_rate": 0.00028251121076233184,
      "loss": 0.1404,
      "step": 874
    },
    {
      "epoch": 0.030532752920239725,
      "grad_norm": 2.7991831302642822,
      "learning_rate": 0.0002822620827105132,
      "loss": 0.1505,
      "step": 875
    },
    {
      "epoch": 0.030567647495005713,
      "grad_norm": 7.265015602111816,
      "learning_rate": 0.0002820129546586946,
      "loss": 0.2547,
      "step": 876
    },
    {
      "epoch": 0.030602542069771702,
      "grad_norm": 3.472608804702759,
      "learning_rate": 0.0002817638266068759,
      "loss": 0.1453,
      "step": 877
    },
    {
      "epoch": 0.03063743664453769,
      "grad_norm": 4.9762420654296875,
      "learning_rate": 0.0002815146985550573,
      "loss": 0.1174,
      "step": 878
    },
    {
      "epoch": 0.03067233121930368,
      "grad_norm": 2.3469440937042236,
      "learning_rate": 0.00028126557050323867,
      "loss": 0.1595,
      "step": 879
    },
    {
      "epoch": 0.030707225794069665,
      "grad_norm": 5.067986488342285,
      "learning_rate": 0.00028101644245142005,
      "loss": 0.1932,
      "step": 880
    },
    {
      "epoch": 0.030742120368835654,
      "grad_norm": 3.7897634506225586,
      "learning_rate": 0.0002807673143996014,
      "loss": 0.3441,
      "step": 881
    },
    {
      "epoch": 0.030777014943601643,
      "grad_norm": 5.13813591003418,
      "learning_rate": 0.00028051818634778274,
      "loss": 0.2301,
      "step": 882
    },
    {
      "epoch": 0.030811909518367632,
      "grad_norm": 5.217123508453369,
      "learning_rate": 0.0002802690582959641,
      "loss": 0.4367,
      "step": 883
    },
    {
      "epoch": 0.03084680409313362,
      "grad_norm": 4.28157901763916,
      "learning_rate": 0.0002800199302441455,
      "loss": 0.2113,
      "step": 884
    },
    {
      "epoch": 0.03088169866789961,
      "grad_norm": 1.6790125370025635,
      "learning_rate": 0.0002797708021923269,
      "loss": 0.2251,
      "step": 885
    },
    {
      "epoch": 0.030916593242665595,
      "grad_norm": 1.2450698614120483,
      "learning_rate": 0.0002795216741405082,
      "loss": 0.0609,
      "step": 886
    },
    {
      "epoch": 0.030951487817431584,
      "grad_norm": 3.5743086338043213,
      "learning_rate": 0.0002792725460886896,
      "loss": 0.1641,
      "step": 887
    },
    {
      "epoch": 0.030986382392197573,
      "grad_norm": 3.4173059463500977,
      "learning_rate": 0.000279023418036871,
      "loss": 0.2511,
      "step": 888
    },
    {
      "epoch": 0.03102127696696356,
      "grad_norm": 2.7398948669433594,
      "learning_rate": 0.0002787742899850523,
      "loss": 0.232,
      "step": 889
    },
    {
      "epoch": 0.03105617154172955,
      "grad_norm": 5.207468509674072,
      "learning_rate": 0.0002785251619332337,
      "loss": 0.3458,
      "step": 890
    },
    {
      "epoch": 0.03109106611649554,
      "grad_norm": 8.673651695251465,
      "learning_rate": 0.000278276033881415,
      "loss": 0.2896,
      "step": 891
    },
    {
      "epoch": 0.031125960691261525,
      "grad_norm": 6.712884426116943,
      "learning_rate": 0.00027802690582959646,
      "loss": 0.3188,
      "step": 892
    },
    {
      "epoch": 0.031160855266027514,
      "grad_norm": 2.941570281982422,
      "learning_rate": 0.0002777777777777778,
      "loss": 0.1608,
      "step": 893
    },
    {
      "epoch": 0.031195749840793503,
      "grad_norm": 2.7982993125915527,
      "learning_rate": 0.00027752864972595915,
      "loss": 0.2528,
      "step": 894
    },
    {
      "epoch": 0.03123064441555949,
      "grad_norm": 4.920527935028076,
      "learning_rate": 0.0002772795216741405,
      "loss": 0.1633,
      "step": 895
    },
    {
      "epoch": 0.03126553899032548,
      "grad_norm": 3.439300537109375,
      "learning_rate": 0.00027703039362232185,
      "loss": 0.184,
      "step": 896
    },
    {
      "epoch": 0.03130043356509147,
      "grad_norm": 5.836511611938477,
      "learning_rate": 0.0002767812655705033,
      "loss": 0.2526,
      "step": 897
    },
    {
      "epoch": 0.031335328139857455,
      "grad_norm": 8.332669258117676,
      "learning_rate": 0.0002765321375186846,
      "loss": 0.5095,
      "step": 898
    },
    {
      "epoch": 0.03137022271462345,
      "grad_norm": 3.7530479431152344,
      "learning_rate": 0.000276283009466866,
      "loss": 0.1352,
      "step": 899
    },
    {
      "epoch": 0.03140511728938943,
      "grad_norm": 2.607642889022827,
      "learning_rate": 0.0002760338814150473,
      "loss": 0.2412,
      "step": 900
    },
    {
      "epoch": 0.03144001186415542,
      "grad_norm": 10.676823616027832,
      "learning_rate": 0.00027578475336322873,
      "loss": 0.6187,
      "step": 901
    },
    {
      "epoch": 0.03147490643892141,
      "grad_norm": 1.597434639930725,
      "learning_rate": 0.00027553562531141006,
      "loss": 0.1788,
      "step": 902
    },
    {
      "epoch": 0.031509801013687395,
      "grad_norm": 3.231649160385132,
      "learning_rate": 0.00027528649725959143,
      "loss": 0.2359,
      "step": 903
    },
    {
      "epoch": 0.03154469558845339,
      "grad_norm": 3.9896373748779297,
      "learning_rate": 0.00027503736920777276,
      "loss": 0.1924,
      "step": 904
    },
    {
      "epoch": 0.03157959016321937,
      "grad_norm": 4.143213272094727,
      "learning_rate": 0.0002747882411559542,
      "loss": 0.1858,
      "step": 905
    },
    {
      "epoch": 0.03161448473798536,
      "grad_norm": 3.6424334049224854,
      "learning_rate": 0.00027453911310413556,
      "loss": 0.2891,
      "step": 906
    },
    {
      "epoch": 0.03164937931275135,
      "grad_norm": 2.0998129844665527,
      "learning_rate": 0.0002742899850523169,
      "loss": 0.1765,
      "step": 907
    },
    {
      "epoch": 0.031684273887517336,
      "grad_norm": 2.112767219543457,
      "learning_rate": 0.00027404085700049826,
      "loss": 0.1191,
      "step": 908
    },
    {
      "epoch": 0.03171916846228333,
      "grad_norm": 3.8501155376434326,
      "learning_rate": 0.00027379172894867964,
      "loss": 0.3223,
      "step": 909
    },
    {
      "epoch": 0.031754063037049314,
      "grad_norm": 1.0112113952636719,
      "learning_rate": 0.000273542600896861,
      "loss": 0.1285,
      "step": 910
    },
    {
      "epoch": 0.031788957611815306,
      "grad_norm": 3.2341322898864746,
      "learning_rate": 0.00027329347284504234,
      "loss": 0.1701,
      "step": 911
    },
    {
      "epoch": 0.03182385218658129,
      "grad_norm": 5.088584899902344,
      "learning_rate": 0.0002730443447932237,
      "loss": 0.1997,
      "step": 912
    },
    {
      "epoch": 0.03185874676134728,
      "grad_norm": 3.8884685039520264,
      "learning_rate": 0.00027279521674140504,
      "loss": 0.2721,
      "step": 913
    },
    {
      "epoch": 0.03189364133611327,
      "grad_norm": 3.304002046585083,
      "learning_rate": 0.00027254608868958647,
      "loss": 0.3934,
      "step": 914
    },
    {
      "epoch": 0.031928535910879255,
      "grad_norm": 3.6242969036102295,
      "learning_rate": 0.00027229696063776784,
      "loss": 0.2474,
      "step": 915
    },
    {
      "epoch": 0.03196343048564525,
      "grad_norm": 2.85565185546875,
      "learning_rate": 0.00027204783258594916,
      "loss": 0.1452,
      "step": 916
    },
    {
      "epoch": 0.03199832506041123,
      "grad_norm": 4.201562881469727,
      "learning_rate": 0.00027179870453413054,
      "loss": 0.1289,
      "step": 917
    },
    {
      "epoch": 0.03203321963517722,
      "grad_norm": 0.8633203506469727,
      "learning_rate": 0.0002715495764823119,
      "loss": 0.0905,
      "step": 918
    },
    {
      "epoch": 0.03206811420994321,
      "grad_norm": 2.8814892768859863,
      "learning_rate": 0.0002713004484304933,
      "loss": 0.2556,
      "step": 919
    },
    {
      "epoch": 0.032103008784709196,
      "grad_norm": 5.365673542022705,
      "learning_rate": 0.0002710513203786746,
      "loss": 0.2819,
      "step": 920
    },
    {
      "epoch": 0.03213790335947519,
      "grad_norm": 6.262377738952637,
      "learning_rate": 0.000270802192326856,
      "loss": 0.381,
      "step": 921
    },
    {
      "epoch": 0.032172797934241174,
      "grad_norm": 3.74310040473938,
      "learning_rate": 0.0002705530642750374,
      "loss": 0.1556,
      "step": 922
    },
    {
      "epoch": 0.03220769250900716,
      "grad_norm": 6.151695251464844,
      "learning_rate": 0.00027030393622321875,
      "loss": 0.1822,
      "step": 923
    },
    {
      "epoch": 0.03224258708377315,
      "grad_norm": 4.725214958190918,
      "learning_rate": 0.0002700548081714001,
      "loss": 0.3469,
      "step": 924
    },
    {
      "epoch": 0.03227748165853914,
      "grad_norm": 3.3116400241851807,
      "learning_rate": 0.00026980568011958144,
      "loss": 0.1692,
      "step": 925
    },
    {
      "epoch": 0.03231237623330513,
      "grad_norm": 13.744720458984375,
      "learning_rate": 0.0002695565520677629,
      "loss": 0.3348,
      "step": 926
    },
    {
      "epoch": 0.032347270808071114,
      "grad_norm": 2.8951499462127686,
      "learning_rate": 0.0002693074240159442,
      "loss": 0.1472,
      "step": 927
    },
    {
      "epoch": 0.03238216538283711,
      "grad_norm": 3.4168055057525635,
      "learning_rate": 0.0002690582959641256,
      "loss": 0.1894,
      "step": 928
    },
    {
      "epoch": 0.03241705995760309,
      "grad_norm": 9.362117767333984,
      "learning_rate": 0.0002688091679123069,
      "loss": 0.3562,
      "step": 929
    },
    {
      "epoch": 0.03245195453236908,
      "grad_norm": 5.7663187980651855,
      "learning_rate": 0.00026856003986048827,
      "loss": 0.3161,
      "step": 930
    },
    {
      "epoch": 0.03248684910713507,
      "grad_norm": 5.488814830780029,
      "learning_rate": 0.0002683109118086697,
      "loss": 0.3371,
      "step": 931
    },
    {
      "epoch": 0.032521743681901055,
      "grad_norm": 3.8816263675689697,
      "learning_rate": 0.000268061783756851,
      "loss": 0.2034,
      "step": 932
    },
    {
      "epoch": 0.03255663825666705,
      "grad_norm": 4.789268493652344,
      "learning_rate": 0.0002678126557050324,
      "loss": 0.2109,
      "step": 933
    },
    {
      "epoch": 0.03259153283143303,
      "grad_norm": 6.881777286529541,
      "learning_rate": 0.0002675635276532137,
      "loss": 0.1839,
      "step": 934
    },
    {
      "epoch": 0.03262642740619902,
      "grad_norm": 10.126921653747559,
      "learning_rate": 0.00026731439960139515,
      "loss": 0.2095,
      "step": 935
    },
    {
      "epoch": 0.03266132198096501,
      "grad_norm": 4.053893089294434,
      "learning_rate": 0.0002670652715495765,
      "loss": 0.2436,
      "step": 936
    },
    {
      "epoch": 0.032696216555730996,
      "grad_norm": 3.579089879989624,
      "learning_rate": 0.00026681614349775785,
      "loss": 0.1395,
      "step": 937
    },
    {
      "epoch": 0.03273111113049699,
      "grad_norm": 5.162708282470703,
      "learning_rate": 0.0002665670154459392,
      "loss": 0.2654,
      "step": 938
    },
    {
      "epoch": 0.032766005705262974,
      "grad_norm": 2.484940767288208,
      "learning_rate": 0.0002663178873941206,
      "loss": 0.1317,
      "step": 939
    },
    {
      "epoch": 0.03280090028002896,
      "grad_norm": 5.062836647033691,
      "learning_rate": 0.000266068759342302,
      "loss": 0.1936,
      "step": 940
    },
    {
      "epoch": 0.03283579485479495,
      "grad_norm": 4.4560112953186035,
      "learning_rate": 0.0002658196312904833,
      "loss": 0.213,
      "step": 941
    },
    {
      "epoch": 0.03287068942956094,
      "grad_norm": 5.350792407989502,
      "learning_rate": 0.0002655705032386647,
      "loss": 0.3093,
      "step": 942
    },
    {
      "epoch": 0.03290558400432693,
      "grad_norm": 5.599205017089844,
      "learning_rate": 0.00026532137518684606,
      "loss": 0.2038,
      "step": 943
    },
    {
      "epoch": 0.032940478579092915,
      "grad_norm": 5.094818115234375,
      "learning_rate": 0.00026507224713502743,
      "loss": 0.1601,
      "step": 944
    },
    {
      "epoch": 0.03297537315385891,
      "grad_norm": 5.9593186378479,
      "learning_rate": 0.00026482311908320876,
      "loss": 0.4183,
      "step": 945
    },
    {
      "epoch": 0.03301026772862489,
      "grad_norm": 11.21213436126709,
      "learning_rate": 0.00026457399103139013,
      "loss": 0.3097,
      "step": 946
    },
    {
      "epoch": 0.03304516230339088,
      "grad_norm": 2.4376304149627686,
      "learning_rate": 0.00026432486297957146,
      "loss": 0.1584,
      "step": 947
    },
    {
      "epoch": 0.03308005687815687,
      "grad_norm": 4.796607971191406,
      "learning_rate": 0.0002640757349277529,
      "loss": 0.3584,
      "step": 948
    },
    {
      "epoch": 0.033114951452922856,
      "grad_norm": 1.9967583417892456,
      "learning_rate": 0.00026382660687593426,
      "loss": 0.1896,
      "step": 949
    },
    {
      "epoch": 0.03314984602768885,
      "grad_norm": 5.717228412628174,
      "learning_rate": 0.0002635774788241156,
      "loss": 0.2675,
      "step": 950
    },
    {
      "epoch": 0.03318474060245483,
      "grad_norm": 3.735736846923828,
      "learning_rate": 0.00026332835077229696,
      "loss": 0.2211,
      "step": 951
    },
    {
      "epoch": 0.03321963517722082,
      "grad_norm": 2.6878180503845215,
      "learning_rate": 0.00026307922272047834,
      "loss": 0.2499,
      "step": 952
    },
    {
      "epoch": 0.03325452975198681,
      "grad_norm": 2.3821332454681396,
      "learning_rate": 0.0002628300946686597,
      "loss": 0.0987,
      "step": 953
    },
    {
      "epoch": 0.033289424326752796,
      "grad_norm": 8.398699760437012,
      "learning_rate": 0.00026258096661684104,
      "loss": 0.3086,
      "step": 954
    },
    {
      "epoch": 0.03332431890151879,
      "grad_norm": 3.329040050506592,
      "learning_rate": 0.0002623318385650224,
      "loss": 0.0821,
      "step": 955
    },
    {
      "epoch": 0.033359213476284774,
      "grad_norm": 1.615146517753601,
      "learning_rate": 0.0002620827105132038,
      "loss": 0.1304,
      "step": 956
    },
    {
      "epoch": 0.03339410805105076,
      "grad_norm": 6.350064277648926,
      "learning_rate": 0.00026183358246138517,
      "loss": 0.3104,
      "step": 957
    },
    {
      "epoch": 0.03342900262581675,
      "grad_norm": 2.7525148391723633,
      "learning_rate": 0.00026158445440956654,
      "loss": 0.1597,
      "step": 958
    },
    {
      "epoch": 0.03346389720058274,
      "grad_norm": 2.1787517070770264,
      "learning_rate": 0.00026133532635774786,
      "loss": 0.1168,
      "step": 959
    },
    {
      "epoch": 0.03349879177534873,
      "grad_norm": 4.356083393096924,
      "learning_rate": 0.0002610861983059293,
      "loss": 0.1768,
      "step": 960
    },
    {
      "epoch": 0.033533686350114715,
      "grad_norm": 3.9593324661254883,
      "learning_rate": 0.0002608370702541106,
      "loss": 0.3304,
      "step": 961
    },
    {
      "epoch": 0.03356858092488071,
      "grad_norm": 3.6061809062957764,
      "learning_rate": 0.000260587942202292,
      "loss": 0.0953,
      "step": 962
    },
    {
      "epoch": 0.03360347549964669,
      "grad_norm": 1.8562477827072144,
      "learning_rate": 0.0002603388141504733,
      "loss": 0.1725,
      "step": 963
    },
    {
      "epoch": 0.03363837007441268,
      "grad_norm": 6.233709335327148,
      "learning_rate": 0.0002600896860986547,
      "loss": 0.2432,
      "step": 964
    },
    {
      "epoch": 0.03367326464917867,
      "grad_norm": 3.949781894683838,
      "learning_rate": 0.00025984055804683607,
      "loss": 0.1698,
      "step": 965
    },
    {
      "epoch": 0.033708159223944656,
      "grad_norm": 2.321453094482422,
      "learning_rate": 0.00025959142999501745,
      "loss": 0.1814,
      "step": 966
    },
    {
      "epoch": 0.03374305379871065,
      "grad_norm": 3.7521889209747314,
      "learning_rate": 0.0002593423019431988,
      "loss": 0.1807,
      "step": 967
    },
    {
      "epoch": 0.033777948373476634,
      "grad_norm": 5.340089797973633,
      "learning_rate": 0.00025909317389138014,
      "loss": 0.4094,
      "step": 968
    },
    {
      "epoch": 0.03381284294824262,
      "grad_norm": 3.1214256286621094,
      "learning_rate": 0.0002588440458395616,
      "loss": 0.1767,
      "step": 969
    },
    {
      "epoch": 0.03384773752300861,
      "grad_norm": 5.539236068725586,
      "learning_rate": 0.0002585949177877429,
      "loss": 0.2259,
      "step": 970
    },
    {
      "epoch": 0.0338826320977746,
      "grad_norm": 4.056146621704102,
      "learning_rate": 0.0002583457897359243,
      "loss": 0.2324,
      "step": 971
    },
    {
      "epoch": 0.03391752667254059,
      "grad_norm": 4.9011335372924805,
      "learning_rate": 0.0002580966616841056,
      "loss": 0.2839,
      "step": 972
    },
    {
      "epoch": 0.033952421247306575,
      "grad_norm": 4.457884788513184,
      "learning_rate": 0.000257847533632287,
      "loss": 0.1968,
      "step": 973
    },
    {
      "epoch": 0.03398731582207256,
      "grad_norm": 7.111489772796631,
      "learning_rate": 0.00025759840558046835,
      "loss": 0.3442,
      "step": 974
    },
    {
      "epoch": 0.03402221039683855,
      "grad_norm": 6.3713884353637695,
      "learning_rate": 0.0002573492775286497,
      "loss": 0.3438,
      "step": 975
    },
    {
      "epoch": 0.03405710497160454,
      "grad_norm": 4.038403511047363,
      "learning_rate": 0.0002571001494768311,
      "loss": 0.343,
      "step": 976
    },
    {
      "epoch": 0.03409199954637053,
      "grad_norm": 3.952515125274658,
      "learning_rate": 0.0002568510214250125,
      "loss": 0.1285,
      "step": 977
    },
    {
      "epoch": 0.034126894121136515,
      "grad_norm": 5.2101898193359375,
      "learning_rate": 0.00025660189337319385,
      "loss": 0.3189,
      "step": 978
    },
    {
      "epoch": 0.03416178869590251,
      "grad_norm": 2.554462194442749,
      "learning_rate": 0.0002563527653213752,
      "loss": 0.1796,
      "step": 979
    },
    {
      "epoch": 0.03419668327066849,
      "grad_norm": 4.2684125900268555,
      "learning_rate": 0.00025610363726955655,
      "loss": 0.1909,
      "step": 980
    },
    {
      "epoch": 0.03423157784543448,
      "grad_norm": 2.3291022777557373,
      "learning_rate": 0.0002558545092177379,
      "loss": 0.1025,
      "step": 981
    },
    {
      "epoch": 0.03426647242020047,
      "grad_norm": 0.943019688129425,
      "learning_rate": 0.0002556053811659193,
      "loss": 0.104,
      "step": 982
    },
    {
      "epoch": 0.034301366994966456,
      "grad_norm": 3.405951738357544,
      "learning_rate": 0.0002553562531141007,
      "loss": 0.1214,
      "step": 983
    },
    {
      "epoch": 0.03433626156973245,
      "grad_norm": 4.674050331115723,
      "learning_rate": 0.000255107125062282,
      "loss": 0.1778,
      "step": 984
    },
    {
      "epoch": 0.034371156144498434,
      "grad_norm": 4.038733005523682,
      "learning_rate": 0.0002548579970104634,
      "loss": 0.187,
      "step": 985
    },
    {
      "epoch": 0.03440605071926442,
      "grad_norm": 5.7104573249816895,
      "learning_rate": 0.00025460886895864476,
      "loss": 0.2949,
      "step": 986
    },
    {
      "epoch": 0.03444094529403041,
      "grad_norm": 1.0903418064117432,
      "learning_rate": 0.00025435974090682613,
      "loss": 0.0638,
      "step": 987
    },
    {
      "epoch": 0.0344758398687964,
      "grad_norm": 2.6084651947021484,
      "learning_rate": 0.00025411061285500746,
      "loss": 0.1429,
      "step": 988
    },
    {
      "epoch": 0.03451073444356239,
      "grad_norm": 2.7259111404418945,
      "learning_rate": 0.00025386148480318883,
      "loss": 0.1656,
      "step": 989
    },
    {
      "epoch": 0.034545629018328375,
      "grad_norm": 3.2785751819610596,
      "learning_rate": 0.0002536123567513702,
      "loss": 0.1157,
      "step": 990
    },
    {
      "epoch": 0.03458052359309436,
      "grad_norm": 2.3783462047576904,
      "learning_rate": 0.0002533632286995516,
      "loss": 0.0976,
      "step": 991
    },
    {
      "epoch": 0.03461541816786035,
      "grad_norm": 2.6737208366394043,
      "learning_rate": 0.00025311410064773296,
      "loss": 0.1367,
      "step": 992
    },
    {
      "epoch": 0.03465031274262634,
      "grad_norm": 2.5707173347473145,
      "learning_rate": 0.0002528649725959143,
      "loss": 0.153,
      "step": 993
    },
    {
      "epoch": 0.03468520731739233,
      "grad_norm": 3.679690361022949,
      "learning_rate": 0.0002526158445440957,
      "loss": 0.1157,
      "step": 994
    },
    {
      "epoch": 0.034720101892158316,
      "grad_norm": 4.848487377166748,
      "learning_rate": 0.00025236671649227704,
      "loss": 0.1688,
      "step": 995
    },
    {
      "epoch": 0.03475499646692431,
      "grad_norm": 6.806256294250488,
      "learning_rate": 0.0002521175884404584,
      "loss": 0.2148,
      "step": 996
    },
    {
      "epoch": 0.03478989104169029,
      "grad_norm": 4.864534378051758,
      "learning_rate": 0.00025186846038863974,
      "loss": 0.1748,
      "step": 997
    },
    {
      "epoch": 0.03482478561645628,
      "grad_norm": 8.34158706665039,
      "learning_rate": 0.0002516193323368211,
      "loss": 0.4067,
      "step": 998
    },
    {
      "epoch": 0.03485968019122227,
      "grad_norm": 4.0566487312316895,
      "learning_rate": 0.0002513702042850025,
      "loss": 0.2558,
      "step": 999
    },
    {
      "epoch": 0.03489457476598826,
      "grad_norm": 5.514575004577637,
      "learning_rate": 0.00025112107623318387,
      "loss": 0.2151,
      "step": 1000
    },
    {
      "epoch": 0.03492946934075425,
      "grad_norm": 2.5718517303466797,
      "learning_rate": 0.00025087194818136524,
      "loss": 0.3852,
      "step": 1001
    },
    {
      "epoch": 0.034964363915520234,
      "grad_norm": 4.356133460998535,
      "learning_rate": 0.00025062282012954656,
      "loss": 0.1142,
      "step": 1002
    },
    {
      "epoch": 0.03499925849028622,
      "grad_norm": 1.5501539707183838,
      "learning_rate": 0.000250373692077728,
      "loss": 0.0503,
      "step": 1003
    },
    {
      "epoch": 0.03503415306505221,
      "grad_norm": 8.972655296325684,
      "learning_rate": 0.0002501245640259093,
      "loss": 0.1654,
      "step": 1004
    },
    {
      "epoch": 0.0350690476398182,
      "grad_norm": 7.610670566558838,
      "learning_rate": 0.0002498754359740907,
      "loss": 0.1673,
      "step": 1005
    },
    {
      "epoch": 0.03510394221458419,
      "grad_norm": 2.832559585571289,
      "learning_rate": 0.00024962630792227207,
      "loss": 0.1589,
      "step": 1006
    },
    {
      "epoch": 0.035138836789350175,
      "grad_norm": 3.986375093460083,
      "learning_rate": 0.0002493771798704534,
      "loss": 0.263,
      "step": 1007
    },
    {
      "epoch": 0.03517373136411616,
      "grad_norm": 8.190374374389648,
      "learning_rate": 0.00024912805181863477,
      "loss": 0.1816,
      "step": 1008
    },
    {
      "epoch": 0.03520862593888215,
      "grad_norm": 3.407316207885742,
      "learning_rate": 0.00024887892376681615,
      "loss": 0.1753,
      "step": 1009
    },
    {
      "epoch": 0.03524352051364814,
      "grad_norm": 4.663054466247559,
      "learning_rate": 0.0002486297957149975,
      "loss": 0.217,
      "step": 1010
    },
    {
      "epoch": 0.03527841508841413,
      "grad_norm": 2.692608594894409,
      "learning_rate": 0.0002483806676631789,
      "loss": 0.1329,
      "step": 1011
    },
    {
      "epoch": 0.035313309663180116,
      "grad_norm": 3.4945590496063232,
      "learning_rate": 0.0002481315396113603,
      "loss": 0.2655,
      "step": 1012
    },
    {
      "epoch": 0.03534820423794611,
      "grad_norm": 1.4319367408752441,
      "learning_rate": 0.0002478824115595416,
      "loss": 0.1173,
      "step": 1013
    },
    {
      "epoch": 0.035383098812712094,
      "grad_norm": 6.241280555725098,
      "learning_rate": 0.000247633283507723,
      "loss": 0.2585,
      "step": 1014
    },
    {
      "epoch": 0.03541799338747808,
      "grad_norm": 2.9826033115386963,
      "learning_rate": 0.00024738415545590435,
      "loss": 0.1205,
      "step": 1015
    },
    {
      "epoch": 0.03545288796224407,
      "grad_norm": 5.921497821807861,
      "learning_rate": 0.00024713502740408567,
      "loss": 0.3476,
      "step": 1016
    },
    {
      "epoch": 0.03548778253701006,
      "grad_norm": 1.546226143836975,
      "learning_rate": 0.00024688589935226705,
      "loss": 0.0515,
      "step": 1017
    },
    {
      "epoch": 0.03552267711177605,
      "grad_norm": 3.818364381790161,
      "learning_rate": 0.0002466367713004484,
      "loss": 0.2187,
      "step": 1018
    },
    {
      "epoch": 0.035557571686542035,
      "grad_norm": 5.756488800048828,
      "learning_rate": 0.0002463876432486298,
      "loss": 0.3567,
      "step": 1019
    },
    {
      "epoch": 0.03559246626130802,
      "grad_norm": 2.7823586463928223,
      "learning_rate": 0.0002461385151968112,
      "loss": 0.1516,
      "step": 1020
    },
    {
      "epoch": 0.03562736083607401,
      "grad_norm": 3.449674129486084,
      "learning_rate": 0.00024588938714499255,
      "loss": 0.0667,
      "step": 1021
    },
    {
      "epoch": 0.03566225541084,
      "grad_norm": 1.7944754362106323,
      "learning_rate": 0.0002456402590931739,
      "loss": 0.1563,
      "step": 1022
    },
    {
      "epoch": 0.03569714998560599,
      "grad_norm": 4.825964450836182,
      "learning_rate": 0.00024539113104135525,
      "loss": 0.2073,
      "step": 1023
    },
    {
      "epoch": 0.035732044560371976,
      "grad_norm": 2.804320812225342,
      "learning_rate": 0.00024514200298953663,
      "loss": 0.0938,
      "step": 1024
    },
    {
      "epoch": 0.03576693913513797,
      "grad_norm": 7.025259017944336,
      "learning_rate": 0.000244892874937718,
      "loss": 0.2027,
      "step": 1025
    },
    {
      "epoch": 0.03580183370990395,
      "grad_norm": 8.466179847717285,
      "learning_rate": 0.00024464374688589933,
      "loss": 0.3247,
      "step": 1026
    },
    {
      "epoch": 0.03583672828466994,
      "grad_norm": 3.2322916984558105,
      "learning_rate": 0.00024439461883408076,
      "loss": 0.1779,
      "step": 1027
    },
    {
      "epoch": 0.03587162285943593,
      "grad_norm": 2.680828094482422,
      "learning_rate": 0.0002441454907822621,
      "loss": 0.1311,
      "step": 1028
    },
    {
      "epoch": 0.035906517434201916,
      "grad_norm": 2.21624493598938,
      "learning_rate": 0.00024389636273044346,
      "loss": 0.1021,
      "step": 1029
    },
    {
      "epoch": 0.03594141200896791,
      "grad_norm": 9.313796997070312,
      "learning_rate": 0.0002436472346786248,
      "loss": 0.3215,
      "step": 1030
    },
    {
      "epoch": 0.035976306583733894,
      "grad_norm": 3.8676977157592773,
      "learning_rate": 0.00024339810662680618,
      "loss": 0.1686,
      "step": 1031
    },
    {
      "epoch": 0.03601120115849988,
      "grad_norm": 2.4082841873168945,
      "learning_rate": 0.00024314897857498756,
      "loss": 0.1121,
      "step": 1032
    },
    {
      "epoch": 0.03604609573326587,
      "grad_norm": 3.5628161430358887,
      "learning_rate": 0.0002428998505231689,
      "loss": 0.1801,
      "step": 1033
    },
    {
      "epoch": 0.03608099030803186,
      "grad_norm": 5.30739688873291,
      "learning_rate": 0.00024265072247135029,
      "loss": 0.2437,
      "step": 1034
    },
    {
      "epoch": 0.03611588488279785,
      "grad_norm": 6.516195774078369,
      "learning_rate": 0.00024240159441953163,
      "loss": 0.0995,
      "step": 1035
    },
    {
      "epoch": 0.036150779457563835,
      "grad_norm": 8.715289115905762,
      "learning_rate": 0.000242152466367713,
      "loss": 0.0829,
      "step": 1036
    },
    {
      "epoch": 0.03618567403232982,
      "grad_norm": 6.46530818939209,
      "learning_rate": 0.00024190333831589436,
      "loss": 0.238,
      "step": 1037
    },
    {
      "epoch": 0.03622056860709581,
      "grad_norm": 7.055422782897949,
      "learning_rate": 0.00024165421026407574,
      "loss": 0.2643,
      "step": 1038
    },
    {
      "epoch": 0.0362554631818618,
      "grad_norm": 4.977772235870361,
      "learning_rate": 0.00024140508221225709,
      "loss": 0.1555,
      "step": 1039
    },
    {
      "epoch": 0.03629035775662779,
      "grad_norm": 1.2806426286697388,
      "learning_rate": 0.0002411559541604385,
      "loss": 0.068,
      "step": 1040
    },
    {
      "epoch": 0.036325252331393776,
      "grad_norm": 6.469905376434326,
      "learning_rate": 0.00024090682610861984,
      "loss": 0.3284,
      "step": 1041
    },
    {
      "epoch": 0.03636014690615977,
      "grad_norm": 5.002241611480713,
      "learning_rate": 0.00024065769805680122,
      "loss": 0.0941,
      "step": 1042
    },
    {
      "epoch": 0.036395041480925754,
      "grad_norm": 5.097162246704102,
      "learning_rate": 0.00024040857000498256,
      "loss": 0.2616,
      "step": 1043
    },
    {
      "epoch": 0.03642993605569174,
      "grad_norm": 6.390489101409912,
      "learning_rate": 0.00024015944195316394,
      "loss": 0.1936,
      "step": 1044
    },
    {
      "epoch": 0.03646483063045773,
      "grad_norm": 3.021585702896118,
      "learning_rate": 0.0002399103139013453,
      "loss": 0.1508,
      "step": 1045
    },
    {
      "epoch": 0.03649972520522372,
      "grad_norm": 2.6838600635528564,
      "learning_rate": 0.00023966118584952667,
      "loss": 0.187,
      "step": 1046
    },
    {
      "epoch": 0.03653461977998971,
      "grad_norm": 2.1982944011688232,
      "learning_rate": 0.00023941205779770802,
      "loss": 0.0673,
      "step": 1047
    },
    {
      "epoch": 0.036569514354755694,
      "grad_norm": 2.979327917098999,
      "learning_rate": 0.00023916292974588937,
      "loss": 0.1651,
      "step": 1048
    },
    {
      "epoch": 0.03660440892952168,
      "grad_norm": 1.120642066001892,
      "learning_rate": 0.00023891380169407077,
      "loss": 0.0522,
      "step": 1049
    },
    {
      "epoch": 0.03663930350428767,
      "grad_norm": 7.027842998504639,
      "learning_rate": 0.00023866467364225212,
      "loss": 0.4592,
      "step": 1050
    },
    {
      "epoch": 0.03667419807905366,
      "grad_norm": 2.494718313217163,
      "learning_rate": 0.0002384155455904335,
      "loss": 0.0894,
      "step": 1051
    },
    {
      "epoch": 0.03670909265381965,
      "grad_norm": 4.280145645141602,
      "learning_rate": 0.00023816641753861484,
      "loss": 0.081,
      "step": 1052
    },
    {
      "epoch": 0.036743987228585635,
      "grad_norm": 4.1228790283203125,
      "learning_rate": 0.00023791728948679622,
      "loss": 0.2483,
      "step": 1053
    },
    {
      "epoch": 0.03677888180335162,
      "grad_norm": 5.572309494018555,
      "learning_rate": 0.00023766816143497757,
      "loss": 0.0833,
      "step": 1054
    },
    {
      "epoch": 0.03681377637811761,
      "grad_norm": 6.799740314483643,
      "learning_rate": 0.00023741903338315895,
      "loss": 0.3114,
      "step": 1055
    },
    {
      "epoch": 0.0368486709528836,
      "grad_norm": 3.022562026977539,
      "learning_rate": 0.0002371699053313403,
      "loss": 0.1761,
      "step": 1056
    },
    {
      "epoch": 0.03688356552764959,
      "grad_norm": 2.82094669342041,
      "learning_rate": 0.0002369207772795217,
      "loss": 0.1464,
      "step": 1057
    },
    {
      "epoch": 0.036918460102415576,
      "grad_norm": 3.54366397857666,
      "learning_rate": 0.00023667164922770305,
      "loss": 0.0735,
      "step": 1058
    },
    {
      "epoch": 0.03695335467718157,
      "grad_norm": 4.701437950134277,
      "learning_rate": 0.00023642252117588443,
      "loss": 0.1622,
      "step": 1059
    },
    {
      "epoch": 0.036988249251947554,
      "grad_norm": 3.297358989715576,
      "learning_rate": 0.00023617339312406577,
      "loss": 0.2198,
      "step": 1060
    },
    {
      "epoch": 0.03702314382671354,
      "grad_norm": 3.650607109069824,
      "learning_rate": 0.00023592426507224715,
      "loss": 0.124,
      "step": 1061
    },
    {
      "epoch": 0.03705803840147953,
      "grad_norm": 6.214969158172607,
      "learning_rate": 0.0002356751370204285,
      "loss": 0.127,
      "step": 1062
    },
    {
      "epoch": 0.03709293297624552,
      "grad_norm": 3.959369421005249,
      "learning_rate": 0.00023542600896860988,
      "loss": 0.1761,
      "step": 1063
    },
    {
      "epoch": 0.03712782755101151,
      "grad_norm": 6.089611530303955,
      "learning_rate": 0.00023517688091679123,
      "loss": 0.2623,
      "step": 1064
    },
    {
      "epoch": 0.037162722125777495,
      "grad_norm": 7.809354305267334,
      "learning_rate": 0.00023492775286497258,
      "loss": 0.1639,
      "step": 1065
    },
    {
      "epoch": 0.03719761670054348,
      "grad_norm": 4.329279899597168,
      "learning_rate": 0.00023467862481315398,
      "loss": 0.1494,
      "step": 1066
    },
    {
      "epoch": 0.03723251127530947,
      "grad_norm": 10.467981338500977,
      "learning_rate": 0.00023442949676133533,
      "loss": 0.4396,
      "step": 1067
    },
    {
      "epoch": 0.03726740585007546,
      "grad_norm": 2.479677200317383,
      "learning_rate": 0.0002341803687095167,
      "loss": 0.1635,
      "step": 1068
    },
    {
      "epoch": 0.03730230042484145,
      "grad_norm": 4.632659435272217,
      "learning_rate": 0.00023393124065769805,
      "loss": 0.3986,
      "step": 1069
    },
    {
      "epoch": 0.037337194999607436,
      "grad_norm": 1.021768569946289,
      "learning_rate": 0.00023368211260587943,
      "loss": 0.1029,
      "step": 1070
    },
    {
      "epoch": 0.03737208957437342,
      "grad_norm": 1.3262230157852173,
      "learning_rate": 0.00023343298455406078,
      "loss": 0.0601,
      "step": 1071
    },
    {
      "epoch": 0.03740698414913941,
      "grad_norm": 9.660595893859863,
      "learning_rate": 0.00023318385650224216,
      "loss": 0.4044,
      "step": 1072
    },
    {
      "epoch": 0.0374418787239054,
      "grad_norm": 1.8230469226837158,
      "learning_rate": 0.0002329347284504235,
      "loss": 0.0981,
      "step": 1073
    },
    {
      "epoch": 0.03747677329867139,
      "grad_norm": 3.5618319511413574,
      "learning_rate": 0.0002326856003986049,
      "loss": 0.0913,
      "step": 1074
    },
    {
      "epoch": 0.037511667873437377,
      "grad_norm": 2.2155838012695312,
      "learning_rate": 0.00023243647234678626,
      "loss": 0.2658,
      "step": 1075
    },
    {
      "epoch": 0.03754656244820337,
      "grad_norm": 2.5628116130828857,
      "learning_rate": 0.00023218734429496764,
      "loss": 0.198,
      "step": 1076
    },
    {
      "epoch": 0.037581457022969354,
      "grad_norm": 2.863054037094116,
      "learning_rate": 0.00023193821624314898,
      "loss": 0.2295,
      "step": 1077
    },
    {
      "epoch": 0.03761635159773534,
      "grad_norm": 8.906744003295898,
      "learning_rate": 0.00023168908819133036,
      "loss": 0.3544,
      "step": 1078
    },
    {
      "epoch": 0.03765124617250133,
      "grad_norm": 6.273894309997559,
      "learning_rate": 0.0002314399601395117,
      "loss": 0.5192,
      "step": 1079
    },
    {
      "epoch": 0.03768614074726732,
      "grad_norm": 4.896782398223877,
      "learning_rate": 0.0002311908320876931,
      "loss": 0.1132,
      "step": 1080
    },
    {
      "epoch": 0.03772103532203331,
      "grad_norm": 1.5320090055465698,
      "learning_rate": 0.00023094170403587444,
      "loss": 0.0381,
      "step": 1081
    },
    {
      "epoch": 0.037755929896799295,
      "grad_norm": 4.018491744995117,
      "learning_rate": 0.00023069257598405579,
      "loss": 0.2869,
      "step": 1082
    },
    {
      "epoch": 0.03779082447156528,
      "grad_norm": 2.811018466949463,
      "learning_rate": 0.0002304434479322372,
      "loss": 0.1171,
      "step": 1083
    },
    {
      "epoch": 0.03782571904633127,
      "grad_norm": 6.210611343383789,
      "learning_rate": 0.00023019431988041854,
      "loss": 0.1749,
      "step": 1084
    },
    {
      "epoch": 0.03786061362109726,
      "grad_norm": 3.2182395458221436,
      "learning_rate": 0.00022994519182859992,
      "loss": 0.1409,
      "step": 1085
    },
    {
      "epoch": 0.03789550819586325,
      "grad_norm": 9.736105918884277,
      "learning_rate": 0.00022969606377678126,
      "loss": 0.2499,
      "step": 1086
    },
    {
      "epoch": 0.037930402770629236,
      "grad_norm": 4.619214057922363,
      "learning_rate": 0.00022944693572496264,
      "loss": 0.2584,
      "step": 1087
    },
    {
      "epoch": 0.03796529734539522,
      "grad_norm": 2.7215113639831543,
      "learning_rate": 0.000229197807673144,
      "loss": 0.1604,
      "step": 1088
    },
    {
      "epoch": 0.038000191920161214,
      "grad_norm": 4.159398555755615,
      "learning_rate": 0.00022894867962132537,
      "loss": 0.1688,
      "step": 1089
    },
    {
      "epoch": 0.0380350864949272,
      "grad_norm": 1.1416693925857544,
      "learning_rate": 0.00022869955156950672,
      "loss": 0.1421,
      "step": 1090
    },
    {
      "epoch": 0.03806998106969319,
      "grad_norm": 8.573405265808105,
      "learning_rate": 0.0002284504235176881,
      "loss": 0.2361,
      "step": 1091
    },
    {
      "epoch": 0.03810487564445918,
      "grad_norm": 2.741997003555298,
      "learning_rate": 0.00022820129546586947,
      "loss": 0.1957,
      "step": 1092
    },
    {
      "epoch": 0.03813977021922517,
      "grad_norm": 1.2634241580963135,
      "learning_rate": 0.00022795216741405085,
      "loss": 0.1519,
      "step": 1093
    },
    {
      "epoch": 0.038174664793991155,
      "grad_norm": 3.2304956912994385,
      "learning_rate": 0.0002277030393622322,
      "loss": 0.0573,
      "step": 1094
    },
    {
      "epoch": 0.03820955936875714,
      "grad_norm": 5.753212928771973,
      "learning_rate": 0.00022745391131041357,
      "loss": 0.2326,
      "step": 1095
    },
    {
      "epoch": 0.03824445394352313,
      "grad_norm": 2.6068456172943115,
      "learning_rate": 0.00022720478325859492,
      "loss": 0.135,
      "step": 1096
    },
    {
      "epoch": 0.03827934851828912,
      "grad_norm": 1.109179139137268,
      "learning_rate": 0.0002269556552067763,
      "loss": 0.0512,
      "step": 1097
    },
    {
      "epoch": 0.03831424309305511,
      "grad_norm": 4.084250450134277,
      "learning_rate": 0.00022670652715495765,
      "loss": 0.2388,
      "step": 1098
    },
    {
      "epoch": 0.038349137667821095,
      "grad_norm": 3.302867889404297,
      "learning_rate": 0.000226457399103139,
      "loss": 0.1897,
      "step": 1099
    },
    {
      "epoch": 0.03838403224258708,
      "grad_norm": 3.921734094619751,
      "learning_rate": 0.00022620827105132037,
      "loss": 0.167,
      "step": 1100
    },
    {
      "epoch": 0.03841892681735307,
      "grad_norm": 3.8033602237701416,
      "learning_rate": 0.00022595914299950175,
      "loss": 0.1003,
      "step": 1101
    },
    {
      "epoch": 0.03845382139211906,
      "grad_norm": 2.8984243869781494,
      "learning_rate": 0.00022571001494768313,
      "loss": 0.1209,
      "step": 1102
    },
    {
      "epoch": 0.03848871596688505,
      "grad_norm": 15.080937385559082,
      "learning_rate": 0.00022546088689586447,
      "loss": 0.154,
      "step": 1103
    },
    {
      "epoch": 0.038523610541651036,
      "grad_norm": 2.331925868988037,
      "learning_rate": 0.00022521175884404585,
      "loss": 0.1202,
      "step": 1104
    },
    {
      "epoch": 0.03855850511641702,
      "grad_norm": 2.598883867263794,
      "learning_rate": 0.0002249626307922272,
      "loss": 0.0811,
      "step": 1105
    },
    {
      "epoch": 0.038593399691183014,
      "grad_norm": 1.4795546531677246,
      "learning_rate": 0.00022471350274040858,
      "loss": 0.0544,
      "step": 1106
    },
    {
      "epoch": 0.038628294265949,
      "grad_norm": 6.087765216827393,
      "learning_rate": 0.00022446437468858993,
      "loss": 0.2423,
      "step": 1107
    },
    {
      "epoch": 0.03866318884071499,
      "grad_norm": 3.7098612785339355,
      "learning_rate": 0.0002242152466367713,
      "loss": 0.1217,
      "step": 1108
    },
    {
      "epoch": 0.03869808341548098,
      "grad_norm": 6.624035358428955,
      "learning_rate": 0.00022396611858495268,
      "loss": 0.1542,
      "step": 1109
    },
    {
      "epoch": 0.03873297799024697,
      "grad_norm": 1.8246641159057617,
      "learning_rate": 0.00022371699053313406,
      "loss": 0.1245,
      "step": 1110
    },
    {
      "epoch": 0.038767872565012955,
      "grad_norm": 4.656383514404297,
      "learning_rate": 0.0002234678624813154,
      "loss": 0.2395,
      "step": 1111
    },
    {
      "epoch": 0.03880276713977894,
      "grad_norm": 4.676043510437012,
      "learning_rate": 0.00022321873442949678,
      "loss": 0.1767,
      "step": 1112
    },
    {
      "epoch": 0.03883766171454493,
      "grad_norm": 1.855168342590332,
      "learning_rate": 0.00022296960637767813,
      "loss": 0.101,
      "step": 1113
    },
    {
      "epoch": 0.03887255628931092,
      "grad_norm": 6.3340935707092285,
      "learning_rate": 0.0002227204783258595,
      "loss": 0.2676,
      "step": 1114
    },
    {
      "epoch": 0.03890745086407691,
      "grad_norm": 2.4779891967773438,
      "learning_rate": 0.00022247135027404086,
      "loss": 0.1394,
      "step": 1115
    },
    {
      "epoch": 0.038942345438842896,
      "grad_norm": 5.050297737121582,
      "learning_rate": 0.0002222222222222222,
      "loss": 0.2455,
      "step": 1116
    },
    {
      "epoch": 0.03897724001360888,
      "grad_norm": 8.233749389648438,
      "learning_rate": 0.00022197309417040358,
      "loss": 0.1973,
      "step": 1117
    },
    {
      "epoch": 0.039012134588374874,
      "grad_norm": 0.9322627782821655,
      "learning_rate": 0.00022172396611858496,
      "loss": 0.0323,
      "step": 1118
    },
    {
      "epoch": 0.03904702916314086,
      "grad_norm": 6.752718925476074,
      "learning_rate": 0.00022147483806676634,
      "loss": 0.1108,
      "step": 1119
    },
    {
      "epoch": 0.03908192373790685,
      "grad_norm": 3.5800580978393555,
      "learning_rate": 0.00022122571001494768,
      "loss": 0.2094,
      "step": 1120
    },
    {
      "epoch": 0.03911681831267284,
      "grad_norm": 7.735408782958984,
      "learning_rate": 0.00022097658196312906,
      "loss": 0.3621,
      "step": 1121
    },
    {
      "epoch": 0.03915171288743882,
      "grad_norm": 2.3749988079071045,
      "learning_rate": 0.0002207274539113104,
      "loss": 0.1065,
      "step": 1122
    },
    {
      "epoch": 0.039186607462204814,
      "grad_norm": 0.9886324405670166,
      "learning_rate": 0.0002204783258594918,
      "loss": 0.0317,
      "step": 1123
    },
    {
      "epoch": 0.0392215020369708,
      "grad_norm": 5.4046630859375,
      "learning_rate": 0.00022022919780767314,
      "loss": 0.2144,
      "step": 1124
    },
    {
      "epoch": 0.03925639661173679,
      "grad_norm": 1.3203431367874146,
      "learning_rate": 0.0002199800697558545,
      "loss": 0.0765,
      "step": 1125
    },
    {
      "epoch": 0.03929129118650278,
      "grad_norm": 3.333707809448242,
      "learning_rate": 0.00021973094170403586,
      "loss": 0.1547,
      "step": 1126
    },
    {
      "epoch": 0.03932618576126877,
      "grad_norm": 1.929870843887329,
      "learning_rate": 0.00021948181365221727,
      "loss": 0.1495,
      "step": 1127
    },
    {
      "epoch": 0.039361080336034755,
      "grad_norm": 3.480957508087158,
      "learning_rate": 0.00021923268560039861,
      "loss": 0.1153,
      "step": 1128
    },
    {
      "epoch": 0.03939597491080074,
      "grad_norm": 5.501406192779541,
      "learning_rate": 0.00021898355754858,
      "loss": 0.2831,
      "step": 1129
    },
    {
      "epoch": 0.03943086948556673,
      "grad_norm": 3.350656747817993,
      "learning_rate": 0.00021873442949676134,
      "loss": 0.1876,
      "step": 1130
    },
    {
      "epoch": 0.03946576406033272,
      "grad_norm": 2.1318695545196533,
      "learning_rate": 0.0002184853014449427,
      "loss": 0.1293,
      "step": 1131
    },
    {
      "epoch": 0.03950065863509871,
      "grad_norm": 3.1659746170043945,
      "learning_rate": 0.00021823617339312407,
      "loss": 0.1154,
      "step": 1132
    },
    {
      "epoch": 0.039535553209864696,
      "grad_norm": 4.678065299987793,
      "learning_rate": 0.00021798704534130542,
      "loss": 0.1866,
      "step": 1133
    },
    {
      "epoch": 0.03957044778463068,
      "grad_norm": 1.6355050802230835,
      "learning_rate": 0.0002177379172894868,
      "loss": 0.1107,
      "step": 1134
    },
    {
      "epoch": 0.039605342359396674,
      "grad_norm": 4.28283166885376,
      "learning_rate": 0.00021748878923766817,
      "loss": 0.2088,
      "step": 1135
    },
    {
      "epoch": 0.03964023693416266,
      "grad_norm": 8.663763999938965,
      "learning_rate": 0.00021723966118584955,
      "loss": 0.2299,
      "step": 1136
    },
    {
      "epoch": 0.03967513150892865,
      "grad_norm": 1.8391809463500977,
      "learning_rate": 0.0002169905331340309,
      "loss": 0.0907,
      "step": 1137
    },
    {
      "epoch": 0.03971002608369464,
      "grad_norm": 5.449975967407227,
      "learning_rate": 0.00021674140508221227,
      "loss": 0.2212,
      "step": 1138
    },
    {
      "epoch": 0.03974492065846062,
      "grad_norm": 3.904444456100464,
      "learning_rate": 0.00021649227703039362,
      "loss": 0.2299,
      "step": 1139
    },
    {
      "epoch": 0.039779815233226615,
      "grad_norm": 3.307199478149414,
      "learning_rate": 0.000216243148978575,
      "loss": 0.2811,
      "step": 1140
    },
    {
      "epoch": 0.0398147098079926,
      "grad_norm": 3.009463310241699,
      "learning_rate": 0.00021599402092675635,
      "loss": 0.1868,
      "step": 1141
    },
    {
      "epoch": 0.03984960438275859,
      "grad_norm": 3.8688549995422363,
      "learning_rate": 0.00021574489287493772,
      "loss": 0.0935,
      "step": 1142
    },
    {
      "epoch": 0.03988449895752458,
      "grad_norm": 4.446933746337891,
      "learning_rate": 0.00021549576482311907,
      "loss": 0.1782,
      "step": 1143
    },
    {
      "epoch": 0.03991939353229057,
      "grad_norm": 2.3522415161132812,
      "learning_rate": 0.00021524663677130048,
      "loss": 0.1387,
      "step": 1144
    },
    {
      "epoch": 0.039954288107056556,
      "grad_norm": 3.7475993633270264,
      "learning_rate": 0.00021499750871948182,
      "loss": 0.1335,
      "step": 1145
    },
    {
      "epoch": 0.03998918268182254,
      "grad_norm": 2.7888343334198,
      "learning_rate": 0.0002147483806676632,
      "loss": 0.0906,
      "step": 1146
    },
    {
      "epoch": 0.04002407725658853,
      "grad_norm": 1.0371372699737549,
      "learning_rate": 0.00021449925261584455,
      "loss": 0.0318,
      "step": 1147
    },
    {
      "epoch": 0.04005897183135452,
      "grad_norm": 2.4702577590942383,
      "learning_rate": 0.0002142501245640259,
      "loss": 0.0655,
      "step": 1148
    },
    {
      "epoch": 0.04009386640612051,
      "grad_norm": 3.304105758666992,
      "learning_rate": 0.00021400099651220728,
      "loss": 0.1353,
      "step": 1149
    },
    {
      "epoch": 0.040128760980886496,
      "grad_norm": 5.30216121673584,
      "learning_rate": 0.00021375186846038863,
      "loss": 0.2164,
      "step": 1150
    },
    {
      "epoch": 0.04016365555565248,
      "grad_norm": 2.3909668922424316,
      "learning_rate": 0.00021350274040857,
      "loss": 0.1648,
      "step": 1151
    },
    {
      "epoch": 0.040198550130418474,
      "grad_norm": 3.060568332672119,
      "learning_rate": 0.00021325361235675135,
      "loss": 0.0555,
      "step": 1152
    },
    {
      "epoch": 0.04023344470518446,
      "grad_norm": 3.1124155521392822,
      "learning_rate": 0.00021300448430493276,
      "loss": 0.2021,
      "step": 1153
    },
    {
      "epoch": 0.04026833927995045,
      "grad_norm": 1.857828974723816,
      "learning_rate": 0.0002127553562531141,
      "loss": 0.0748,
      "step": 1154
    },
    {
      "epoch": 0.04030323385471644,
      "grad_norm": 4.6076979637146,
      "learning_rate": 0.00021250622820129548,
      "loss": 0.2364,
      "step": 1155
    },
    {
      "epoch": 0.04033812842948243,
      "grad_norm": 2.4626004695892334,
      "learning_rate": 0.00021225710014947683,
      "loss": 0.0528,
      "step": 1156
    },
    {
      "epoch": 0.040373023004248415,
      "grad_norm": 3.8132312297821045,
      "learning_rate": 0.0002120079720976582,
      "loss": 0.0675,
      "step": 1157
    },
    {
      "epoch": 0.0404079175790144,
      "grad_norm": 4.9121270179748535,
      "learning_rate": 0.00021175884404583956,
      "loss": 0.2324,
      "step": 1158
    },
    {
      "epoch": 0.04044281215378039,
      "grad_norm": 2.276394844055176,
      "learning_rate": 0.00021150971599402093,
      "loss": 0.1048,
      "step": 1159
    },
    {
      "epoch": 0.04047770672854638,
      "grad_norm": 8.022014617919922,
      "learning_rate": 0.00021126058794220228,
      "loss": 0.3134,
      "step": 1160
    },
    {
      "epoch": 0.04051260130331237,
      "grad_norm": 7.493403434753418,
      "learning_rate": 0.00021101145989038369,
      "loss": 0.1781,
      "step": 1161
    },
    {
      "epoch": 0.040547495878078356,
      "grad_norm": 6.237024307250977,
      "learning_rate": 0.00021076233183856503,
      "loss": 0.4067,
      "step": 1162
    },
    {
      "epoch": 0.04058239045284434,
      "grad_norm": 4.598085403442383,
      "learning_rate": 0.0002105132037867464,
      "loss": 0.2098,
      "step": 1163
    },
    {
      "epoch": 0.040617285027610334,
      "grad_norm": 5.23566198348999,
      "learning_rate": 0.00021026407573492776,
      "loss": 0.146,
      "step": 1164
    },
    {
      "epoch": 0.04065217960237632,
      "grad_norm": 2.84378719329834,
      "learning_rate": 0.0002100149476831091,
      "loss": 0.1512,
      "step": 1165
    },
    {
      "epoch": 0.04068707417714231,
      "grad_norm": 2.5254967212677,
      "learning_rate": 0.00020976581963129049,
      "loss": 0.1421,
      "step": 1166
    },
    {
      "epoch": 0.0407219687519083,
      "grad_norm": 4.089735984802246,
      "learning_rate": 0.00020951669157947184,
      "loss": 0.2075,
      "step": 1167
    },
    {
      "epoch": 0.04075686332667428,
      "grad_norm": 5.572238445281982,
      "learning_rate": 0.0002092675635276532,
      "loss": 0.2639,
      "step": 1168
    },
    {
      "epoch": 0.040791757901440275,
      "grad_norm": 4.636008262634277,
      "learning_rate": 0.00020901843547583456,
      "loss": 0.2487,
      "step": 1169
    },
    {
      "epoch": 0.04082665247620626,
      "grad_norm": 3.396869421005249,
      "learning_rate": 0.00020876930742401597,
      "loss": 0.2411,
      "step": 1170
    },
    {
      "epoch": 0.04086154705097225,
      "grad_norm": 3.9142136573791504,
      "learning_rate": 0.00020852017937219731,
      "loss": 0.1199,
      "step": 1171
    },
    {
      "epoch": 0.04089644162573824,
      "grad_norm": 5.963383674621582,
      "learning_rate": 0.0002082710513203787,
      "loss": 0.1754,
      "step": 1172
    },
    {
      "epoch": 0.04093133620050423,
      "grad_norm": 7.170577049255371,
      "learning_rate": 0.00020802192326856004,
      "loss": 0.0699,
      "step": 1173
    },
    {
      "epoch": 0.040966230775270215,
      "grad_norm": 1.2142144441604614,
      "learning_rate": 0.00020777279521674142,
      "loss": 0.061,
      "step": 1174
    },
    {
      "epoch": 0.0410011253500362,
      "grad_norm": 5.7863264083862305,
      "learning_rate": 0.00020752366716492277,
      "loss": 0.2183,
      "step": 1175
    },
    {
      "epoch": 0.04103601992480219,
      "grad_norm": 5.98493766784668,
      "learning_rate": 0.00020727453911310414,
      "loss": 0.1692,
      "step": 1176
    },
    {
      "epoch": 0.04107091449956818,
      "grad_norm": 2.677884578704834,
      "learning_rate": 0.0002070254110612855,
      "loss": 0.1267,
      "step": 1177
    },
    {
      "epoch": 0.04110580907433417,
      "grad_norm": 4.169095039367676,
      "learning_rate": 0.00020677628300946687,
      "loss": 0.2164,
      "step": 1178
    },
    {
      "epoch": 0.041140703649100156,
      "grad_norm": 3.1266634464263916,
      "learning_rate": 0.00020652715495764824,
      "loss": 0.2028,
      "step": 1179
    },
    {
      "epoch": 0.04117559822386614,
      "grad_norm": 2.836326837539673,
      "learning_rate": 0.00020627802690582962,
      "loss": 0.0739,
      "step": 1180
    },
    {
      "epoch": 0.041210492798632134,
      "grad_norm": 1.6850658655166626,
      "learning_rate": 0.00020602889885401097,
      "loss": 0.1152,
      "step": 1181
    },
    {
      "epoch": 0.04124538737339812,
      "grad_norm": 1.6593198776245117,
      "learning_rate": 0.00020577977080219232,
      "loss": 0.0849,
      "step": 1182
    },
    {
      "epoch": 0.04128028194816411,
      "grad_norm": 1.6340970993041992,
      "learning_rate": 0.0002055306427503737,
      "loss": 0.1278,
      "step": 1183
    },
    {
      "epoch": 0.0413151765229301,
      "grad_norm": 6.599425792694092,
      "learning_rate": 0.00020528151469855505,
      "loss": 0.0819,
      "step": 1184
    },
    {
      "epoch": 0.04135007109769608,
      "grad_norm": 4.348822116851807,
      "learning_rate": 0.00020503238664673642,
      "loss": 0.1903,
      "step": 1185
    },
    {
      "epoch": 0.041384965672462075,
      "grad_norm": 2.923625946044922,
      "learning_rate": 0.00020478325859491777,
      "loss": 0.2554,
      "step": 1186
    },
    {
      "epoch": 0.04141986024722806,
      "grad_norm": 3.7337143421173096,
      "learning_rate": 0.00020453413054309917,
      "loss": 0.1663,
      "step": 1187
    },
    {
      "epoch": 0.04145475482199405,
      "grad_norm": 2.27649188041687,
      "learning_rate": 0.00020428500249128052,
      "loss": 0.0685,
      "step": 1188
    },
    {
      "epoch": 0.04148964939676004,
      "grad_norm": 2.89164662361145,
      "learning_rate": 0.0002040358744394619,
      "loss": 0.1657,
      "step": 1189
    },
    {
      "epoch": 0.04152454397152603,
      "grad_norm": 7.892327308654785,
      "learning_rate": 0.00020378674638764325,
      "loss": 0.2919,
      "step": 1190
    },
    {
      "epoch": 0.041559438546292016,
      "grad_norm": 0.4075383245944977,
      "learning_rate": 0.00020353761833582463,
      "loss": 0.0167,
      "step": 1191
    },
    {
      "epoch": 0.041594333121058,
      "grad_norm": 1.1798473596572876,
      "learning_rate": 0.00020328849028400598,
      "loss": 0.0813,
      "step": 1192
    },
    {
      "epoch": 0.04162922769582399,
      "grad_norm": 3.4290997982025146,
      "learning_rate": 0.00020303936223218735,
      "loss": 0.1526,
      "step": 1193
    },
    {
      "epoch": 0.04166412227058998,
      "grad_norm": 1.5688529014587402,
      "learning_rate": 0.0002027902341803687,
      "loss": 0.1397,
      "step": 1194
    },
    {
      "epoch": 0.04169901684535597,
      "grad_norm": 4.792984962463379,
      "learning_rate": 0.00020254110612855008,
      "loss": 0.1627,
      "step": 1195
    },
    {
      "epoch": 0.04173391142012196,
      "grad_norm": 5.055742263793945,
      "learning_rate": 0.00020229197807673145,
      "loss": 0.2066,
      "step": 1196
    },
    {
      "epoch": 0.04176880599488794,
      "grad_norm": 2.6488380432128906,
      "learning_rate": 0.00020204285002491283,
      "loss": 0.0841,
      "step": 1197
    },
    {
      "epoch": 0.041803700569653934,
      "grad_norm": 6.601930618286133,
      "learning_rate": 0.00020179372197309418,
      "loss": 0.2647,
      "step": 1198
    },
    {
      "epoch": 0.04183859514441992,
      "grad_norm": 5.80733060836792,
      "learning_rate": 0.00020154459392127553,
      "loss": 0.2081,
      "step": 1199
    },
    {
      "epoch": 0.04187348971918591,
      "grad_norm": 3.3398547172546387,
      "learning_rate": 0.0002012954658694569,
      "loss": 0.2697,
      "step": 1200
    },
    {
      "epoch": 0.0419083842939519,
      "grad_norm": 2.616506576538086,
      "learning_rate": 0.00020104633781763826,
      "loss": 0.1512,
      "step": 1201
    },
    {
      "epoch": 0.04194327886871788,
      "grad_norm": 13.160503387451172,
      "learning_rate": 0.00020079720976581963,
      "loss": 0.3357,
      "step": 1202
    },
    {
      "epoch": 0.041978173443483875,
      "grad_norm": 3.0950026512145996,
      "learning_rate": 0.00020054808171400098,
      "loss": 0.1821,
      "step": 1203
    },
    {
      "epoch": 0.04201306801824986,
      "grad_norm": 1.7381815910339355,
      "learning_rate": 0.00020029895366218236,
      "loss": 0.1721,
      "step": 1204
    },
    {
      "epoch": 0.04204796259301585,
      "grad_norm": 3.4221203327178955,
      "learning_rate": 0.00020004982561036373,
      "loss": 0.0643,
      "step": 1205
    },
    {
      "epoch": 0.04208285716778184,
      "grad_norm": 1.5004714727401733,
      "learning_rate": 0.0001998006975585451,
      "loss": 0.1425,
      "step": 1206
    },
    {
      "epoch": 0.04211775174254783,
      "grad_norm": 6.620568752288818,
      "learning_rate": 0.00019955156950672646,
      "loss": 0.3631,
      "step": 1207
    },
    {
      "epoch": 0.042152646317313816,
      "grad_norm": 2.0380842685699463,
      "learning_rate": 0.00019930244145490784,
      "loss": 0.1589,
      "step": 1208
    },
    {
      "epoch": 0.0421875408920798,
      "grad_norm": 5.246463775634766,
      "learning_rate": 0.00019905331340308919,
      "loss": 0.1416,
      "step": 1209
    },
    {
      "epoch": 0.042222435466845794,
      "grad_norm": 6.6348724365234375,
      "learning_rate": 0.00019880418535127056,
      "loss": 0.2556,
      "step": 1210
    },
    {
      "epoch": 0.04225733004161178,
      "grad_norm": 7.490714073181152,
      "learning_rate": 0.0001985550572994519,
      "loss": 0.1362,
      "step": 1211
    },
    {
      "epoch": 0.04229222461637777,
      "grad_norm": 9.553526878356934,
      "learning_rate": 0.0001983059292476333,
      "loss": 0.246,
      "step": 1212
    },
    {
      "epoch": 0.04232711919114376,
      "grad_norm": 4.856010437011719,
      "learning_rate": 0.00019805680119581464,
      "loss": 0.2454,
      "step": 1213
    },
    {
      "epoch": 0.04236201376590974,
      "grad_norm": 5.6523261070251465,
      "learning_rate": 0.00019780767314399604,
      "loss": 0.1815,
      "step": 1214
    },
    {
      "epoch": 0.042396908340675735,
      "grad_norm": 3.2351343631744385,
      "learning_rate": 0.0001975585450921774,
      "loss": 0.2514,
      "step": 1215
    },
    {
      "epoch": 0.04243180291544172,
      "grad_norm": 1.3367466926574707,
      "learning_rate": 0.00019730941704035874,
      "loss": 0.1227,
      "step": 1216
    },
    {
      "epoch": 0.04246669749020771,
      "grad_norm": 1.7829630374908447,
      "learning_rate": 0.00019706028898854012,
      "loss": 0.122,
      "step": 1217
    },
    {
      "epoch": 0.0425015920649737,
      "grad_norm": 3.0938355922698975,
      "learning_rate": 0.00019681116093672147,
      "loss": 0.1074,
      "step": 1218
    },
    {
      "epoch": 0.04253648663973968,
      "grad_norm": 2.560002565383911,
      "learning_rate": 0.00019656203288490284,
      "loss": 0.2068,
      "step": 1219
    },
    {
      "epoch": 0.042571381214505676,
      "grad_norm": 6.6051344871521,
      "learning_rate": 0.0001963129048330842,
      "loss": 0.2035,
      "step": 1220
    },
    {
      "epoch": 0.04260627578927166,
      "grad_norm": 2.28169584274292,
      "learning_rate": 0.00019606377678126557,
      "loss": 0.1525,
      "step": 1221
    },
    {
      "epoch": 0.04264117036403765,
      "grad_norm": 3.7920401096343994,
      "learning_rate": 0.00019581464872944694,
      "loss": 0.3397,
      "step": 1222
    },
    {
      "epoch": 0.04267606493880364,
      "grad_norm": 1.6200902462005615,
      "learning_rate": 0.00019556552067762832,
      "loss": 0.0949,
      "step": 1223
    },
    {
      "epoch": 0.04271095951356963,
      "grad_norm": 3.2976815700531006,
      "learning_rate": 0.00019531639262580967,
      "loss": 0.1981,
      "step": 1224
    },
    {
      "epoch": 0.042745854088335616,
      "grad_norm": 4.382696628570557,
      "learning_rate": 0.00019506726457399105,
      "loss": 0.1597,
      "step": 1225
    },
    {
      "epoch": 0.0427807486631016,
      "grad_norm": 5.793843746185303,
      "learning_rate": 0.0001948181365221724,
      "loss": 0.4903,
      "step": 1226
    },
    {
      "epoch": 0.042815643237867594,
      "grad_norm": 2.224364757537842,
      "learning_rate": 0.00019456900847035377,
      "loss": 0.1003,
      "step": 1227
    },
    {
      "epoch": 0.04285053781263358,
      "grad_norm": 1.8263494968414307,
      "learning_rate": 0.00019431988041853512,
      "loss": 0.1255,
      "step": 1228
    },
    {
      "epoch": 0.04288543238739957,
      "grad_norm": 3.681400775909424,
      "learning_rate": 0.0001940707523667165,
      "loss": 0.1141,
      "step": 1229
    },
    {
      "epoch": 0.04292032696216556,
      "grad_norm": 1.6690202951431274,
      "learning_rate": 0.00019382162431489785,
      "loss": 0.1641,
      "step": 1230
    },
    {
      "epoch": 0.04295522153693154,
      "grad_norm": 2.911978006362915,
      "learning_rate": 0.00019357249626307925,
      "loss": 0.1226,
      "step": 1231
    },
    {
      "epoch": 0.042990116111697535,
      "grad_norm": 2.883641004562378,
      "learning_rate": 0.0001933233682112606,
      "loss": 0.1141,
      "step": 1232
    },
    {
      "epoch": 0.04302501068646352,
      "grad_norm": 2.9701359272003174,
      "learning_rate": 0.00019307424015944195,
      "loss": 0.1333,
      "step": 1233
    },
    {
      "epoch": 0.04305990526122951,
      "grad_norm": 3.345513343811035,
      "learning_rate": 0.00019282511210762333,
      "loss": 0.1595,
      "step": 1234
    },
    {
      "epoch": 0.0430947998359955,
      "grad_norm": 1.8807326555252075,
      "learning_rate": 0.00019257598405580468,
      "loss": 0.2075,
      "step": 1235
    },
    {
      "epoch": 0.043129694410761484,
      "grad_norm": 6.422310829162598,
      "learning_rate": 0.00019232685600398605,
      "loss": 0.2563,
      "step": 1236
    },
    {
      "epoch": 0.043164588985527476,
      "grad_norm": 0.8687852025032043,
      "learning_rate": 0.0001920777279521674,
      "loss": 0.0325,
      "step": 1237
    },
    {
      "epoch": 0.04319948356029346,
      "grad_norm": 4.2439284324646,
      "learning_rate": 0.00019182859990034878,
      "loss": 0.1217,
      "step": 1238
    },
    {
      "epoch": 0.043234378135059454,
      "grad_norm": 7.436714172363281,
      "learning_rate": 0.00019157947184853013,
      "loss": 0.159,
      "step": 1239
    },
    {
      "epoch": 0.04326927270982544,
      "grad_norm": 3.6632256507873535,
      "learning_rate": 0.00019133034379671153,
      "loss": 0.3188,
      "step": 1240
    },
    {
      "epoch": 0.04330416728459143,
      "grad_norm": 4.274519443511963,
      "learning_rate": 0.00019108121574489288,
      "loss": 0.1204,
      "step": 1241
    },
    {
      "epoch": 0.04333906185935742,
      "grad_norm": 7.402740955352783,
      "learning_rate": 0.00019083208769307426,
      "loss": 0.2167,
      "step": 1242
    },
    {
      "epoch": 0.0433739564341234,
      "grad_norm": 4.071639537811279,
      "learning_rate": 0.0001905829596412556,
      "loss": 0.2067,
      "step": 1243
    },
    {
      "epoch": 0.043408851008889394,
      "grad_norm": 4.049901962280273,
      "learning_rate": 0.00019033383158943698,
      "loss": 0.2069,
      "step": 1244
    },
    {
      "epoch": 0.04344374558365538,
      "grad_norm": 5.503778457641602,
      "learning_rate": 0.00019008470353761833,
      "loss": 0.1263,
      "step": 1245
    },
    {
      "epoch": 0.04347864015842137,
      "grad_norm": 3.096449375152588,
      "learning_rate": 0.0001898355754857997,
      "loss": 0.0928,
      "step": 1246
    },
    {
      "epoch": 0.04351353473318736,
      "grad_norm": 1.744642734527588,
      "learning_rate": 0.00018958644743398106,
      "loss": 0.0714,
      "step": 1247
    },
    {
      "epoch": 0.04354842930795334,
      "grad_norm": 2.6946675777435303,
      "learning_rate": 0.00018933731938216246,
      "loss": 0.0979,
      "step": 1248
    },
    {
      "epoch": 0.043583323882719335,
      "grad_norm": 5.814515113830566,
      "learning_rate": 0.0001890881913303438,
      "loss": 0.1353,
      "step": 1249
    },
    {
      "epoch": 0.04361821845748532,
      "grad_norm": 1.6104340553283691,
      "learning_rate": 0.00018883906327852516,
      "loss": 0.0487,
      "step": 1250
    },
    {
      "epoch": 0.04365311303225131,
      "grad_norm": 2.4790737628936768,
      "learning_rate": 0.00018858993522670654,
      "loss": 0.1715,
      "step": 1251
    },
    {
      "epoch": 0.0436880076070173,
      "grad_norm": 5.668730735778809,
      "learning_rate": 0.00018834080717488789,
      "loss": 0.1535,
      "step": 1252
    },
    {
      "epoch": 0.043722902181783284,
      "grad_norm": 6.774897575378418,
      "learning_rate": 0.00018809167912306926,
      "loss": 0.1752,
      "step": 1253
    },
    {
      "epoch": 0.043757796756549276,
      "grad_norm": 2.5824670791625977,
      "learning_rate": 0.0001878425510712506,
      "loss": 0.0724,
      "step": 1254
    },
    {
      "epoch": 0.04379269133131526,
      "grad_norm": 9.279220581054688,
      "learning_rate": 0.000187593423019432,
      "loss": 0.3335,
      "step": 1255
    },
    {
      "epoch": 0.043827585906081254,
      "grad_norm": 6.844034671783447,
      "learning_rate": 0.00018734429496761334,
      "loss": 0.1451,
      "step": 1256
    },
    {
      "epoch": 0.04386248048084724,
      "grad_norm": 4.286397933959961,
      "learning_rate": 0.00018709516691579474,
      "loss": 0.097,
      "step": 1257
    },
    {
      "epoch": 0.04389737505561323,
      "grad_norm": 2.292186737060547,
      "learning_rate": 0.0001868460388639761,
      "loss": 0.0852,
      "step": 1258
    },
    {
      "epoch": 0.04393226963037922,
      "grad_norm": 9.564586639404297,
      "learning_rate": 0.00018659691081215747,
      "loss": 0.3372,
      "step": 1259
    },
    {
      "epoch": 0.0439671642051452,
      "grad_norm": 5.580056190490723,
      "learning_rate": 0.00018634778276033882,
      "loss": 0.2431,
      "step": 1260
    },
    {
      "epoch": 0.044002058779911195,
      "grad_norm": 3.982815980911255,
      "learning_rate": 0.0001860986547085202,
      "loss": 0.2043,
      "step": 1261
    },
    {
      "epoch": 0.04403695335467718,
      "grad_norm": 2.0499777793884277,
      "learning_rate": 0.00018584952665670154,
      "loss": 0.0645,
      "step": 1262
    },
    {
      "epoch": 0.04407184792944317,
      "grad_norm": 0.9542719125747681,
      "learning_rate": 0.00018560039860488292,
      "loss": 0.0373,
      "step": 1263
    },
    {
      "epoch": 0.04410674250420916,
      "grad_norm": 4.470494270324707,
      "learning_rate": 0.00018535127055306427,
      "loss": 0.0795,
      "step": 1264
    },
    {
      "epoch": 0.04414163707897514,
      "grad_norm": 2.3369674682617188,
      "learning_rate": 0.00018510214250124564,
      "loss": 0.1405,
      "step": 1265
    },
    {
      "epoch": 0.044176531653741136,
      "grad_norm": 4.913806438446045,
      "learning_rate": 0.00018485301444942702,
      "loss": 0.1179,
      "step": 1266
    },
    {
      "epoch": 0.04421142622850712,
      "grad_norm": 2.9021029472351074,
      "learning_rate": 0.00018460388639760837,
      "loss": 0.126,
      "step": 1267
    },
    {
      "epoch": 0.04424632080327311,
      "grad_norm": 1.6671977043151855,
      "learning_rate": 0.00018435475834578975,
      "loss": 0.0329,
      "step": 1268
    },
    {
      "epoch": 0.0442812153780391,
      "grad_norm": 4.580441951751709,
      "learning_rate": 0.0001841056302939711,
      "loss": 0.1206,
      "step": 1269
    },
    {
      "epoch": 0.044316109952805084,
      "grad_norm": 4.664236068725586,
      "learning_rate": 0.00018385650224215247,
      "loss": 0.2625,
      "step": 1270
    },
    {
      "epoch": 0.044351004527571077,
      "grad_norm": 4.695024490356445,
      "learning_rate": 0.00018360737419033382,
      "loss": 0.152,
      "step": 1271
    },
    {
      "epoch": 0.04438589910233706,
      "grad_norm": 3.5224337577819824,
      "learning_rate": 0.0001833582461385152,
      "loss": 0.1152,
      "step": 1272
    },
    {
      "epoch": 0.044420793677103054,
      "grad_norm": 9.144156455993652,
      "learning_rate": 0.00018310911808669655,
      "loss": 0.2195,
      "step": 1273
    },
    {
      "epoch": 0.04445568825186904,
      "grad_norm": 0.7788309454917908,
      "learning_rate": 0.00018285999003487795,
      "loss": 0.0388,
      "step": 1274
    },
    {
      "epoch": 0.04449058282663503,
      "grad_norm": 3.246893882751465,
      "learning_rate": 0.0001826108619830593,
      "loss": 0.1328,
      "step": 1275
    },
    {
      "epoch": 0.04452547740140102,
      "grad_norm": 6.243300437927246,
      "learning_rate": 0.00018236173393124068,
      "loss": 0.1804,
      "step": 1276
    },
    {
      "epoch": 0.044560371976167,
      "grad_norm": 7.343652725219727,
      "learning_rate": 0.00018211260587942203,
      "loss": 0.1769,
      "step": 1277
    },
    {
      "epoch": 0.044595266550932995,
      "grad_norm": 4.6648640632629395,
      "learning_rate": 0.0001818634778276034,
      "loss": 0.1521,
      "step": 1278
    },
    {
      "epoch": 0.04463016112569898,
      "grad_norm": 1.3926868438720703,
      "learning_rate": 0.00018161434977578475,
      "loss": 0.1012,
      "step": 1279
    },
    {
      "epoch": 0.04466505570046497,
      "grad_norm": 1.3258488178253174,
      "learning_rate": 0.00018136522172396613,
      "loss": 0.0475,
      "step": 1280
    },
    {
      "epoch": 0.04469995027523096,
      "grad_norm": 1.5501821041107178,
      "learning_rate": 0.00018111609367214748,
      "loss": 0.0718,
      "step": 1281
    },
    {
      "epoch": 0.044734844849996944,
      "grad_norm": 5.638949871063232,
      "learning_rate": 0.00018086696562032885,
      "loss": 0.1296,
      "step": 1282
    },
    {
      "epoch": 0.044769739424762936,
      "grad_norm": 2.998624086380005,
      "learning_rate": 0.00018061783756851023,
      "loss": 0.1026,
      "step": 1283
    },
    {
      "epoch": 0.04480463399952892,
      "grad_norm": 3.389604330062866,
      "learning_rate": 0.00018036870951669158,
      "loss": 0.0804,
      "step": 1284
    },
    {
      "epoch": 0.044839528574294914,
      "grad_norm": 4.73856782913208,
      "learning_rate": 0.00018011958146487296,
      "loss": 0.2927,
      "step": 1285
    },
    {
      "epoch": 0.0448744231490609,
      "grad_norm": 1.6191655397415161,
      "learning_rate": 0.0001798704534130543,
      "loss": 0.0857,
      "step": 1286
    },
    {
      "epoch": 0.04490931772382689,
      "grad_norm": 2.9708261489868164,
      "learning_rate": 0.00017962132536123568,
      "loss": 0.1195,
      "step": 1287
    },
    {
      "epoch": 0.04494421229859288,
      "grad_norm": 8.260830879211426,
      "learning_rate": 0.00017937219730941703,
      "loss": 0.3104,
      "step": 1288
    },
    {
      "epoch": 0.04497910687335886,
      "grad_norm": 1.6110857725143433,
      "learning_rate": 0.0001791230692575984,
      "loss": 0.1949,
      "step": 1289
    },
    {
      "epoch": 0.045014001448124855,
      "grad_norm": 2.5036709308624268,
      "learning_rate": 0.00017887394120577976,
      "loss": 0.2156,
      "step": 1290
    },
    {
      "epoch": 0.04504889602289084,
      "grad_norm": 2.735849142074585,
      "learning_rate": 0.00017862481315396113,
      "loss": 0.1813,
      "step": 1291
    },
    {
      "epoch": 0.04508379059765683,
      "grad_norm": 1.47380530834198,
      "learning_rate": 0.0001783756851021425,
      "loss": 0.0726,
      "step": 1292
    },
    {
      "epoch": 0.04511868517242282,
      "grad_norm": 2.851008653640747,
      "learning_rate": 0.00017812655705032389,
      "loss": 0.2622,
      "step": 1293
    },
    {
      "epoch": 0.0451535797471888,
      "grad_norm": 2.0950186252593994,
      "learning_rate": 0.00017787742899850524,
      "loss": 0.0825,
      "step": 1294
    },
    {
      "epoch": 0.045188474321954795,
      "grad_norm": 3.7259910106658936,
      "learning_rate": 0.0001776283009466866,
      "loss": 0.2223,
      "step": 1295
    },
    {
      "epoch": 0.04522336889672078,
      "grad_norm": 1.451428771018982,
      "learning_rate": 0.00017737917289486796,
      "loss": 0.094,
      "step": 1296
    },
    {
      "epoch": 0.04525826347148677,
      "grad_norm": 5.5727338790893555,
      "learning_rate": 0.00017713004484304934,
      "loss": 0.1203,
      "step": 1297
    },
    {
      "epoch": 0.04529315804625276,
      "grad_norm": 1.2138336896896362,
      "learning_rate": 0.0001768809167912307,
      "loss": 0.0605,
      "step": 1298
    },
    {
      "epoch": 0.045328052621018744,
      "grad_norm": 2.1805801391601562,
      "learning_rate": 0.00017663178873941206,
      "loss": 0.1284,
      "step": 1299
    },
    {
      "epoch": 0.045362947195784736,
      "grad_norm": 0.9275555610656738,
      "learning_rate": 0.00017638266068759344,
      "loss": 0.0487,
      "step": 1300
    },
    {
      "epoch": 0.04539784177055072,
      "grad_norm": 3.4062631130218506,
      "learning_rate": 0.0001761335326357748,
      "loss": 0.1767,
      "step": 1301
    },
    {
      "epoch": 0.045432736345316714,
      "grad_norm": 1.4730561971664429,
      "learning_rate": 0.00017588440458395617,
      "loss": 0.0857,
      "step": 1302
    },
    {
      "epoch": 0.0454676309200827,
      "grad_norm": 6.828258991241455,
      "learning_rate": 0.00017563527653213752,
      "loss": 0.185,
      "step": 1303
    },
    {
      "epoch": 0.04550252549484869,
      "grad_norm": 2.382356643676758,
      "learning_rate": 0.0001753861484803189,
      "loss": 0.0632,
      "step": 1304
    },
    {
      "epoch": 0.04553742006961468,
      "grad_norm": 6.91825008392334,
      "learning_rate": 0.00017513702042850024,
      "loss": 0.3126,
      "step": 1305
    },
    {
      "epoch": 0.04557231464438066,
      "grad_norm": 4.088114261627197,
      "learning_rate": 0.00017488789237668162,
      "loss": 0.2174,
      "step": 1306
    },
    {
      "epoch": 0.045607209219146655,
      "grad_norm": 13.405600547790527,
      "learning_rate": 0.00017463876432486297,
      "loss": 0.2094,
      "step": 1307
    },
    {
      "epoch": 0.04564210379391264,
      "grad_norm": 3.9598402976989746,
      "learning_rate": 0.00017438963627304434,
      "loss": 0.1696,
      "step": 1308
    },
    {
      "epoch": 0.04567699836867863,
      "grad_norm": 2.773271322250366,
      "learning_rate": 0.00017414050822122572,
      "loss": 0.103,
      "step": 1309
    },
    {
      "epoch": 0.04571189294344462,
      "grad_norm": 13.0413236618042,
      "learning_rate": 0.0001738913801694071,
      "loss": 0.2013,
      "step": 1310
    },
    {
      "epoch": 0.0457467875182106,
      "grad_norm": 3.8556344509124756,
      "learning_rate": 0.00017364225211758845,
      "loss": 0.1035,
      "step": 1311
    },
    {
      "epoch": 0.045781682092976596,
      "grad_norm": 5.853113174438477,
      "learning_rate": 0.00017339312406576982,
      "loss": 0.2014,
      "step": 1312
    },
    {
      "epoch": 0.04581657666774258,
      "grad_norm": 4.3570756912231445,
      "learning_rate": 0.00017314399601395117,
      "loss": 0.1538,
      "step": 1313
    },
    {
      "epoch": 0.045851471242508574,
      "grad_norm": 2.013594627380371,
      "learning_rate": 0.00017289486796213255,
      "loss": 0.1389,
      "step": 1314
    },
    {
      "epoch": 0.04588636581727456,
      "grad_norm": 1.5970252752304077,
      "learning_rate": 0.0001726457399103139,
      "loss": 0.1177,
      "step": 1315
    },
    {
      "epoch": 0.045921260392040544,
      "grad_norm": 1.854695200920105,
      "learning_rate": 0.00017239661185849527,
      "loss": 0.1387,
      "step": 1316
    },
    {
      "epoch": 0.04595615496680654,
      "grad_norm": 2.605154037475586,
      "learning_rate": 0.00017214748380667662,
      "loss": 0.1558,
      "step": 1317
    },
    {
      "epoch": 0.04599104954157252,
      "grad_norm": 1.0010416507720947,
      "learning_rate": 0.000171898355754858,
      "loss": 0.0786,
      "step": 1318
    },
    {
      "epoch": 0.046025944116338514,
      "grad_norm": 6.5527663230896,
      "learning_rate": 0.00017164922770303938,
      "loss": 0.3312,
      "step": 1319
    },
    {
      "epoch": 0.0460608386911045,
      "grad_norm": 3.431190013885498,
      "learning_rate": 0.00017140009965122073,
      "loss": 0.109,
      "step": 1320
    },
    {
      "epoch": 0.04609573326587049,
      "grad_norm": 3.9531524181365967,
      "learning_rate": 0.0001711509715994021,
      "loss": 0.1428,
      "step": 1321
    },
    {
      "epoch": 0.04613062784063648,
      "grad_norm": 1.816645860671997,
      "learning_rate": 0.00017090184354758345,
      "loss": 0.1308,
      "step": 1322
    },
    {
      "epoch": 0.04616552241540246,
      "grad_norm": 3.990892171859741,
      "learning_rate": 0.00017065271549576483,
      "loss": 0.175,
      "step": 1323
    },
    {
      "epoch": 0.046200416990168455,
      "grad_norm": 2.7290806770324707,
      "learning_rate": 0.00017040358744394618,
      "loss": 0.1253,
      "step": 1324
    },
    {
      "epoch": 0.04623531156493444,
      "grad_norm": 2.453200101852417,
      "learning_rate": 0.00017015445939212755,
      "loss": 0.1419,
      "step": 1325
    },
    {
      "epoch": 0.04627020613970043,
      "grad_norm": 4.749731540679932,
      "learning_rate": 0.0001699053313403089,
      "loss": 0.2199,
      "step": 1326
    },
    {
      "epoch": 0.04630510071446642,
      "grad_norm": 2.056443691253662,
      "learning_rate": 0.0001696562032884903,
      "loss": 0.1317,
      "step": 1327
    },
    {
      "epoch": 0.046339995289232404,
      "grad_norm": 1.617377519607544,
      "learning_rate": 0.00016940707523667166,
      "loss": 0.0647,
      "step": 1328
    },
    {
      "epoch": 0.046374889863998396,
      "grad_norm": 4.436970233917236,
      "learning_rate": 0.00016915794718485303,
      "loss": 0.1811,
      "step": 1329
    },
    {
      "epoch": 0.04640978443876438,
      "grad_norm": 5.360097885131836,
      "learning_rate": 0.00016890881913303438,
      "loss": 0.0953,
      "step": 1330
    },
    {
      "epoch": 0.046444679013530374,
      "grad_norm": 3.8779196739196777,
      "learning_rate": 0.00016865969108121576,
      "loss": 0.2183,
      "step": 1331
    },
    {
      "epoch": 0.04647957358829636,
      "grad_norm": 4.467479228973389,
      "learning_rate": 0.0001684105630293971,
      "loss": 0.1062,
      "step": 1332
    },
    {
      "epoch": 0.046514468163062345,
      "grad_norm": 3.1156539916992188,
      "learning_rate": 0.00016816143497757848,
      "loss": 0.2443,
      "step": 1333
    },
    {
      "epoch": 0.04654936273782834,
      "grad_norm": 6.857384204864502,
      "learning_rate": 0.00016791230692575983,
      "loss": 0.2346,
      "step": 1334
    },
    {
      "epoch": 0.04658425731259432,
      "grad_norm": 5.213716983795166,
      "learning_rate": 0.0001676631788739412,
      "loss": 0.2052,
      "step": 1335
    },
    {
      "epoch": 0.046619151887360315,
      "grad_norm": 3.138605833053589,
      "learning_rate": 0.00016741405082212259,
      "loss": 0.0713,
      "step": 1336
    },
    {
      "epoch": 0.0466540464621263,
      "grad_norm": 3.73665714263916,
      "learning_rate": 0.00016716492277030394,
      "loss": 0.1648,
      "step": 1337
    },
    {
      "epoch": 0.04668894103689229,
      "grad_norm": 1.9763035774230957,
      "learning_rate": 0.0001669157947184853,
      "loss": 0.1188,
      "step": 1338
    },
    {
      "epoch": 0.04672383561165828,
      "grad_norm": 1.967949628829956,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.1425,
      "step": 1339
    },
    {
      "epoch": 0.04675873018642426,
      "grad_norm": 0.7493869662284851,
      "learning_rate": 0.00016641753861484804,
      "loss": 0.0396,
      "step": 1340
    },
    {
      "epoch": 0.046793624761190256,
      "grad_norm": 5.38176965713501,
      "learning_rate": 0.0001661684105630294,
      "loss": 0.1555,
      "step": 1341
    },
    {
      "epoch": 0.04682851933595624,
      "grad_norm": 1.8730807304382324,
      "learning_rate": 0.00016591928251121076,
      "loss": 0.0706,
      "step": 1342
    },
    {
      "epoch": 0.04686341391072223,
      "grad_norm": 1.409341812133789,
      "learning_rate": 0.0001656701544593921,
      "loss": 0.0335,
      "step": 1343
    },
    {
      "epoch": 0.04689830848548822,
      "grad_norm": 2.8809776306152344,
      "learning_rate": 0.00016542102640757352,
      "loss": 0.047,
      "step": 1344
    },
    {
      "epoch": 0.046933203060254204,
      "grad_norm": 2.8019204139709473,
      "learning_rate": 0.00016517189835575487,
      "loss": 0.0673,
      "step": 1345
    },
    {
      "epoch": 0.046968097635020196,
      "grad_norm": 6.028367519378662,
      "learning_rate": 0.00016492277030393624,
      "loss": 0.1136,
      "step": 1346
    },
    {
      "epoch": 0.04700299220978618,
      "grad_norm": 4.746762275695801,
      "learning_rate": 0.0001646736422521176,
      "loss": 0.2476,
      "step": 1347
    },
    {
      "epoch": 0.047037886784552174,
      "grad_norm": 4.867337226867676,
      "learning_rate": 0.00016442451420029897,
      "loss": 0.1011,
      "step": 1348
    },
    {
      "epoch": 0.04707278135931816,
      "grad_norm": 2.381422519683838,
      "learning_rate": 0.00016417538614848032,
      "loss": 0.2385,
      "step": 1349
    },
    {
      "epoch": 0.047107675934084145,
      "grad_norm": 8.98931884765625,
      "learning_rate": 0.0001639262580966617,
      "loss": 0.2714,
      "step": 1350
    },
    {
      "epoch": 0.04714257050885014,
      "grad_norm": 4.4314351081848145,
      "learning_rate": 0.00016367713004484304,
      "loss": 0.2912,
      "step": 1351
    },
    {
      "epoch": 0.04717746508361612,
      "grad_norm": 4.531321048736572,
      "learning_rate": 0.0001634280019930244,
      "loss": 0.0987,
      "step": 1352
    },
    {
      "epoch": 0.047212359658382115,
      "grad_norm": 5.49038553237915,
      "learning_rate": 0.0001631788739412058,
      "loss": 0.2039,
      "step": 1353
    },
    {
      "epoch": 0.0472472542331481,
      "grad_norm": 1.0965876579284668,
      "learning_rate": 0.00016292974588938715,
      "loss": 0.0267,
      "step": 1354
    },
    {
      "epoch": 0.04728214880791409,
      "grad_norm": 4.91207218170166,
      "learning_rate": 0.00016268061783756852,
      "loss": 0.1985,
      "step": 1355
    },
    {
      "epoch": 0.04731704338268008,
      "grad_norm": 1.5590742826461792,
      "learning_rate": 0.00016243148978574987,
      "loss": 0.0257,
      "step": 1356
    },
    {
      "epoch": 0.047351937957446064,
      "grad_norm": 5.5331645011901855,
      "learning_rate": 0.00016218236173393125,
      "loss": 0.1826,
      "step": 1357
    },
    {
      "epoch": 0.047386832532212056,
      "grad_norm": 1.1823326349258423,
      "learning_rate": 0.0001619332336821126,
      "loss": 0.0694,
      "step": 1358
    },
    {
      "epoch": 0.04742172710697804,
      "grad_norm": 4.6313958168029785,
      "learning_rate": 0.00016168410563029397,
      "loss": 0.2147,
      "step": 1359
    },
    {
      "epoch": 0.047456621681744034,
      "grad_norm": 3.6359856128692627,
      "learning_rate": 0.00016143497757847532,
      "loss": 0.0642,
      "step": 1360
    },
    {
      "epoch": 0.04749151625651002,
      "grad_norm": 1.816031813621521,
      "learning_rate": 0.00016118584952665673,
      "loss": 0.0587,
      "step": 1361
    },
    {
      "epoch": 0.047526410831276004,
      "grad_norm": 3.423265218734741,
      "learning_rate": 0.00016093672147483808,
      "loss": 0.1265,
      "step": 1362
    },
    {
      "epoch": 0.047561305406042,
      "grad_norm": 2.236441135406494,
      "learning_rate": 0.00016068759342301945,
      "loss": 0.0808,
      "step": 1363
    },
    {
      "epoch": 0.04759619998080798,
      "grad_norm": 1.44280207157135,
      "learning_rate": 0.0001604384653712008,
      "loss": 0.0861,
      "step": 1364
    },
    {
      "epoch": 0.047631094555573975,
      "grad_norm": 2.590358018875122,
      "learning_rate": 0.00016018933731938218,
      "loss": 0.2737,
      "step": 1365
    },
    {
      "epoch": 0.04766598913033996,
      "grad_norm": 2.0915911197662354,
      "learning_rate": 0.00015994020926756353,
      "loss": 0.2024,
      "step": 1366
    },
    {
      "epoch": 0.047700883705105945,
      "grad_norm": 2.551882266998291,
      "learning_rate": 0.0001596910812157449,
      "loss": 0.1774,
      "step": 1367
    },
    {
      "epoch": 0.04773577827987194,
      "grad_norm": 2.156277656555176,
      "learning_rate": 0.00015944195316392625,
      "loss": 0.1487,
      "step": 1368
    },
    {
      "epoch": 0.04777067285463792,
      "grad_norm": 3.044158935546875,
      "learning_rate": 0.0001591928251121076,
      "loss": 0.2287,
      "step": 1369
    },
    {
      "epoch": 0.047805567429403915,
      "grad_norm": 17.1425724029541,
      "learning_rate": 0.000158943697060289,
      "loss": 0.1943,
      "step": 1370
    },
    {
      "epoch": 0.0478404620041699,
      "grad_norm": 6.549684047698975,
      "learning_rate": 0.00015869456900847036,
      "loss": 0.244,
      "step": 1371
    },
    {
      "epoch": 0.04787535657893589,
      "grad_norm": 2.325650215148926,
      "learning_rate": 0.00015844544095665173,
      "loss": 0.1236,
      "step": 1372
    },
    {
      "epoch": 0.04791025115370188,
      "grad_norm": 2.2312915325164795,
      "learning_rate": 0.00015819631290483308,
      "loss": 0.143,
      "step": 1373
    },
    {
      "epoch": 0.047945145728467864,
      "grad_norm": 0.8740969896316528,
      "learning_rate": 0.00015794718485301446,
      "loss": 0.058,
      "step": 1374
    },
    {
      "epoch": 0.047980040303233856,
      "grad_norm": 7.300428867340088,
      "learning_rate": 0.0001576980568011958,
      "loss": 0.2589,
      "step": 1375
    },
    {
      "epoch": 0.04801493487799984,
      "grad_norm": 3.2872021198272705,
      "learning_rate": 0.00015744892874937718,
      "loss": 0.2204,
      "step": 1376
    },
    {
      "epoch": 0.048049829452765834,
      "grad_norm": 3.76269268989563,
      "learning_rate": 0.00015719980069755853,
      "loss": 0.1245,
      "step": 1377
    },
    {
      "epoch": 0.04808472402753182,
      "grad_norm": 4.344870567321777,
      "learning_rate": 0.0001569506726457399,
      "loss": 0.0623,
      "step": 1378
    },
    {
      "epoch": 0.048119618602297805,
      "grad_norm": 2.9769771099090576,
      "learning_rate": 0.00015670154459392129,
      "loss": 0.1488,
      "step": 1379
    },
    {
      "epoch": 0.0481545131770638,
      "grad_norm": 2.614596128463745,
      "learning_rate": 0.00015645241654210266,
      "loss": 0.1735,
      "step": 1380
    },
    {
      "epoch": 0.04818940775182978,
      "grad_norm": 1.9953635931015015,
      "learning_rate": 0.000156203288490284,
      "loss": 0.0606,
      "step": 1381
    },
    {
      "epoch": 0.048224302326595775,
      "grad_norm": 3.4973056316375732,
      "learning_rate": 0.0001559541604384654,
      "loss": 0.135,
      "step": 1382
    },
    {
      "epoch": 0.04825919690136176,
      "grad_norm": 2.196409225463867,
      "learning_rate": 0.00015570503238664674,
      "loss": 0.1528,
      "step": 1383
    },
    {
      "epoch": 0.048294091476127746,
      "grad_norm": 6.0973968505859375,
      "learning_rate": 0.0001554559043348281,
      "loss": 0.1866,
      "step": 1384
    },
    {
      "epoch": 0.04832898605089374,
      "grad_norm": 12.506922721862793,
      "learning_rate": 0.00015520677628300946,
      "loss": 0.1499,
      "step": 1385
    },
    {
      "epoch": 0.04836388062565972,
      "grad_norm": 1.255971074104309,
      "learning_rate": 0.0001549576482311908,
      "loss": 0.02,
      "step": 1386
    },
    {
      "epoch": 0.048398775200425716,
      "grad_norm": 2.1621463298797607,
      "learning_rate": 0.00015470852017937222,
      "loss": 0.1806,
      "step": 1387
    },
    {
      "epoch": 0.0484336697751917,
      "grad_norm": 3.510612964630127,
      "learning_rate": 0.00015445939212755357,
      "loss": 0.1438,
      "step": 1388
    },
    {
      "epoch": 0.048468564349957693,
      "grad_norm": 2.642003059387207,
      "learning_rate": 0.00015421026407573494,
      "loss": 0.1183,
      "step": 1389
    },
    {
      "epoch": 0.04850345892472368,
      "grad_norm": 2.738222122192383,
      "learning_rate": 0.0001539611360239163,
      "loss": 0.0991,
      "step": 1390
    },
    {
      "epoch": 0.048538353499489664,
      "grad_norm": 3.064225673675537,
      "learning_rate": 0.00015371200797209767,
      "loss": 0.1553,
      "step": 1391
    },
    {
      "epoch": 0.04857324807425566,
      "grad_norm": 2.0764098167419434,
      "learning_rate": 0.00015346287992027902,
      "loss": 0.0806,
      "step": 1392
    },
    {
      "epoch": 0.04860814264902164,
      "grad_norm": 5.713259696960449,
      "learning_rate": 0.0001532137518684604,
      "loss": 0.2293,
      "step": 1393
    },
    {
      "epoch": 0.048643037223787634,
      "grad_norm": 2.8869619369506836,
      "learning_rate": 0.00015296462381664174,
      "loss": 0.0813,
      "step": 1394
    },
    {
      "epoch": 0.04867793179855362,
      "grad_norm": 5.780822277069092,
      "learning_rate": 0.00015271549576482312,
      "loss": 0.3543,
      "step": 1395
    },
    {
      "epoch": 0.048712826373319605,
      "grad_norm": 5.018702983856201,
      "learning_rate": 0.0001524663677130045,
      "loss": 0.0792,
      "step": 1396
    },
    {
      "epoch": 0.0487477209480856,
      "grad_norm": 9.009536743164062,
      "learning_rate": 0.00015221723966118587,
      "loss": 0.2475,
      "step": 1397
    },
    {
      "epoch": 0.04878261552285158,
      "grad_norm": 5.0647783279418945,
      "learning_rate": 0.00015196811160936722,
      "loss": 0.1913,
      "step": 1398
    },
    {
      "epoch": 0.048817510097617575,
      "grad_norm": 4.909687519073486,
      "learning_rate": 0.0001517189835575486,
      "loss": 0.1053,
      "step": 1399
    },
    {
      "epoch": 0.04885240467238356,
      "grad_norm": 2.326113224029541,
      "learning_rate": 0.00015146985550572995,
      "loss": 0.103,
      "step": 1400
    },
    {
      "epoch": 0.048887299247149546,
      "grad_norm": 7.0772271156311035,
      "learning_rate": 0.0001512207274539113,
      "loss": 0.2007,
      "step": 1401
    },
    {
      "epoch": 0.04892219382191554,
      "grad_norm": 4.445921897888184,
      "learning_rate": 0.00015097159940209267,
      "loss": 0.1408,
      "step": 1402
    },
    {
      "epoch": 0.048957088396681524,
      "grad_norm": 2.5936827659606934,
      "learning_rate": 0.00015072247135027402,
      "loss": 0.099,
      "step": 1403
    },
    {
      "epoch": 0.048991982971447516,
      "grad_norm": 2.9658994674682617,
      "learning_rate": 0.0001504733432984554,
      "loss": 0.0483,
      "step": 1404
    },
    {
      "epoch": 0.0490268775462135,
      "grad_norm": 2.6044132709503174,
      "learning_rate": 0.00015022421524663678,
      "loss": 0.0366,
      "step": 1405
    },
    {
      "epoch": 0.049061772120979494,
      "grad_norm": 8.425741195678711,
      "learning_rate": 0.00014997508719481815,
      "loss": 0.1251,
      "step": 1406
    },
    {
      "epoch": 0.04909666669574548,
      "grad_norm": 2.684070110321045,
      "learning_rate": 0.0001497259591429995,
      "loss": 0.1759,
      "step": 1407
    },
    {
      "epoch": 0.049131561270511465,
      "grad_norm": 5.891197204589844,
      "learning_rate": 0.00014947683109118088,
      "loss": 0.0558,
      "step": 1408
    },
    {
      "epoch": 0.04916645584527746,
      "grad_norm": 3.4698140621185303,
      "learning_rate": 0.00014922770303936223,
      "loss": 0.1521,
      "step": 1409
    },
    {
      "epoch": 0.04920135042004344,
      "grad_norm": 4.482642650604248,
      "learning_rate": 0.0001489785749875436,
      "loss": 0.1787,
      "step": 1410
    },
    {
      "epoch": 0.049236244994809435,
      "grad_norm": 1.8703007698059082,
      "learning_rate": 0.00014872944693572495,
      "loss": 0.078,
      "step": 1411
    },
    {
      "epoch": 0.04927113956957542,
      "grad_norm": 0.9600440263748169,
      "learning_rate": 0.00014848031888390633,
      "loss": 0.0386,
      "step": 1412
    },
    {
      "epoch": 0.049306034144341405,
      "grad_norm": 2.341438055038452,
      "learning_rate": 0.0001482311908320877,
      "loss": 0.1224,
      "step": 1413
    },
    {
      "epoch": 0.0493409287191074,
      "grad_norm": 3.3635940551757812,
      "learning_rate": 0.00014798206278026908,
      "loss": 0.1866,
      "step": 1414
    },
    {
      "epoch": 0.04937582329387338,
      "grad_norm": 3.06400990486145,
      "learning_rate": 0.00014773293472845043,
      "loss": 0.0991,
      "step": 1415
    },
    {
      "epoch": 0.049410717868639376,
      "grad_norm": 6.026577949523926,
      "learning_rate": 0.0001474838066766318,
      "loss": 0.1422,
      "step": 1416
    },
    {
      "epoch": 0.04944561244340536,
      "grad_norm": 4.121723651885986,
      "learning_rate": 0.00014723467862481316,
      "loss": 0.2379,
      "step": 1417
    },
    {
      "epoch": 0.04948050701817135,
      "grad_norm": 2.2180497646331787,
      "learning_rate": 0.0001469855505729945,
      "loss": 0.2001,
      "step": 1418
    },
    {
      "epoch": 0.04951540159293734,
      "grad_norm": 11.431625366210938,
      "learning_rate": 0.00014673642252117588,
      "loss": 0.4922,
      "step": 1419
    },
    {
      "epoch": 0.049550296167703324,
      "grad_norm": 2.5013763904571533,
      "learning_rate": 0.00014648729446935723,
      "loss": 0.1897,
      "step": 1420
    },
    {
      "epoch": 0.049585190742469316,
      "grad_norm": 2.1656174659729004,
      "learning_rate": 0.0001462381664175386,
      "loss": 0.1736,
      "step": 1421
    },
    {
      "epoch": 0.0496200853172353,
      "grad_norm": 2.5890748500823975,
      "learning_rate": 0.00014598903836571999,
      "loss": 0.0987,
      "step": 1422
    },
    {
      "epoch": 0.049654979892001294,
      "grad_norm": 2.5502681732177734,
      "learning_rate": 0.00014573991031390136,
      "loss": 0.1465,
      "step": 1423
    },
    {
      "epoch": 0.04968987446676728,
      "grad_norm": 6.271544933319092,
      "learning_rate": 0.0001454907822620827,
      "loss": 0.4579,
      "step": 1424
    },
    {
      "epoch": 0.049724769041533265,
      "grad_norm": 3.452338457107544,
      "learning_rate": 0.0001452416542102641,
      "loss": 0.2229,
      "step": 1425
    },
    {
      "epoch": 0.04975966361629926,
      "grad_norm": 2.2425436973571777,
      "learning_rate": 0.00014499252615844544,
      "loss": 0.0977,
      "step": 1426
    },
    {
      "epoch": 0.04979455819106524,
      "grad_norm": 10.243437767028809,
      "learning_rate": 0.0001447433981066268,
      "loss": 0.2411,
      "step": 1427
    },
    {
      "epoch": 0.049829452765831235,
      "grad_norm": 2.337283134460449,
      "learning_rate": 0.00014449427005480816,
      "loss": 0.1284,
      "step": 1428
    },
    {
      "epoch": 0.04986434734059722,
      "grad_norm": 2.6170804500579834,
      "learning_rate": 0.00014424514200298954,
      "loss": 0.1236,
      "step": 1429
    },
    {
      "epoch": 0.049899241915363206,
      "grad_norm": 4.505554676055908,
      "learning_rate": 0.0001439960139511709,
      "loss": 0.1388,
      "step": 1430
    },
    {
      "epoch": 0.0499341364901292,
      "grad_norm": 2.4188151359558105,
      "learning_rate": 0.0001437468858993523,
      "loss": 0.0467,
      "step": 1431
    },
    {
      "epoch": 0.049969031064895184,
      "grad_norm": 0.7463333606719971,
      "learning_rate": 0.00014349775784753364,
      "loss": 0.0525,
      "step": 1432
    },
    {
      "epoch": 0.050003925639661176,
      "grad_norm": 3.064995288848877,
      "learning_rate": 0.00014324862979571502,
      "loss": 0.1449,
      "step": 1433
    },
    {
      "epoch": 0.05003882021442716,
      "grad_norm": 6.994967460632324,
      "learning_rate": 0.00014299950174389637,
      "loss": 0.1716,
      "step": 1434
    },
    {
      "epoch": 0.050073714789193154,
      "grad_norm": 3.6407864093780518,
      "learning_rate": 0.00014275037369207772,
      "loss": 0.1562,
      "step": 1435
    },
    {
      "epoch": 0.05010860936395914,
      "grad_norm": 2.3110249042510986,
      "learning_rate": 0.0001425012456402591,
      "loss": 0.1567,
      "step": 1436
    },
    {
      "epoch": 0.050143503938725124,
      "grad_norm": 1.7327041625976562,
      "learning_rate": 0.00014225211758844044,
      "loss": 0.126,
      "step": 1437
    },
    {
      "epoch": 0.05017839851349112,
      "grad_norm": 0.33741530776023865,
      "learning_rate": 0.00014200298953662182,
      "loss": 0.0148,
      "step": 1438
    },
    {
      "epoch": 0.0502132930882571,
      "grad_norm": 1.6072906255722046,
      "learning_rate": 0.0001417538614848032,
      "loss": 0.0717,
      "step": 1439
    },
    {
      "epoch": 0.050248187663023094,
      "grad_norm": 2.3485283851623535,
      "learning_rate": 0.00014150473343298457,
      "loss": 0.0457,
      "step": 1440
    },
    {
      "epoch": 0.05028308223778908,
      "grad_norm": 1.2452244758605957,
      "learning_rate": 0.00014125560538116592,
      "loss": 0.0294,
      "step": 1441
    },
    {
      "epoch": 0.050317976812555065,
      "grad_norm": 3.173586130142212,
      "learning_rate": 0.0001410064773293473,
      "loss": 0.2354,
      "step": 1442
    },
    {
      "epoch": 0.05035287138732106,
      "grad_norm": 0.9585750102996826,
      "learning_rate": 0.00014075734927752865,
      "loss": 0.0591,
      "step": 1443
    },
    {
      "epoch": 0.05038776596208704,
      "grad_norm": 2.9846625328063965,
      "learning_rate": 0.00014050822122571002,
      "loss": 0.0713,
      "step": 1444
    },
    {
      "epoch": 0.050422660536853035,
      "grad_norm": 1.3299602270126343,
      "learning_rate": 0.00014025909317389137,
      "loss": 0.0835,
      "step": 1445
    },
    {
      "epoch": 0.05045755511161902,
      "grad_norm": 3.6595053672790527,
      "learning_rate": 0.00014000996512207275,
      "loss": 0.0637,
      "step": 1446
    },
    {
      "epoch": 0.050492449686385006,
      "grad_norm": 1.295790195465088,
      "learning_rate": 0.0001397608370702541,
      "loss": 0.0433,
      "step": 1447
    },
    {
      "epoch": 0.050527344261151,
      "grad_norm": 3.5346338748931885,
      "learning_rate": 0.0001395117090184355,
      "loss": 0.075,
      "step": 1448
    },
    {
      "epoch": 0.050562238835916984,
      "grad_norm": 3.5817196369171143,
      "learning_rate": 0.00013926258096661685,
      "loss": 0.1531,
      "step": 1449
    },
    {
      "epoch": 0.050597133410682976,
      "grad_norm": 3.0796282291412354,
      "learning_rate": 0.00013901345291479823,
      "loss": 0.1727,
      "step": 1450
    },
    {
      "epoch": 0.05063202798544896,
      "grad_norm": 3.286593198776245,
      "learning_rate": 0.00013876432486297958,
      "loss": 0.1092,
      "step": 1451
    },
    {
      "epoch": 0.050666922560214954,
      "grad_norm": 1.9222848415374756,
      "learning_rate": 0.00013851519681116093,
      "loss": 0.1785,
      "step": 1452
    },
    {
      "epoch": 0.05070181713498094,
      "grad_norm": 4.498049736022949,
      "learning_rate": 0.0001382660687593423,
      "loss": 0.2366,
      "step": 1453
    },
    {
      "epoch": 0.050736711709746925,
      "grad_norm": 4.193946838378906,
      "learning_rate": 0.00013801694070752365,
      "loss": 0.0535,
      "step": 1454
    },
    {
      "epoch": 0.05077160628451292,
      "grad_norm": 1.9007725715637207,
      "learning_rate": 0.00013776781265570503,
      "loss": 0.1456,
      "step": 1455
    },
    {
      "epoch": 0.0508065008592789,
      "grad_norm": 2.456624746322632,
      "learning_rate": 0.00013751868460388638,
      "loss": 0.0775,
      "step": 1456
    },
    {
      "epoch": 0.050841395434044895,
      "grad_norm": 3.4755399227142334,
      "learning_rate": 0.00013726955655206778,
      "loss": 0.0578,
      "step": 1457
    },
    {
      "epoch": 0.05087629000881088,
      "grad_norm": 1.499006748199463,
      "learning_rate": 0.00013702042850024913,
      "loss": 0.0803,
      "step": 1458
    },
    {
      "epoch": 0.050911184583576866,
      "grad_norm": 4.962594509124756,
      "learning_rate": 0.0001367713004484305,
      "loss": 0.1906,
      "step": 1459
    },
    {
      "epoch": 0.05094607915834286,
      "grad_norm": 2.338643789291382,
      "learning_rate": 0.00013652217239661186,
      "loss": 0.0473,
      "step": 1460
    },
    {
      "epoch": 0.05098097373310884,
      "grad_norm": 0.4247048795223236,
      "learning_rate": 0.00013627304434479323,
      "loss": 0.0198,
      "step": 1461
    },
    {
      "epoch": 0.051015868307874836,
      "grad_norm": 0.4436851739883423,
      "learning_rate": 0.00013602391629297458,
      "loss": 0.0173,
      "step": 1462
    },
    {
      "epoch": 0.05105076288264082,
      "grad_norm": 4.007726669311523,
      "learning_rate": 0.00013577478824115596,
      "loss": 0.1192,
      "step": 1463
    },
    {
      "epoch": 0.051085657457406806,
      "grad_norm": 9.211669921875,
      "learning_rate": 0.0001355256601893373,
      "loss": 0.0838,
      "step": 1464
    },
    {
      "epoch": 0.0511205520321728,
      "grad_norm": 4.481112957000732,
      "learning_rate": 0.0001352765321375187,
      "loss": 0.0674,
      "step": 1465
    },
    {
      "epoch": 0.051155446606938784,
      "grad_norm": 5.266711711883545,
      "learning_rate": 0.00013502740408570006,
      "loss": 0.2417,
      "step": 1466
    },
    {
      "epoch": 0.05119034118170478,
      "grad_norm": 2.7824387550354004,
      "learning_rate": 0.00013477827603388144,
      "loss": 0.1877,
      "step": 1467
    },
    {
      "epoch": 0.05122523575647076,
      "grad_norm": 1.0036758184432983,
      "learning_rate": 0.0001345291479820628,
      "loss": 0.0347,
      "step": 1468
    },
    {
      "epoch": 0.051260130331236754,
      "grad_norm": 8.66191577911377,
      "learning_rate": 0.00013428001993024414,
      "loss": 0.3305,
      "step": 1469
    },
    {
      "epoch": 0.05129502490600274,
      "grad_norm": 4.107899188995361,
      "learning_rate": 0.0001340308918784255,
      "loss": 0.0767,
      "step": 1470
    },
    {
      "epoch": 0.051329919480768725,
      "grad_norm": 2.6682026386260986,
      "learning_rate": 0.00013378176382660686,
      "loss": 0.082,
      "step": 1471
    },
    {
      "epoch": 0.05136481405553472,
      "grad_norm": 4.338529586791992,
      "learning_rate": 0.00013353263577478824,
      "loss": 0.1453,
      "step": 1472
    },
    {
      "epoch": 0.0513997086303007,
      "grad_norm": 1.4106131792068481,
      "learning_rate": 0.0001332835077229696,
      "loss": 0.0668,
      "step": 1473
    },
    {
      "epoch": 0.051434603205066695,
      "grad_norm": 6.555473327636719,
      "learning_rate": 0.000133034379671151,
      "loss": 0.2892,
      "step": 1474
    },
    {
      "epoch": 0.05146949777983268,
      "grad_norm": 2.5189168453216553,
      "learning_rate": 0.00013278525161933234,
      "loss": 0.0621,
      "step": 1475
    },
    {
      "epoch": 0.051504392354598666,
      "grad_norm": 5.234265327453613,
      "learning_rate": 0.00013253612356751372,
      "loss": 0.2809,
      "step": 1476
    },
    {
      "epoch": 0.05153928692936466,
      "grad_norm": 2.3306610584259033,
      "learning_rate": 0.00013228699551569507,
      "loss": 0.1249,
      "step": 1477
    },
    {
      "epoch": 0.051574181504130644,
      "grad_norm": 7.370274543762207,
      "learning_rate": 0.00013203786746387644,
      "loss": 0.2622,
      "step": 1478
    },
    {
      "epoch": 0.051609076078896636,
      "grad_norm": 1.9584001302719116,
      "learning_rate": 0.0001317887394120578,
      "loss": 0.044,
      "step": 1479
    },
    {
      "epoch": 0.05164397065366262,
      "grad_norm": 11.602933883666992,
      "learning_rate": 0.00013153961136023917,
      "loss": 0.1895,
      "step": 1480
    },
    {
      "epoch": 0.05167886522842861,
      "grad_norm": 6.422626495361328,
      "learning_rate": 0.00013129048330842052,
      "loss": 0.2584,
      "step": 1481
    },
    {
      "epoch": 0.0517137598031946,
      "grad_norm": 1.1839687824249268,
      "learning_rate": 0.0001310413552566019,
      "loss": 0.0516,
      "step": 1482
    },
    {
      "epoch": 0.051748654377960585,
      "grad_norm": 4.131566524505615,
      "learning_rate": 0.00013079222720478327,
      "loss": 0.4126,
      "step": 1483
    },
    {
      "epoch": 0.05178354895272658,
      "grad_norm": 27.352367401123047,
      "learning_rate": 0.00013054309915296465,
      "loss": 0.2327,
      "step": 1484
    },
    {
      "epoch": 0.05181844352749256,
      "grad_norm": 2.150491714477539,
      "learning_rate": 0.000130293971101146,
      "loss": 0.0798,
      "step": 1485
    },
    {
      "epoch": 0.051853338102258555,
      "grad_norm": 2.720590353012085,
      "learning_rate": 0.00013004484304932735,
      "loss": 0.1588,
      "step": 1486
    },
    {
      "epoch": 0.05188823267702454,
      "grad_norm": 5.676937580108643,
      "learning_rate": 0.00012979571499750872,
      "loss": 0.0589,
      "step": 1487
    },
    {
      "epoch": 0.051923127251790525,
      "grad_norm": 0.6387702226638794,
      "learning_rate": 0.00012954658694569007,
      "loss": 0.0462,
      "step": 1488
    },
    {
      "epoch": 0.05195802182655652,
      "grad_norm": 3.725504159927368,
      "learning_rate": 0.00012929745889387145,
      "loss": 0.121,
      "step": 1489
    },
    {
      "epoch": 0.0519929164013225,
      "grad_norm": 1.2217813730239868,
      "learning_rate": 0.0001290483308420528,
      "loss": 0.0375,
      "step": 1490
    },
    {
      "epoch": 0.052027810976088495,
      "grad_norm": 4.899023532867432,
      "learning_rate": 0.00012879920279023417,
      "loss": 0.2665,
      "step": 1491
    },
    {
      "epoch": 0.05206270555085448,
      "grad_norm": 3.8915023803710938,
      "learning_rate": 0.00012855007473841555,
      "loss": 0.1506,
      "step": 1492
    },
    {
      "epoch": 0.052097600125620466,
      "grad_norm": 3.159414768218994,
      "learning_rate": 0.00012830094668659693,
      "loss": 0.1669,
      "step": 1493
    },
    {
      "epoch": 0.05213249470038646,
      "grad_norm": 6.634977340698242,
      "learning_rate": 0.00012805181863477828,
      "loss": 0.2468,
      "step": 1494
    },
    {
      "epoch": 0.052167389275152444,
      "grad_norm": 2.902862787246704,
      "learning_rate": 0.00012780269058295965,
      "loss": 0.1286,
      "step": 1495
    },
    {
      "epoch": 0.052202283849918436,
      "grad_norm": 5.618625640869141,
      "learning_rate": 0.000127553562531141,
      "loss": 0.3281,
      "step": 1496
    },
    {
      "epoch": 0.05223717842468442,
      "grad_norm": 3.1843695640563965,
      "learning_rate": 0.00012730443447932238,
      "loss": 0.0862,
      "step": 1497
    },
    {
      "epoch": 0.05227207299945041,
      "grad_norm": 1.5303057432174683,
      "learning_rate": 0.00012705530642750373,
      "loss": 0.0696,
      "step": 1498
    },
    {
      "epoch": 0.0523069675742164,
      "grad_norm": 2.195768356323242,
      "learning_rate": 0.0001268061783756851,
      "loss": 0.1933,
      "step": 1499
    },
    {
      "epoch": 0.052341862148982385,
      "grad_norm": 3.5080854892730713,
      "learning_rate": 0.00012655705032386648,
      "loss": 0.0963,
      "step": 1500
    },
    {
      "epoch": 0.05237675672374838,
      "grad_norm": 2.5194976329803467,
      "learning_rate": 0.00012630792227204786,
      "loss": 0.1384,
      "step": 1501
    },
    {
      "epoch": 0.05241165129851436,
      "grad_norm": 10.312322616577148,
      "learning_rate": 0.0001260587942202292,
      "loss": 0.3544,
      "step": 1502
    },
    {
      "epoch": 0.052446545873280355,
      "grad_norm": 5.99788236618042,
      "learning_rate": 0.00012580966616841056,
      "loss": 0.2467,
      "step": 1503
    },
    {
      "epoch": 0.05248144044804634,
      "grad_norm": 3.249239921569824,
      "learning_rate": 0.00012556053811659193,
      "loss": 0.0677,
      "step": 1504
    },
    {
      "epoch": 0.052516335022812326,
      "grad_norm": 3.0156586170196533,
      "learning_rate": 0.00012531141006477328,
      "loss": 0.2048,
      "step": 1505
    },
    {
      "epoch": 0.05255122959757832,
      "grad_norm": 1.9208645820617676,
      "learning_rate": 0.00012506228201295466,
      "loss": 0.0472,
      "step": 1506
    },
    {
      "epoch": 0.052586124172344303,
      "grad_norm": 1.9037455320358276,
      "learning_rate": 0.00012481315396113603,
      "loss": 0.1368,
      "step": 1507
    },
    {
      "epoch": 0.052621018747110296,
      "grad_norm": 1.7813506126403809,
      "learning_rate": 0.00012456402590931738,
      "loss": 0.1473,
      "step": 1508
    },
    {
      "epoch": 0.05265591332187628,
      "grad_norm": 2.4162797927856445,
      "learning_rate": 0.00012431489785749876,
      "loss": 0.1272,
      "step": 1509
    },
    {
      "epoch": 0.05269080789664227,
      "grad_norm": 3.9764044284820557,
      "learning_rate": 0.00012406576980568014,
      "loss": 0.1298,
      "step": 1510
    },
    {
      "epoch": 0.05272570247140826,
      "grad_norm": 4.735785007476807,
      "learning_rate": 0.0001238166417538615,
      "loss": 0.2136,
      "step": 1511
    },
    {
      "epoch": 0.052760597046174244,
      "grad_norm": 3.0611989498138428,
      "learning_rate": 0.00012356751370204284,
      "loss": 0.0929,
      "step": 1512
    },
    {
      "epoch": 0.05279549162094024,
      "grad_norm": 3.503537178039551,
      "learning_rate": 0.0001233183856502242,
      "loss": 0.09,
      "step": 1513
    },
    {
      "epoch": 0.05283038619570622,
      "grad_norm": 1.9576935768127441,
      "learning_rate": 0.0001230692575984056,
      "loss": 0.0974,
      "step": 1514
    },
    {
      "epoch": 0.05286528077047221,
      "grad_norm": 1.322506308555603,
      "learning_rate": 0.00012282012954658694,
      "loss": 0.0479,
      "step": 1515
    },
    {
      "epoch": 0.0529001753452382,
      "grad_norm": 1.6599950790405273,
      "learning_rate": 0.00012257100149476831,
      "loss": 0.0777,
      "step": 1516
    },
    {
      "epoch": 0.052935069920004185,
      "grad_norm": 6.715090751647949,
      "learning_rate": 0.00012232187344294966,
      "loss": 0.2851,
      "step": 1517
    },
    {
      "epoch": 0.05296996449477018,
      "grad_norm": 2.981527328491211,
      "learning_rate": 0.00012207274539113104,
      "loss": 0.0882,
      "step": 1518
    },
    {
      "epoch": 0.05300485906953616,
      "grad_norm": 1.6927670240402222,
      "learning_rate": 0.0001218236173393124,
      "loss": 0.0878,
      "step": 1519
    },
    {
      "epoch": 0.053039753644302155,
      "grad_norm": 1.8772988319396973,
      "learning_rate": 0.00012157448928749378,
      "loss": 0.0796,
      "step": 1520
    },
    {
      "epoch": 0.05307464821906814,
      "grad_norm": 0.8992725014686584,
      "learning_rate": 0.00012132536123567514,
      "loss": 0.0423,
      "step": 1521
    },
    {
      "epoch": 0.053109542793834126,
      "grad_norm": 0.9571605324745178,
      "learning_rate": 0.0001210762331838565,
      "loss": 0.0554,
      "step": 1522
    },
    {
      "epoch": 0.05314443736860012,
      "grad_norm": 1.590519905090332,
      "learning_rate": 0.00012082710513203787,
      "loss": 0.0667,
      "step": 1523
    },
    {
      "epoch": 0.053179331943366104,
      "grad_norm": 4.2184157371521,
      "learning_rate": 0.00012057797708021924,
      "loss": 0.0699,
      "step": 1524
    },
    {
      "epoch": 0.053214226518132096,
      "grad_norm": 2.4164323806762695,
      "learning_rate": 0.00012032884902840061,
      "loss": 0.0971,
      "step": 1525
    },
    {
      "epoch": 0.05324912109289808,
      "grad_norm": 3.057065486907959,
      "learning_rate": 0.00012007972097658197,
      "loss": 0.0519,
      "step": 1526
    },
    {
      "epoch": 0.05328401566766407,
      "grad_norm": 1.90810227394104,
      "learning_rate": 0.00011983059292476333,
      "loss": 0.0318,
      "step": 1527
    },
    {
      "epoch": 0.05331891024243006,
      "grad_norm": 4.324715614318848,
      "learning_rate": 0.00011958146487294468,
      "loss": 0.1385,
      "step": 1528
    },
    {
      "epoch": 0.053353804817196045,
      "grad_norm": 1.296869158744812,
      "learning_rate": 0.00011933233682112606,
      "loss": 0.0341,
      "step": 1529
    },
    {
      "epoch": 0.05338869939196204,
      "grad_norm": 1.1390693187713623,
      "learning_rate": 0.00011908320876930742,
      "loss": 0.0534,
      "step": 1530
    },
    {
      "epoch": 0.05342359396672802,
      "grad_norm": 2.8130364418029785,
      "learning_rate": 0.00011883408071748879,
      "loss": 0.0402,
      "step": 1531
    },
    {
      "epoch": 0.05345848854149401,
      "grad_norm": 2.7163705825805664,
      "learning_rate": 0.00011858495266567015,
      "loss": 0.1887,
      "step": 1532
    },
    {
      "epoch": 0.05349338311626,
      "grad_norm": 9.524260520935059,
      "learning_rate": 0.00011833582461385152,
      "loss": 0.2288,
      "step": 1533
    },
    {
      "epoch": 0.053528277691025986,
      "grad_norm": 1.468475103378296,
      "learning_rate": 0.00011808669656203289,
      "loss": 0.0723,
      "step": 1534
    },
    {
      "epoch": 0.05356317226579198,
      "grad_norm": 2.1367902755737305,
      "learning_rate": 0.00011783756851021425,
      "loss": 0.0891,
      "step": 1535
    },
    {
      "epoch": 0.05359806684055796,
      "grad_norm": 2.256938934326172,
      "learning_rate": 0.00011758844045839561,
      "loss": 0.0772,
      "step": 1536
    },
    {
      "epoch": 0.053632961415323956,
      "grad_norm": 4.017137050628662,
      "learning_rate": 0.00011733931240657699,
      "loss": 0.0883,
      "step": 1537
    },
    {
      "epoch": 0.05366785599008994,
      "grad_norm": 3.316708564758301,
      "learning_rate": 0.00011709018435475835,
      "loss": 0.1534,
      "step": 1538
    },
    {
      "epoch": 0.053702750564855926,
      "grad_norm": 6.343635082244873,
      "learning_rate": 0.00011684105630293972,
      "loss": 0.1258,
      "step": 1539
    },
    {
      "epoch": 0.05373764513962192,
      "grad_norm": 7.751708030700684,
      "learning_rate": 0.00011659192825112108,
      "loss": 0.2951,
      "step": 1540
    },
    {
      "epoch": 0.053772539714387904,
      "grad_norm": 0.863747775554657,
      "learning_rate": 0.00011634280019930245,
      "loss": 0.0393,
      "step": 1541
    },
    {
      "epoch": 0.053807434289153896,
      "grad_norm": 2.2208175659179688,
      "learning_rate": 0.00011609367214748382,
      "loss": 0.1318,
      "step": 1542
    },
    {
      "epoch": 0.05384232886391988,
      "grad_norm": 0.38235586881637573,
      "learning_rate": 0.00011584454409566518,
      "loss": 0.0131,
      "step": 1543
    },
    {
      "epoch": 0.05387722343868587,
      "grad_norm": 10.58935832977295,
      "learning_rate": 0.00011559541604384654,
      "loss": 0.1795,
      "step": 1544
    },
    {
      "epoch": 0.05391211801345186,
      "grad_norm": 3.151087760925293,
      "learning_rate": 0.00011534628799202789,
      "loss": 0.1538,
      "step": 1545
    },
    {
      "epoch": 0.053947012588217845,
      "grad_norm": 1.8023674488067627,
      "learning_rate": 0.00011509715994020927,
      "loss": 0.0367,
      "step": 1546
    },
    {
      "epoch": 0.05398190716298384,
      "grad_norm": 5.8230299949646,
      "learning_rate": 0.00011484803188839063,
      "loss": 0.1658,
      "step": 1547
    },
    {
      "epoch": 0.05401680173774982,
      "grad_norm": 1.9010555744171143,
      "learning_rate": 0.000114598903836572,
      "loss": 0.0402,
      "step": 1548
    },
    {
      "epoch": 0.05405169631251581,
      "grad_norm": 3.114553213119507,
      "learning_rate": 0.00011434977578475336,
      "loss": 0.0422,
      "step": 1549
    },
    {
      "epoch": 0.0540865908872818,
      "grad_norm": 1.8305104970932007,
      "learning_rate": 0.00011410064773293473,
      "loss": 0.0275,
      "step": 1550
    },
    {
      "epoch": 0.054121485462047786,
      "grad_norm": 10.215521812438965,
      "learning_rate": 0.0001138515196811161,
      "loss": 0.1152,
      "step": 1551
    },
    {
      "epoch": 0.05415638003681378,
      "grad_norm": 4.342519283294678,
      "learning_rate": 0.00011360239162929746,
      "loss": 0.1008,
      "step": 1552
    },
    {
      "epoch": 0.054191274611579764,
      "grad_norm": 4.983795642852783,
      "learning_rate": 0.00011335326357747882,
      "loss": 0.2184,
      "step": 1553
    },
    {
      "epoch": 0.054226169186345756,
      "grad_norm": 4.917092800140381,
      "learning_rate": 0.00011310413552566019,
      "loss": 0.2029,
      "step": 1554
    },
    {
      "epoch": 0.05426106376111174,
      "grad_norm": 6.507162570953369,
      "learning_rate": 0.00011285500747384156,
      "loss": 0.2356,
      "step": 1555
    },
    {
      "epoch": 0.05429595833587773,
      "grad_norm": 1.5839037895202637,
      "learning_rate": 0.00011260587942202293,
      "loss": 0.1119,
      "step": 1556
    },
    {
      "epoch": 0.05433085291064372,
      "grad_norm": 2.7431559562683105,
      "learning_rate": 0.00011235675137020429,
      "loss": 0.0653,
      "step": 1557
    },
    {
      "epoch": 0.054365747485409704,
      "grad_norm": 4.12769889831543,
      "learning_rate": 0.00011210762331838565,
      "loss": 0.1345,
      "step": 1558
    },
    {
      "epoch": 0.0544006420601757,
      "grad_norm": 2.2880635261535645,
      "learning_rate": 0.00011185849526656703,
      "loss": 0.1812,
      "step": 1559
    },
    {
      "epoch": 0.05443553663494168,
      "grad_norm": 4.5066962242126465,
      "learning_rate": 0.00011160936721474839,
      "loss": 0.1137,
      "step": 1560
    },
    {
      "epoch": 0.05447043120970767,
      "grad_norm": 2.8132474422454834,
      "learning_rate": 0.00011136023916292975,
      "loss": 0.1751,
      "step": 1561
    },
    {
      "epoch": 0.05450532578447366,
      "grad_norm": 2.776254892349243,
      "learning_rate": 0.0001111111111111111,
      "loss": 0.0491,
      "step": 1562
    },
    {
      "epoch": 0.054540220359239645,
      "grad_norm": 2.119154214859009,
      "learning_rate": 0.00011086198305929248,
      "loss": 0.0958,
      "step": 1563
    },
    {
      "epoch": 0.05457511493400564,
      "grad_norm": 1.7108097076416016,
      "learning_rate": 0.00011061285500747384,
      "loss": 0.0715,
      "step": 1564
    },
    {
      "epoch": 0.05461000950877162,
      "grad_norm": 2.843794822692871,
      "learning_rate": 0.0001103637269556552,
      "loss": 0.0542,
      "step": 1565
    },
    {
      "epoch": 0.054644904083537615,
      "grad_norm": 1.2541002035140991,
      "learning_rate": 0.00011011459890383657,
      "loss": 0.0612,
      "step": 1566
    },
    {
      "epoch": 0.0546797986583036,
      "grad_norm": 1.151776671409607,
      "learning_rate": 0.00010986547085201793,
      "loss": 0.0312,
      "step": 1567
    },
    {
      "epoch": 0.054714693233069586,
      "grad_norm": 1.916211485862732,
      "learning_rate": 0.00010961634280019931,
      "loss": 0.0283,
      "step": 1568
    },
    {
      "epoch": 0.05474958780783558,
      "grad_norm": 3.101287364959717,
      "learning_rate": 0.00010936721474838067,
      "loss": 0.1049,
      "step": 1569
    },
    {
      "epoch": 0.054784482382601564,
      "grad_norm": 5.578420639038086,
      "learning_rate": 0.00010911808669656203,
      "loss": 0.3648,
      "step": 1570
    },
    {
      "epoch": 0.054819376957367556,
      "grad_norm": 2.008315324783325,
      "learning_rate": 0.0001088689586447434,
      "loss": 0.0625,
      "step": 1571
    },
    {
      "epoch": 0.05485427153213354,
      "grad_norm": 10.692476272583008,
      "learning_rate": 0.00010861983059292477,
      "loss": 0.2639,
      "step": 1572
    },
    {
      "epoch": 0.05488916610689953,
      "grad_norm": 4.8324174880981445,
      "learning_rate": 0.00010837070254110614,
      "loss": 0.2317,
      "step": 1573
    },
    {
      "epoch": 0.05492406068166552,
      "grad_norm": 1.4197882413864136,
      "learning_rate": 0.0001081215744892875,
      "loss": 0.0477,
      "step": 1574
    },
    {
      "epoch": 0.054958955256431505,
      "grad_norm": 3.0639874935150146,
      "learning_rate": 0.00010787244643746886,
      "loss": 0.1574,
      "step": 1575
    },
    {
      "epoch": 0.0549938498311975,
      "grad_norm": 1.7016478776931763,
      "learning_rate": 0.00010762331838565024,
      "loss": 0.1072,
      "step": 1576
    },
    {
      "epoch": 0.05502874440596348,
      "grad_norm": 0.9361273646354675,
      "learning_rate": 0.0001073741903338316,
      "loss": 0.0169,
      "step": 1577
    },
    {
      "epoch": 0.05506363898072947,
      "grad_norm": 4.014181613922119,
      "learning_rate": 0.00010712506228201295,
      "loss": 0.0563,
      "step": 1578
    },
    {
      "epoch": 0.05509853355549546,
      "grad_norm": 0.7089726328849792,
      "learning_rate": 0.00010687593423019431,
      "loss": 0.0233,
      "step": 1579
    },
    {
      "epoch": 0.055133428130261446,
      "grad_norm": 2.664273262023926,
      "learning_rate": 0.00010662680617837568,
      "loss": 0.0982,
      "step": 1580
    },
    {
      "epoch": 0.05516832270502744,
      "grad_norm": 1.5542949438095093,
      "learning_rate": 0.00010637767812655705,
      "loss": 0.0306,
      "step": 1581
    },
    {
      "epoch": 0.05520321727979342,
      "grad_norm": 2.0329642295837402,
      "learning_rate": 0.00010612855007473842,
      "loss": 0.0231,
      "step": 1582
    },
    {
      "epoch": 0.055238111854559416,
      "grad_norm": 7.373058795928955,
      "learning_rate": 0.00010587942202291978,
      "loss": 0.1599,
      "step": 1583
    },
    {
      "epoch": 0.0552730064293254,
      "grad_norm": 2.157273769378662,
      "learning_rate": 0.00010563029397110114,
      "loss": 0.1033,
      "step": 1584
    },
    {
      "epoch": 0.05530790100409139,
      "grad_norm": 4.46662712097168,
      "learning_rate": 0.00010538116591928252,
      "loss": 0.1403,
      "step": 1585
    },
    {
      "epoch": 0.05534279557885738,
      "grad_norm": 5.304354190826416,
      "learning_rate": 0.00010513203786746388,
      "loss": 0.1127,
      "step": 1586
    },
    {
      "epoch": 0.055377690153623364,
      "grad_norm": 6.010459899902344,
      "learning_rate": 0.00010488290981564524,
      "loss": 0.1457,
      "step": 1587
    },
    {
      "epoch": 0.05541258472838936,
      "grad_norm": 0.8688355684280396,
      "learning_rate": 0.0001046337817638266,
      "loss": 0.0111,
      "step": 1588
    },
    {
      "epoch": 0.05544747930315534,
      "grad_norm": 1.5313550233840942,
      "learning_rate": 0.00010438465371200798,
      "loss": 0.0557,
      "step": 1589
    },
    {
      "epoch": 0.05548237387792133,
      "grad_norm": 1.5179167985916138,
      "learning_rate": 0.00010413552566018935,
      "loss": 0.042,
      "step": 1590
    },
    {
      "epoch": 0.05551726845268732,
      "grad_norm": 3.408689498901367,
      "learning_rate": 0.00010388639760837071,
      "loss": 0.1312,
      "step": 1591
    },
    {
      "epoch": 0.055552163027453305,
      "grad_norm": 4.104863166809082,
      "learning_rate": 0.00010363726955655207,
      "loss": 0.0661,
      "step": 1592
    },
    {
      "epoch": 0.0555870576022193,
      "grad_norm": 2.7412774562835693,
      "learning_rate": 0.00010338814150473343,
      "loss": 0.0288,
      "step": 1593
    },
    {
      "epoch": 0.05562195217698528,
      "grad_norm": 3.9233920574188232,
      "learning_rate": 0.00010313901345291481,
      "loss": 0.12,
      "step": 1594
    },
    {
      "epoch": 0.05565684675175127,
      "grad_norm": 1.7059804201126099,
      "learning_rate": 0.00010288988540109616,
      "loss": 0.0637,
      "step": 1595
    },
    {
      "epoch": 0.05569174132651726,
      "grad_norm": 3.1076056957244873,
      "learning_rate": 0.00010264075734927752,
      "loss": 0.0509,
      "step": 1596
    },
    {
      "epoch": 0.055726635901283246,
      "grad_norm": 0.8715668320655823,
      "learning_rate": 0.00010239162929745889,
      "loss": 0.0265,
      "step": 1597
    },
    {
      "epoch": 0.05576153047604924,
      "grad_norm": 10.472412109375,
      "learning_rate": 0.00010214250124564026,
      "loss": 0.2293,
      "step": 1598
    },
    {
      "epoch": 0.055796425050815224,
      "grad_norm": 4.631590843200684,
      "learning_rate": 0.00010189337319382163,
      "loss": 0.2162,
      "step": 1599
    },
    {
      "epoch": 0.055831319625581216,
      "grad_norm": 0.8885453939437866,
      "learning_rate": 0.00010164424514200299,
      "loss": 0.0287,
      "step": 1600
    },
    {
      "epoch": 0.0558662142003472,
      "grad_norm": 2.096383810043335,
      "learning_rate": 0.00010139511709018435,
      "loss": 0.0288,
      "step": 1601
    },
    {
      "epoch": 0.05590110877511319,
      "grad_norm": 4.728913307189941,
      "learning_rate": 0.00010114598903836573,
      "loss": 0.0846,
      "step": 1602
    },
    {
      "epoch": 0.05593600334987918,
      "grad_norm": 1.9346922636032104,
      "learning_rate": 0.00010089686098654709,
      "loss": 0.0462,
      "step": 1603
    },
    {
      "epoch": 0.055970897924645165,
      "grad_norm": 3.339768409729004,
      "learning_rate": 0.00010064773293472845,
      "loss": 0.2565,
      "step": 1604
    },
    {
      "epoch": 0.05600579249941116,
      "grad_norm": 5.833096504211426,
      "learning_rate": 0.00010039860488290982,
      "loss": 0.1886,
      "step": 1605
    },
    {
      "epoch": 0.05604068707417714,
      "grad_norm": 2.947484016418457,
      "learning_rate": 0.00010014947683109118,
      "loss": 0.1229,
      "step": 1606
    },
    {
      "epoch": 0.05607558164894313,
      "grad_norm": 1.1108195781707764,
      "learning_rate": 9.990034877927256e-05,
      "loss": 0.037,
      "step": 1607
    },
    {
      "epoch": 0.05611047622370912,
      "grad_norm": 0.29022496938705444,
      "learning_rate": 9.965122072745392e-05,
      "loss": 0.0052,
      "step": 1608
    },
    {
      "epoch": 0.056145370798475105,
      "grad_norm": 1.0349451303482056,
      "learning_rate": 9.940209267563528e-05,
      "loss": 0.0204,
      "step": 1609
    },
    {
      "epoch": 0.0561802653732411,
      "grad_norm": 6.454912185668945,
      "learning_rate": 9.915296462381664e-05,
      "loss": 0.1866,
      "step": 1610
    },
    {
      "epoch": 0.05621515994800708,
      "grad_norm": 0.5099454522132874,
      "learning_rate": 9.890383657199802e-05,
      "loss": 0.0079,
      "step": 1611
    },
    {
      "epoch": 0.05625005452277307,
      "grad_norm": 1.6539640426635742,
      "learning_rate": 9.865470852017937e-05,
      "loss": 0.0297,
      "step": 1612
    },
    {
      "epoch": 0.05628494909753906,
      "grad_norm": 4.394382476806641,
      "learning_rate": 9.840558046836073e-05,
      "loss": 0.0743,
      "step": 1613
    },
    {
      "epoch": 0.056319843672305046,
      "grad_norm": 2.9309890270233154,
      "learning_rate": 9.81564524165421e-05,
      "loss": 0.067,
      "step": 1614
    },
    {
      "epoch": 0.05635473824707104,
      "grad_norm": 0.9927437901496887,
      "learning_rate": 9.790732436472347e-05,
      "loss": 0.0249,
      "step": 1615
    },
    {
      "epoch": 0.056389632821837024,
      "grad_norm": 2.9189367294311523,
      "learning_rate": 9.765819631290484e-05,
      "loss": 0.056,
      "step": 1616
    },
    {
      "epoch": 0.056424527396603016,
      "grad_norm": 3.3195576667785645,
      "learning_rate": 9.74090682610862e-05,
      "loss": 0.1336,
      "step": 1617
    },
    {
      "epoch": 0.056459421971369,
      "grad_norm": 1.4485371112823486,
      "learning_rate": 9.715994020926756e-05,
      "loss": 0.0573,
      "step": 1618
    },
    {
      "epoch": 0.05649431654613499,
      "grad_norm": 0.1639367938041687,
      "learning_rate": 9.691081215744892e-05,
      "loss": 0.0044,
      "step": 1619
    },
    {
      "epoch": 0.05652921112090098,
      "grad_norm": 11.786614418029785,
      "learning_rate": 9.66616841056303e-05,
      "loss": 0.1098,
      "step": 1620
    },
    {
      "epoch": 0.056564105695666965,
      "grad_norm": 4.569461345672607,
      "learning_rate": 9.641255605381166e-05,
      "loss": 0.1965,
      "step": 1621
    },
    {
      "epoch": 0.05659900027043296,
      "grad_norm": 7.133143901824951,
      "learning_rate": 9.616342800199303e-05,
      "loss": 0.1288,
      "step": 1622
    },
    {
      "epoch": 0.05663389484519894,
      "grad_norm": 1.9152511358261108,
      "learning_rate": 9.591429995017439e-05,
      "loss": 0.0461,
      "step": 1623
    },
    {
      "epoch": 0.05666878941996493,
      "grad_norm": 2.616347074508667,
      "learning_rate": 9.566517189835577e-05,
      "loss": 0.0435,
      "step": 1624
    },
    {
      "epoch": 0.05670368399473092,
      "grad_norm": 2.5497050285339355,
      "learning_rate": 9.541604384653713e-05,
      "loss": 0.128,
      "step": 1625
    },
    {
      "epoch": 0.056738578569496906,
      "grad_norm": 3.2569544315338135,
      "learning_rate": 9.516691579471849e-05,
      "loss": 0.0347,
      "step": 1626
    },
    {
      "epoch": 0.0567734731442629,
      "grad_norm": 4.758884429931641,
      "learning_rate": 9.491778774289985e-05,
      "loss": 0.1065,
      "step": 1627
    },
    {
      "epoch": 0.056808367719028884,
      "grad_norm": 10.278413772583008,
      "learning_rate": 9.466865969108123e-05,
      "loss": 0.1992,
      "step": 1628
    },
    {
      "epoch": 0.05684326229379487,
      "grad_norm": 7.162569046020508,
      "learning_rate": 9.441953163926258e-05,
      "loss": 0.0832,
      "step": 1629
    },
    {
      "epoch": 0.05687815686856086,
      "grad_norm": 3.921447277069092,
      "learning_rate": 9.417040358744394e-05,
      "loss": 0.1574,
      "step": 1630
    },
    {
      "epoch": 0.05691305144332685,
      "grad_norm": 0.1498529613018036,
      "learning_rate": 9.39212755356253e-05,
      "loss": 0.0065,
      "step": 1631
    },
    {
      "epoch": 0.05694794601809284,
      "grad_norm": 3.5061898231506348,
      "learning_rate": 9.367214748380667e-05,
      "loss": 0.0987,
      "step": 1632
    },
    {
      "epoch": 0.056982840592858824,
      "grad_norm": 2.446322441101074,
      "learning_rate": 9.342301943198804e-05,
      "loss": 0.0704,
      "step": 1633
    },
    {
      "epoch": 0.05701773516762482,
      "grad_norm": 1.6505988836288452,
      "learning_rate": 9.317389138016941e-05,
      "loss": 0.0571,
      "step": 1634
    },
    {
      "epoch": 0.0570526297423908,
      "grad_norm": 2.1121985912323,
      "learning_rate": 9.292476332835077e-05,
      "loss": 0.0512,
      "step": 1635
    },
    {
      "epoch": 0.05708752431715679,
      "grad_norm": 4.00403356552124,
      "learning_rate": 9.267563527653213e-05,
      "loss": 0.1515,
      "step": 1636
    },
    {
      "epoch": 0.05712241889192278,
      "grad_norm": 9.3562593460083,
      "learning_rate": 9.242650722471351e-05,
      "loss": 0.1245,
      "step": 1637
    },
    {
      "epoch": 0.057157313466688765,
      "grad_norm": 5.06843376159668,
      "learning_rate": 9.217737917289487e-05,
      "loss": 0.1445,
      "step": 1638
    },
    {
      "epoch": 0.05719220804145476,
      "grad_norm": 16.150083541870117,
      "learning_rate": 9.192825112107624e-05,
      "loss": 0.2173,
      "step": 1639
    },
    {
      "epoch": 0.05722710261622074,
      "grad_norm": 2.0761878490448,
      "learning_rate": 9.16791230692576e-05,
      "loss": 0.0773,
      "step": 1640
    },
    {
      "epoch": 0.05726199719098673,
      "grad_norm": 1.8773975372314453,
      "learning_rate": 9.142999501743898e-05,
      "loss": 0.0718,
      "step": 1641
    },
    {
      "epoch": 0.05729689176575272,
      "grad_norm": 7.363664150238037,
      "learning_rate": 9.118086696562034e-05,
      "loss": 0.4434,
      "step": 1642
    },
    {
      "epoch": 0.057331786340518706,
      "grad_norm": 3.3472847938537598,
      "learning_rate": 9.09317389138017e-05,
      "loss": 0.0825,
      "step": 1643
    },
    {
      "epoch": 0.0573666809152847,
      "grad_norm": 1.8888614177703857,
      "learning_rate": 9.068261086198306e-05,
      "loss": 0.0724,
      "step": 1644
    },
    {
      "epoch": 0.057401575490050684,
      "grad_norm": 5.069665908813477,
      "learning_rate": 9.043348281016443e-05,
      "loss": 0.065,
      "step": 1645
    },
    {
      "epoch": 0.05743647006481667,
      "grad_norm": 4.571195125579834,
      "learning_rate": 9.018435475834579e-05,
      "loss": 0.2222,
      "step": 1646
    },
    {
      "epoch": 0.05747136463958266,
      "grad_norm": 2.307659387588501,
      "learning_rate": 8.993522670652715e-05,
      "loss": 0.0739,
      "step": 1647
    },
    {
      "epoch": 0.05750625921434865,
      "grad_norm": 0.40231746435165405,
      "learning_rate": 8.968609865470852e-05,
      "loss": 0.0107,
      "step": 1648
    },
    {
      "epoch": 0.05754115378911464,
      "grad_norm": 0.4610840082168579,
      "learning_rate": 8.943697060288988e-05,
      "loss": 0.0247,
      "step": 1649
    },
    {
      "epoch": 0.057576048363880625,
      "grad_norm": 4.351922988891602,
      "learning_rate": 8.918784255107125e-05,
      "loss": 0.2226,
      "step": 1650
    },
    {
      "epoch": 0.05761094293864662,
      "grad_norm": 3.7566418647766113,
      "learning_rate": 8.893871449925262e-05,
      "loss": 0.1296,
      "step": 1651
    },
    {
      "epoch": 0.0576458375134126,
      "grad_norm": 1.8905549049377441,
      "learning_rate": 8.868958644743398e-05,
      "loss": 0.0418,
      "step": 1652
    },
    {
      "epoch": 0.05768073208817859,
      "grad_norm": 8.703133583068848,
      "learning_rate": 8.844045839561534e-05,
      "loss": 0.1729,
      "step": 1653
    },
    {
      "epoch": 0.05771562666294458,
      "grad_norm": 3.440695285797119,
      "learning_rate": 8.819133034379672e-05,
      "loss": 0.0479,
      "step": 1654
    },
    {
      "epoch": 0.057750521237710566,
      "grad_norm": 5.025244235992432,
      "learning_rate": 8.794220229197808e-05,
      "loss": 0.2244,
      "step": 1655
    },
    {
      "epoch": 0.05778541581247656,
      "grad_norm": 2.8304429054260254,
      "learning_rate": 8.769307424015945e-05,
      "loss": 0.1709,
      "step": 1656
    },
    {
      "epoch": 0.05782031038724254,
      "grad_norm": 1.0260131359100342,
      "learning_rate": 8.744394618834081e-05,
      "loss": 0.1086,
      "step": 1657
    },
    {
      "epoch": 0.05785520496200853,
      "grad_norm": 1.6612356901168823,
      "learning_rate": 8.719481813652217e-05,
      "loss": 0.0645,
      "step": 1658
    },
    {
      "epoch": 0.05789009953677452,
      "grad_norm": 4.364709377288818,
      "learning_rate": 8.694569008470355e-05,
      "loss": 0.1762,
      "step": 1659
    },
    {
      "epoch": 0.057924994111540506,
      "grad_norm": 2.3566412925720215,
      "learning_rate": 8.669656203288491e-05,
      "loss": 0.1153,
      "step": 1660
    },
    {
      "epoch": 0.0579598886863065,
      "grad_norm": 1.5149110555648804,
      "learning_rate": 8.644743398106627e-05,
      "loss": 0.0714,
      "step": 1661
    },
    {
      "epoch": 0.057994783261072484,
      "grad_norm": 1.5905028581619263,
      "learning_rate": 8.619830592924764e-05,
      "loss": 0.0261,
      "step": 1662
    },
    {
      "epoch": 0.05802967783583847,
      "grad_norm": 1.31438148021698,
      "learning_rate": 8.5949177877429e-05,
      "loss": 0.0584,
      "step": 1663
    },
    {
      "epoch": 0.05806457241060446,
      "grad_norm": 5.620153903961182,
      "learning_rate": 8.570004982561036e-05,
      "loss": 0.1205,
      "step": 1664
    },
    {
      "epoch": 0.05809946698537045,
      "grad_norm": 2.023369073867798,
      "learning_rate": 8.545092177379173e-05,
      "loss": 0.0445,
      "step": 1665
    },
    {
      "epoch": 0.05813436156013644,
      "grad_norm": 1.8415465354919434,
      "learning_rate": 8.520179372197309e-05,
      "loss": 0.0536,
      "step": 1666
    },
    {
      "epoch": 0.058169256134902425,
      "grad_norm": 1.117727518081665,
      "learning_rate": 8.495266567015445e-05,
      "loss": 0.0302,
      "step": 1667
    },
    {
      "epoch": 0.05820415070966842,
      "grad_norm": 0.5990923047065735,
      "learning_rate": 8.470353761833583e-05,
      "loss": 0.0117,
      "step": 1668
    },
    {
      "epoch": 0.0582390452844344,
      "grad_norm": 5.901498794555664,
      "learning_rate": 8.445440956651719e-05,
      "loss": 0.3659,
      "step": 1669
    },
    {
      "epoch": 0.05827393985920039,
      "grad_norm": 5.263214111328125,
      "learning_rate": 8.420528151469855e-05,
      "loss": 0.2621,
      "step": 1670
    },
    {
      "epoch": 0.05830883443396638,
      "grad_norm": 1.1532557010650635,
      "learning_rate": 8.395615346287992e-05,
      "loss": 0.0555,
      "step": 1671
    },
    {
      "epoch": 0.058343729008732366,
      "grad_norm": 1.3648929595947266,
      "learning_rate": 8.370702541106129e-05,
      "loss": 0.0346,
      "step": 1672
    },
    {
      "epoch": 0.05837862358349836,
      "grad_norm": 6.089099407196045,
      "learning_rate": 8.345789735924266e-05,
      "loss": 0.2034,
      "step": 1673
    },
    {
      "epoch": 0.058413518158264344,
      "grad_norm": 3.1396780014038086,
      "learning_rate": 8.320876930742402e-05,
      "loss": 0.1272,
      "step": 1674
    },
    {
      "epoch": 0.05844841273303033,
      "grad_norm": 0.8244196772575378,
      "learning_rate": 8.295964125560538e-05,
      "loss": 0.0336,
      "step": 1675
    },
    {
      "epoch": 0.05848330730779632,
      "grad_norm": 1.3126336336135864,
      "learning_rate": 8.271051320378676e-05,
      "loss": 0.0273,
      "step": 1676
    },
    {
      "epoch": 0.05851820188256231,
      "grad_norm": 2.1942920684814453,
      "learning_rate": 8.246138515196812e-05,
      "loss": 0.0254,
      "step": 1677
    },
    {
      "epoch": 0.0585530964573283,
      "grad_norm": 2.903559446334839,
      "learning_rate": 8.221225710014948e-05,
      "loss": 0.1324,
      "step": 1678
    },
    {
      "epoch": 0.058587991032094285,
      "grad_norm": 4.61713981628418,
      "learning_rate": 8.196312904833085e-05,
      "loss": 0.0785,
      "step": 1679
    },
    {
      "epoch": 0.05862288560686027,
      "grad_norm": 2.3889167308807373,
      "learning_rate": 8.17140009965122e-05,
      "loss": 0.048,
      "step": 1680
    },
    {
      "epoch": 0.05865778018162626,
      "grad_norm": 0.28755056858062744,
      "learning_rate": 8.146487294469357e-05,
      "loss": 0.0072,
      "step": 1681
    },
    {
      "epoch": 0.05869267475639225,
      "grad_norm": 1.3892353773117065,
      "learning_rate": 8.121574489287494e-05,
      "loss": 0.0157,
      "step": 1682
    },
    {
      "epoch": 0.05872756933115824,
      "grad_norm": 5.153807163238525,
      "learning_rate": 8.09666168410563e-05,
      "loss": 0.2549,
      "step": 1683
    },
    {
      "epoch": 0.058762463905924225,
      "grad_norm": 4.027645111083984,
      "learning_rate": 8.071748878923766e-05,
      "loss": 0.2159,
      "step": 1684
    },
    {
      "epoch": 0.05879735848069022,
      "grad_norm": 5.785085678100586,
      "learning_rate": 8.046836073741904e-05,
      "loss": 0.3828,
      "step": 1685
    },
    {
      "epoch": 0.0588322530554562,
      "grad_norm": 8.561139106750488,
      "learning_rate": 8.02192326856004e-05,
      "loss": 0.3106,
      "step": 1686
    },
    {
      "epoch": 0.05886714763022219,
      "grad_norm": 4.495250225067139,
      "learning_rate": 7.997010463378176e-05,
      "loss": 0.2028,
      "step": 1687
    },
    {
      "epoch": 0.05890204220498818,
      "grad_norm": 4.992857933044434,
      "learning_rate": 7.972097658196313e-05,
      "loss": 0.1775,
      "step": 1688
    },
    {
      "epoch": 0.058936936779754166,
      "grad_norm": 2.0290157794952393,
      "learning_rate": 7.94718485301445e-05,
      "loss": 0.0731,
      "step": 1689
    },
    {
      "epoch": 0.05897183135452016,
      "grad_norm": 0.9449316263198853,
      "learning_rate": 7.922272047832587e-05,
      "loss": 0.0269,
      "step": 1690
    },
    {
      "epoch": 0.059006725929286144,
      "grad_norm": 1.3592907190322876,
      "learning_rate": 7.897359242650723e-05,
      "loss": 0.0604,
      "step": 1691
    },
    {
      "epoch": 0.05904162050405213,
      "grad_norm": 3.560464382171631,
      "learning_rate": 7.872446437468859e-05,
      "loss": 0.0406,
      "step": 1692
    },
    {
      "epoch": 0.05907651507881812,
      "grad_norm": 2.055581569671631,
      "learning_rate": 7.847533632286995e-05,
      "loss": 0.0582,
      "step": 1693
    },
    {
      "epoch": 0.05911140965358411,
      "grad_norm": 4.019700050354004,
      "learning_rate": 7.822620827105133e-05,
      "loss": 0.1228,
      "step": 1694
    },
    {
      "epoch": 0.0591463042283501,
      "grad_norm": 5.477177143096924,
      "learning_rate": 7.79770802192327e-05,
      "loss": 0.2704,
      "step": 1695
    },
    {
      "epoch": 0.059181198803116085,
      "grad_norm": 0.4170977473258972,
      "learning_rate": 7.772795216741404e-05,
      "loss": 0.0103,
      "step": 1696
    },
    {
      "epoch": 0.05921609337788208,
      "grad_norm": 1.5870448350906372,
      "learning_rate": 7.74788241155954e-05,
      "loss": 0.0205,
      "step": 1697
    },
    {
      "epoch": 0.05925098795264806,
      "grad_norm": 7.126399993896484,
      "learning_rate": 7.722969606377678e-05,
      "loss": 0.0928,
      "step": 1698
    },
    {
      "epoch": 0.05928588252741405,
      "grad_norm": 2.0024170875549316,
      "learning_rate": 7.698056801195815e-05,
      "loss": 0.0656,
      "step": 1699
    },
    {
      "epoch": 0.05932077710218004,
      "grad_norm": 2.5552725791931152,
      "learning_rate": 7.673143996013951e-05,
      "loss": 0.1102,
      "step": 1700
    },
    {
      "epoch": 0.059355671676946026,
      "grad_norm": 2.573725461959839,
      "learning_rate": 7.648231190832087e-05,
      "loss": 0.1351,
      "step": 1701
    },
    {
      "epoch": 0.05939056625171202,
      "grad_norm": 3.4520087242126465,
      "learning_rate": 7.623318385650225e-05,
      "loss": 0.2169,
      "step": 1702
    },
    {
      "epoch": 0.059425460826478003,
      "grad_norm": 4.583260536193848,
      "learning_rate": 7.598405580468361e-05,
      "loss": 0.1196,
      "step": 1703
    },
    {
      "epoch": 0.05946035540124399,
      "grad_norm": 0.4527836740016937,
      "learning_rate": 7.573492775286497e-05,
      "loss": 0.0139,
      "step": 1704
    },
    {
      "epoch": 0.05949524997600998,
      "grad_norm": 2.2010583877563477,
      "learning_rate": 7.548579970104634e-05,
      "loss": 0.1174,
      "step": 1705
    },
    {
      "epoch": 0.05953014455077597,
      "grad_norm": 3.6776390075683594,
      "learning_rate": 7.52366716492277e-05,
      "loss": 0.0894,
      "step": 1706
    },
    {
      "epoch": 0.05956503912554196,
      "grad_norm": 4.522076606750488,
      "learning_rate": 7.498754359740908e-05,
      "loss": 0.1067,
      "step": 1707
    },
    {
      "epoch": 0.059599933700307944,
      "grad_norm": 2.906522035598755,
      "learning_rate": 7.473841554559044e-05,
      "loss": 0.1955,
      "step": 1708
    },
    {
      "epoch": 0.05963482827507393,
      "grad_norm": 1.8770054578781128,
      "learning_rate": 7.44892874937718e-05,
      "loss": 0.1118,
      "step": 1709
    },
    {
      "epoch": 0.05966972284983992,
      "grad_norm": 1.873257040977478,
      "learning_rate": 7.424015944195316e-05,
      "loss": 0.0592,
      "step": 1710
    },
    {
      "epoch": 0.05970461742460591,
      "grad_norm": 2.026151657104492,
      "learning_rate": 7.399103139013454e-05,
      "loss": 0.0917,
      "step": 1711
    },
    {
      "epoch": 0.0597395119993719,
      "grad_norm": 0.6341426372528076,
      "learning_rate": 7.37419033383159e-05,
      "loss": 0.0174,
      "step": 1712
    },
    {
      "epoch": 0.059774406574137885,
      "grad_norm": 3.475847005844116,
      "learning_rate": 7.349277528649725e-05,
      "loss": 0.0874,
      "step": 1713
    },
    {
      "epoch": 0.05980930114890388,
      "grad_norm": 1.2610973119735718,
      "learning_rate": 7.324364723467862e-05,
      "loss": 0.0472,
      "step": 1714
    },
    {
      "epoch": 0.05984419572366986,
      "grad_norm": 0.5587411522865295,
      "learning_rate": 7.299451918285999e-05,
      "loss": 0.0239,
      "step": 1715
    },
    {
      "epoch": 0.05987909029843585,
      "grad_norm": 2.262845277786255,
      "learning_rate": 7.274539113104136e-05,
      "loss": 0.1299,
      "step": 1716
    },
    {
      "epoch": 0.05991398487320184,
      "grad_norm": 0.12009850889444351,
      "learning_rate": 7.249626307922272e-05,
      "loss": 0.002,
      "step": 1717
    },
    {
      "epoch": 0.059948879447967826,
      "grad_norm": 2.118149995803833,
      "learning_rate": 7.224713502740408e-05,
      "loss": 0.114,
      "step": 1718
    },
    {
      "epoch": 0.05998377402273382,
      "grad_norm": 4.844379901885986,
      "learning_rate": 7.199800697558544e-05,
      "loss": 0.0434,
      "step": 1719
    },
    {
      "epoch": 0.060018668597499804,
      "grad_norm": 1.7526878118515015,
      "learning_rate": 7.174887892376682e-05,
      "loss": 0.0623,
      "step": 1720
    },
    {
      "epoch": 0.06005356317226579,
      "grad_norm": 2.187216281890869,
      "learning_rate": 7.149975087194818e-05,
      "loss": 0.053,
      "step": 1721
    },
    {
      "epoch": 0.06008845774703178,
      "grad_norm": 6.6828293800354,
      "learning_rate": 7.125062282012955e-05,
      "loss": 0.1125,
      "step": 1722
    },
    {
      "epoch": 0.06012335232179777,
      "grad_norm": 0.9528689980506897,
      "learning_rate": 7.100149476831091e-05,
      "loss": 0.0383,
      "step": 1723
    },
    {
      "epoch": 0.06015824689656376,
      "grad_norm": 0.41396889090538025,
      "learning_rate": 7.075236671649229e-05,
      "loss": 0.0127,
      "step": 1724
    },
    {
      "epoch": 0.060193141471329745,
      "grad_norm": 5.531512260437012,
      "learning_rate": 7.050323866467365e-05,
      "loss": 0.0776,
      "step": 1725
    },
    {
      "epoch": 0.06022803604609573,
      "grad_norm": 3.0290560722351074,
      "learning_rate": 7.025411061285501e-05,
      "loss": 0.0378,
      "step": 1726
    },
    {
      "epoch": 0.06026293062086172,
      "grad_norm": 8.392494201660156,
      "learning_rate": 7.000498256103637e-05,
      "loss": 0.1486,
      "step": 1727
    },
    {
      "epoch": 0.06029782519562771,
      "grad_norm": 2.5378172397613525,
      "learning_rate": 6.975585450921775e-05,
      "loss": 0.1615,
      "step": 1728
    },
    {
      "epoch": 0.0603327197703937,
      "grad_norm": 8.480663299560547,
      "learning_rate": 6.950672645739911e-05,
      "loss": 0.314,
      "step": 1729
    },
    {
      "epoch": 0.060367614345159686,
      "grad_norm": 7.780488967895508,
      "learning_rate": 6.925759840558046e-05,
      "loss": 0.1454,
      "step": 1730
    },
    {
      "epoch": 0.06040250891992568,
      "grad_norm": 5.407418727874756,
      "learning_rate": 6.900847035376183e-05,
      "loss": 0.0568,
      "step": 1731
    },
    {
      "epoch": 0.06043740349469166,
      "grad_norm": 1.9909398555755615,
      "learning_rate": 6.875934230194319e-05,
      "loss": 0.0945,
      "step": 1732
    },
    {
      "epoch": 0.06047229806945765,
      "grad_norm": 4.270140647888184,
      "learning_rate": 6.851021425012457e-05,
      "loss": 0.0982,
      "step": 1733
    },
    {
      "epoch": 0.06050719264422364,
      "grad_norm": 3.573500871658325,
      "learning_rate": 6.826108619830593e-05,
      "loss": 0.0636,
      "step": 1734
    },
    {
      "epoch": 0.060542087218989626,
      "grad_norm": 1.3505679368972778,
      "learning_rate": 6.801195814648729e-05,
      "loss": 0.0351,
      "step": 1735
    },
    {
      "epoch": 0.06057698179375562,
      "grad_norm": 2.2097725868225098,
      "learning_rate": 6.776283009466865e-05,
      "loss": 0.1656,
      "step": 1736
    },
    {
      "epoch": 0.060611876368521604,
      "grad_norm": 2.4972922801971436,
      "learning_rate": 6.751370204285003e-05,
      "loss": 0.0458,
      "step": 1737
    },
    {
      "epoch": 0.06064677094328759,
      "grad_norm": 2.0066592693328857,
      "learning_rate": 6.72645739910314e-05,
      "loss": 0.0568,
      "step": 1738
    },
    {
      "epoch": 0.06068166551805358,
      "grad_norm": 2.5736167430877686,
      "learning_rate": 6.701544593921276e-05,
      "loss": 0.0846,
      "step": 1739
    },
    {
      "epoch": 0.06071656009281957,
      "grad_norm": 3.2047407627105713,
      "learning_rate": 6.676631788739412e-05,
      "loss": 0.1312,
      "step": 1740
    },
    {
      "epoch": 0.06075145466758556,
      "grad_norm": 4.116878986358643,
      "learning_rate": 6.65171898355755e-05,
      "loss": 0.0917,
      "step": 1741
    },
    {
      "epoch": 0.060786349242351545,
      "grad_norm": 3.7794220447540283,
      "learning_rate": 6.626806178375686e-05,
      "loss": 0.1798,
      "step": 1742
    },
    {
      "epoch": 0.06082124381711753,
      "grad_norm": 3.9618351459503174,
      "learning_rate": 6.601893373193822e-05,
      "loss": 0.1449,
      "step": 1743
    },
    {
      "epoch": 0.06085613839188352,
      "grad_norm": 1.2782520055770874,
      "learning_rate": 6.576980568011958e-05,
      "loss": 0.0465,
      "step": 1744
    },
    {
      "epoch": 0.06089103296664951,
      "grad_norm": 4.4420366287231445,
      "learning_rate": 6.552067762830095e-05,
      "loss": 0.1106,
      "step": 1745
    },
    {
      "epoch": 0.0609259275414155,
      "grad_norm": 0.942403256893158,
      "learning_rate": 6.527154957648232e-05,
      "loss": 0.0197,
      "step": 1746
    },
    {
      "epoch": 0.060960822116181486,
      "grad_norm": 3.897324562072754,
      "learning_rate": 6.502242152466367e-05,
      "loss": 0.1813,
      "step": 1747
    },
    {
      "epoch": 0.06099571669094748,
      "grad_norm": 3.2038772106170654,
      "learning_rate": 6.477329347284504e-05,
      "loss": 0.2205,
      "step": 1748
    },
    {
      "epoch": 0.061030611265713464,
      "grad_norm": 1.0744848251342773,
      "learning_rate": 6.45241654210264e-05,
      "loss": 0.0267,
      "step": 1749
    },
    {
      "epoch": 0.06106550584047945,
      "grad_norm": 1.5895277261734009,
      "learning_rate": 6.427503736920778e-05,
      "loss": 0.0187,
      "step": 1750
    },
    {
      "epoch": 0.06110040041524544,
      "grad_norm": 3.7681212425231934,
      "learning_rate": 6.402590931738914e-05,
      "loss": 0.1562,
      "step": 1751
    },
    {
      "epoch": 0.06113529499001143,
      "grad_norm": 3.558882713317871,
      "learning_rate": 6.37767812655705e-05,
      "loss": 0.0787,
      "step": 1752
    },
    {
      "epoch": 0.06117018956477742,
      "grad_norm": 1.2914518117904663,
      "learning_rate": 6.352765321375186e-05,
      "loss": 0.0256,
      "step": 1753
    },
    {
      "epoch": 0.061205084139543404,
      "grad_norm": 12.041674613952637,
      "learning_rate": 6.327852516193324e-05,
      "loss": 0.1617,
      "step": 1754
    },
    {
      "epoch": 0.06123997871430939,
      "grad_norm": 4.522006034851074,
      "learning_rate": 6.30293971101146e-05,
      "loss": 0.0783,
      "step": 1755
    },
    {
      "epoch": 0.06127487328907538,
      "grad_norm": 1.3519349098205566,
      "learning_rate": 6.278026905829597e-05,
      "loss": 0.068,
      "step": 1756
    },
    {
      "epoch": 0.06130976786384137,
      "grad_norm": 9.057316780090332,
      "learning_rate": 6.253114100647733e-05,
      "loss": 0.1114,
      "step": 1757
    },
    {
      "epoch": 0.06134466243860736,
      "grad_norm": 0.9253690242767334,
      "learning_rate": 6.228201295465869e-05,
      "loss": 0.0289,
      "step": 1758
    },
    {
      "epoch": 0.061379557013373345,
      "grad_norm": 4.2345662117004395,
      "learning_rate": 6.203288490284007e-05,
      "loss": 0.1955,
      "step": 1759
    },
    {
      "epoch": 0.06141445158813933,
      "grad_norm": 3.271672248840332,
      "learning_rate": 6.178375685102142e-05,
      "loss": 0.1362,
      "step": 1760
    },
    {
      "epoch": 0.06144934616290532,
      "grad_norm": 2.702326536178589,
      "learning_rate": 6.15346287992028e-05,
      "loss": 0.1088,
      "step": 1761
    },
    {
      "epoch": 0.06148424073767131,
      "grad_norm": 1.350210428237915,
      "learning_rate": 6.128550074738416e-05,
      "loss": 0.0437,
      "step": 1762
    },
    {
      "epoch": 0.0615191353124373,
      "grad_norm": 2.0635907649993896,
      "learning_rate": 6.103637269556552e-05,
      "loss": 0.1053,
      "step": 1763
    },
    {
      "epoch": 0.061554029887203286,
      "grad_norm": 5.837034702301025,
      "learning_rate": 6.078724464374689e-05,
      "loss": 0.1165,
      "step": 1764
    },
    {
      "epoch": 0.06158892446196928,
      "grad_norm": 7.223244667053223,
      "learning_rate": 6.053811659192825e-05,
      "loss": 0.0977,
      "step": 1765
    },
    {
      "epoch": 0.061623819036735264,
      "grad_norm": 2.5236074924468994,
      "learning_rate": 6.028898854010962e-05,
      "loss": 0.0664,
      "step": 1766
    },
    {
      "epoch": 0.06165871361150125,
      "grad_norm": 6.620641231536865,
      "learning_rate": 6.0039860488290985e-05,
      "loss": 0.2061,
      "step": 1767
    },
    {
      "epoch": 0.06169360818626724,
      "grad_norm": 0.9608339071273804,
      "learning_rate": 5.979073243647234e-05,
      "loss": 0.0387,
      "step": 1768
    },
    {
      "epoch": 0.06172850276103323,
      "grad_norm": 2.8547184467315674,
      "learning_rate": 5.954160438465371e-05,
      "loss": 0.0643,
      "step": 1769
    },
    {
      "epoch": 0.06176339733579922,
      "grad_norm": 3.3912363052368164,
      "learning_rate": 5.9292476332835074e-05,
      "loss": 0.0685,
      "step": 1770
    },
    {
      "epoch": 0.061798291910565205,
      "grad_norm": 6.335825443267822,
      "learning_rate": 5.9043348281016444e-05,
      "loss": 0.1997,
      "step": 1771
    },
    {
      "epoch": 0.06183318648533119,
      "grad_norm": 10.971900939941406,
      "learning_rate": 5.879422022919781e-05,
      "loss": 0.1487,
      "step": 1772
    },
    {
      "epoch": 0.06186808106009718,
      "grad_norm": 1.5361384153366089,
      "learning_rate": 5.8545092177379176e-05,
      "loss": 0.0501,
      "step": 1773
    },
    {
      "epoch": 0.06190297563486317,
      "grad_norm": 1.3752580881118774,
      "learning_rate": 5.829596412556054e-05,
      "loss": 0.1054,
      "step": 1774
    },
    {
      "epoch": 0.06193787020962916,
      "grad_norm": 1.7733078002929688,
      "learning_rate": 5.804683607374191e-05,
      "loss": 0.0602,
      "step": 1775
    },
    {
      "epoch": 0.061972764784395146,
      "grad_norm": 1.8269394636154175,
      "learning_rate": 5.779770802192327e-05,
      "loss": 0.1169,
      "step": 1776
    },
    {
      "epoch": 0.06200765935916113,
      "grad_norm": 9.046987533569336,
      "learning_rate": 5.7548579970104635e-05,
      "loss": 0.2266,
      "step": 1777
    },
    {
      "epoch": 0.06204255393392712,
      "grad_norm": 3.037381172180176,
      "learning_rate": 5.7299451918286e-05,
      "loss": 0.0463,
      "step": 1778
    },
    {
      "epoch": 0.06207744850869311,
      "grad_norm": 10.230158805847168,
      "learning_rate": 5.705032386646737e-05,
      "loss": 0.3307,
      "step": 1779
    },
    {
      "epoch": 0.0621123430834591,
      "grad_norm": 9.662030220031738,
      "learning_rate": 5.680119581464873e-05,
      "loss": 0.332,
      "step": 1780
    },
    {
      "epoch": 0.06214723765822509,
      "grad_norm": 5.018238067626953,
      "learning_rate": 5.655206776283009e-05,
      "loss": 0.1143,
      "step": 1781
    },
    {
      "epoch": 0.06218213223299108,
      "grad_norm": 5.665428161621094,
      "learning_rate": 5.630293971101146e-05,
      "loss": 0.1096,
      "step": 1782
    },
    {
      "epoch": 0.062217026807757064,
      "grad_norm": 1.3843027353286743,
      "learning_rate": 5.6053811659192826e-05,
      "loss": 0.0522,
      "step": 1783
    },
    {
      "epoch": 0.06225192138252305,
      "grad_norm": 0.42543694376945496,
      "learning_rate": 5.5804683607374195e-05,
      "loss": 0.0077,
      "step": 1784
    },
    {
      "epoch": 0.06228681595728904,
      "grad_norm": 4.9887213706970215,
      "learning_rate": 5.555555555555555e-05,
      "loss": 0.2212,
      "step": 1785
    },
    {
      "epoch": 0.06232171053205503,
      "grad_norm": 2.2829437255859375,
      "learning_rate": 5.530642750373692e-05,
      "loss": 0.0841,
      "step": 1786
    },
    {
      "epoch": 0.06235660510682102,
      "grad_norm": 2.017422914505005,
      "learning_rate": 5.5057299451918284e-05,
      "loss": 0.0462,
      "step": 1787
    },
    {
      "epoch": 0.062391499681587005,
      "grad_norm": 0.18811063468456268,
      "learning_rate": 5.4808171400099654e-05,
      "loss": 0.004,
      "step": 1788
    },
    {
      "epoch": 0.06242639425635299,
      "grad_norm": 3.79966139793396,
      "learning_rate": 5.4559043348281017e-05,
      "loss": 0.1663,
      "step": 1789
    },
    {
      "epoch": 0.06246128883111898,
      "grad_norm": 3.836728811264038,
      "learning_rate": 5.4309915296462386e-05,
      "loss": 0.0401,
      "step": 1790
    },
    {
      "epoch": 0.06249618340588497,
      "grad_norm": 1.7958176136016846,
      "learning_rate": 5.406078724464375e-05,
      "loss": 0.1086,
      "step": 1791
    },
    {
      "epoch": 0.06253107798065095,
      "grad_norm": 0.8995267748832703,
      "learning_rate": 5.381165919282512e-05,
      "loss": 0.0343,
      "step": 1792
    },
    {
      "epoch": 0.06256597255541695,
      "grad_norm": 0.706530749797821,
      "learning_rate": 5.3562531141006475e-05,
      "loss": 0.0192,
      "step": 1793
    },
    {
      "epoch": 0.06260086713018294,
      "grad_norm": 0.3254523277282715,
      "learning_rate": 5.331340308918784e-05,
      "loss": 0.011,
      "step": 1794
    },
    {
      "epoch": 0.06263576170494892,
      "grad_norm": 1.4484635591506958,
      "learning_rate": 5.306427503736921e-05,
      "loss": 0.0824,
      "step": 1795
    },
    {
      "epoch": 0.06267065627971491,
      "grad_norm": 2.9410343170166016,
      "learning_rate": 5.281514698555057e-05,
      "loss": 0.046,
      "step": 1796
    },
    {
      "epoch": 0.0627055508544809,
      "grad_norm": 0.29523545503616333,
      "learning_rate": 5.256601893373194e-05,
      "loss": 0.0104,
      "step": 1797
    },
    {
      "epoch": 0.0627404454292469,
      "grad_norm": 1.2631992101669312,
      "learning_rate": 5.23168908819133e-05,
      "loss": 0.0348,
      "step": 1798
    },
    {
      "epoch": 0.06277534000401287,
      "grad_norm": 1.2540453672409058,
      "learning_rate": 5.206776283009467e-05,
      "loss": 0.0393,
      "step": 1799
    },
    {
      "epoch": 0.06281023457877886,
      "grad_norm": 0.8473117351531982,
      "learning_rate": 5.1818634778276036e-05,
      "loss": 0.0123,
      "step": 1800
    },
    {
      "epoch": 0.06284512915354486,
      "grad_norm": 3.12534236907959,
      "learning_rate": 5.1569506726457405e-05,
      "loss": 0.2492,
      "step": 1801
    },
    {
      "epoch": 0.06288002372831084,
      "grad_norm": 3.0156350135803223,
      "learning_rate": 5.132037867463876e-05,
      "loss": 0.1072,
      "step": 1802
    },
    {
      "epoch": 0.06291491830307683,
      "grad_norm": 0.4592217206954956,
      "learning_rate": 5.107125062282013e-05,
      "loss": 0.0134,
      "step": 1803
    },
    {
      "epoch": 0.06294981287784282,
      "grad_norm": 2.084652900695801,
      "learning_rate": 5.0822122571001494e-05,
      "loss": 0.0878,
      "step": 1804
    },
    {
      "epoch": 0.06298470745260881,
      "grad_norm": 6.14607048034668,
      "learning_rate": 5.0572994519182864e-05,
      "loss": 0.2672,
      "step": 1805
    },
    {
      "epoch": 0.06301960202737479,
      "grad_norm": 2.40303897857666,
      "learning_rate": 5.0323866467364227e-05,
      "loss": 0.1234,
      "step": 1806
    },
    {
      "epoch": 0.06305449660214078,
      "grad_norm": 0.7096556425094604,
      "learning_rate": 5.007473841554559e-05,
      "loss": 0.0159,
      "step": 1807
    },
    {
      "epoch": 0.06308939117690678,
      "grad_norm": 6.828240394592285,
      "learning_rate": 4.982561036372696e-05,
      "loss": 0.2226,
      "step": 1808
    },
    {
      "epoch": 0.06312428575167275,
      "grad_norm": 1.5110623836517334,
      "learning_rate": 4.957648231190832e-05,
      "loss": 0.0512,
      "step": 1809
    },
    {
      "epoch": 0.06315918032643875,
      "grad_norm": 2.7668094635009766,
      "learning_rate": 4.9327354260089685e-05,
      "loss": 0.0691,
      "step": 1810
    },
    {
      "epoch": 0.06319407490120474,
      "grad_norm": 0.6646650433540344,
      "learning_rate": 4.907822620827105e-05,
      "loss": 0.0141,
      "step": 1811
    },
    {
      "epoch": 0.06322896947597072,
      "grad_norm": 1.5036174058914185,
      "learning_rate": 4.882909815645242e-05,
      "loss": 0.0299,
      "step": 1812
    },
    {
      "epoch": 0.06326386405073671,
      "grad_norm": 7.581295013427734,
      "learning_rate": 4.857997010463378e-05,
      "loss": 0.1838,
      "step": 1813
    },
    {
      "epoch": 0.0632987586255027,
      "grad_norm": 8.003944396972656,
      "learning_rate": 4.833084205281515e-05,
      "loss": 0.156,
      "step": 1814
    },
    {
      "epoch": 0.0633336532002687,
      "grad_norm": 4.222650527954102,
      "learning_rate": 4.808171400099651e-05,
      "loss": 0.0391,
      "step": 1815
    },
    {
      "epoch": 0.06336854777503467,
      "grad_norm": 2.994947671890259,
      "learning_rate": 4.783258594917788e-05,
      "loss": 0.0651,
      "step": 1816
    },
    {
      "epoch": 0.06340344234980066,
      "grad_norm": 2.6967217922210693,
      "learning_rate": 4.7583457897359246e-05,
      "loss": 0.1184,
      "step": 1817
    },
    {
      "epoch": 0.06343833692456666,
      "grad_norm": 1.85793936252594,
      "learning_rate": 4.7334329845540615e-05,
      "loss": 0.0397,
      "step": 1818
    },
    {
      "epoch": 0.06347323149933264,
      "grad_norm": 3.353637218475342,
      "learning_rate": 4.708520179372197e-05,
      "loss": 0.1829,
      "step": 1819
    },
    {
      "epoch": 0.06350812607409863,
      "grad_norm": 1.3343144655227661,
      "learning_rate": 4.6836073741903334e-05,
      "loss": 0.042,
      "step": 1820
    },
    {
      "epoch": 0.06354302064886462,
      "grad_norm": 3.343078851699829,
      "learning_rate": 4.6586945690084704e-05,
      "loss": 0.187,
      "step": 1821
    },
    {
      "epoch": 0.06357791522363061,
      "grad_norm": 0.43919339776039124,
      "learning_rate": 4.633781763826607e-05,
      "loss": 0.0093,
      "step": 1822
    },
    {
      "epoch": 0.06361280979839659,
      "grad_norm": 7.267047882080078,
      "learning_rate": 4.6088689586447437e-05,
      "loss": 0.085,
      "step": 1823
    },
    {
      "epoch": 0.06364770437316258,
      "grad_norm": 3.107922077178955,
      "learning_rate": 4.58395615346288e-05,
      "loss": 0.1891,
      "step": 1824
    },
    {
      "epoch": 0.06368259894792858,
      "grad_norm": 2.038991689682007,
      "learning_rate": 4.559043348281017e-05,
      "loss": 0.1322,
      "step": 1825
    },
    {
      "epoch": 0.06371749352269455,
      "grad_norm": 4.677075386047363,
      "learning_rate": 4.534130543099153e-05,
      "loss": 0.138,
      "step": 1826
    },
    {
      "epoch": 0.06375238809746055,
      "grad_norm": 4.674906253814697,
      "learning_rate": 4.5092177379172895e-05,
      "loss": 0.071,
      "step": 1827
    },
    {
      "epoch": 0.06378728267222654,
      "grad_norm": 4.535922050476074,
      "learning_rate": 4.484304932735426e-05,
      "loss": 0.0745,
      "step": 1828
    },
    {
      "epoch": 0.06382217724699252,
      "grad_norm": 0.822046160697937,
      "learning_rate": 4.459392127553563e-05,
      "loss": 0.017,
      "step": 1829
    },
    {
      "epoch": 0.06385707182175851,
      "grad_norm": 0.7878811955451965,
      "learning_rate": 4.434479322371699e-05,
      "loss": 0.0204,
      "step": 1830
    },
    {
      "epoch": 0.0638919663965245,
      "grad_norm": 0.5390154123306274,
      "learning_rate": 4.409566517189836e-05,
      "loss": 0.0108,
      "step": 1831
    },
    {
      "epoch": 0.0639268609712905,
      "grad_norm": 3.2454235553741455,
      "learning_rate": 4.384653712007972e-05,
      "loss": 0.1969,
      "step": 1832
    },
    {
      "epoch": 0.06396175554605647,
      "grad_norm": 2.6414384841918945,
      "learning_rate": 4.3597409068261086e-05,
      "loss": 0.0762,
      "step": 1833
    },
    {
      "epoch": 0.06399665012082247,
      "grad_norm": 3.9969584941864014,
      "learning_rate": 4.3348281016442456e-05,
      "loss": 0.0574,
      "step": 1834
    },
    {
      "epoch": 0.06403154469558846,
      "grad_norm": 1.5503183603286743,
      "learning_rate": 4.309915296462382e-05,
      "loss": 0.0365,
      "step": 1835
    },
    {
      "epoch": 0.06406643927035444,
      "grad_norm": 4.345291614532471,
      "learning_rate": 4.285002491280518e-05,
      "loss": 0.0853,
      "step": 1836
    },
    {
      "epoch": 0.06410133384512043,
      "grad_norm": 3.4257421493530273,
      "learning_rate": 4.2600896860986544e-05,
      "loss": 0.0305,
      "step": 1837
    },
    {
      "epoch": 0.06413622841988642,
      "grad_norm": 0.9750918745994568,
      "learning_rate": 4.2351768809167914e-05,
      "loss": 0.0203,
      "step": 1838
    },
    {
      "epoch": 0.06417112299465241,
      "grad_norm": 4.342115879058838,
      "learning_rate": 4.210264075734928e-05,
      "loss": 0.0528,
      "step": 1839
    },
    {
      "epoch": 0.06420601756941839,
      "grad_norm": 2.6899425983428955,
      "learning_rate": 4.1853512705530646e-05,
      "loss": 0.0312,
      "step": 1840
    },
    {
      "epoch": 0.06424091214418438,
      "grad_norm": 3.75663423538208,
      "learning_rate": 4.160438465371201e-05,
      "loss": 0.1346,
      "step": 1841
    },
    {
      "epoch": 0.06427580671895038,
      "grad_norm": 3.3686108589172363,
      "learning_rate": 4.135525660189338e-05,
      "loss": 0.2059,
      "step": 1842
    },
    {
      "epoch": 0.06431070129371635,
      "grad_norm": 16.19610595703125,
      "learning_rate": 4.110612855007474e-05,
      "loss": 0.1823,
      "step": 1843
    },
    {
      "epoch": 0.06434559586848235,
      "grad_norm": 2.5190000534057617,
      "learning_rate": 4.08570004982561e-05,
      "loss": 0.0797,
      "step": 1844
    },
    {
      "epoch": 0.06438049044324834,
      "grad_norm": 0.4083196222782135,
      "learning_rate": 4.060787244643747e-05,
      "loss": 0.0076,
      "step": 1845
    },
    {
      "epoch": 0.06441538501801432,
      "grad_norm": 4.952287673950195,
      "learning_rate": 4.035874439461883e-05,
      "loss": 0.1052,
      "step": 1846
    },
    {
      "epoch": 0.06445027959278031,
      "grad_norm": 6.375383377075195,
      "learning_rate": 4.01096163428002e-05,
      "loss": 0.367,
      "step": 1847
    },
    {
      "epoch": 0.0644851741675463,
      "grad_norm": 3.4663069248199463,
      "learning_rate": 3.986048829098156e-05,
      "loss": 0.1486,
      "step": 1848
    },
    {
      "epoch": 0.0645200687423123,
      "grad_norm": 1.0023735761642456,
      "learning_rate": 3.961136023916293e-05,
      "loss": 0.0382,
      "step": 1849
    },
    {
      "epoch": 0.06455496331707827,
      "grad_norm": 5.645595550537109,
      "learning_rate": 3.9362232187344296e-05,
      "loss": 0.1658,
      "step": 1850
    },
    {
      "epoch": 0.06458985789184427,
      "grad_norm": 3.80960750579834,
      "learning_rate": 3.9113104135525665e-05,
      "loss": 0.1506,
      "step": 1851
    },
    {
      "epoch": 0.06462475246661026,
      "grad_norm": 0.7763314247131348,
      "learning_rate": 3.886397608370702e-05,
      "loss": 0.0291,
      "step": 1852
    },
    {
      "epoch": 0.06465964704137624,
      "grad_norm": 5.6943793296813965,
      "learning_rate": 3.861484803188839e-05,
      "loss": 0.2112,
      "step": 1853
    },
    {
      "epoch": 0.06469454161614223,
      "grad_norm": 1.0341854095458984,
      "learning_rate": 3.8365719980069754e-05,
      "loss": 0.0389,
      "step": 1854
    },
    {
      "epoch": 0.06472943619090822,
      "grad_norm": 5.822505474090576,
      "learning_rate": 3.8116591928251124e-05,
      "loss": 0.1307,
      "step": 1855
    },
    {
      "epoch": 0.06476433076567421,
      "grad_norm": 4.618297576904297,
      "learning_rate": 3.786746387643249e-05,
      "loss": 0.0375,
      "step": 1856
    },
    {
      "epoch": 0.06479922534044019,
      "grad_norm": 6.320760250091553,
      "learning_rate": 3.761833582461385e-05,
      "loss": 0.1936,
      "step": 1857
    },
    {
      "epoch": 0.06483411991520618,
      "grad_norm": 0.08732209354639053,
      "learning_rate": 3.736920777279522e-05,
      "loss": 0.0032,
      "step": 1858
    },
    {
      "epoch": 0.06486901448997218,
      "grad_norm": 2.8545162677764893,
      "learning_rate": 3.712007972097658e-05,
      "loss": 0.0705,
      "step": 1859
    },
    {
      "epoch": 0.06490390906473816,
      "grad_norm": 0.9893774390220642,
      "learning_rate": 3.687095166915795e-05,
      "loss": 0.0188,
      "step": 1860
    },
    {
      "epoch": 0.06493880363950415,
      "grad_norm": 3.004091739654541,
      "learning_rate": 3.662182361733931e-05,
      "loss": 0.2006,
      "step": 1861
    },
    {
      "epoch": 0.06497369821427014,
      "grad_norm": 3.5832555294036865,
      "learning_rate": 3.637269556552068e-05,
      "loss": 0.0418,
      "step": 1862
    },
    {
      "epoch": 0.06500859278903612,
      "grad_norm": 0.9952279329299927,
      "learning_rate": 3.612356751370204e-05,
      "loss": 0.0262,
      "step": 1863
    },
    {
      "epoch": 0.06504348736380211,
      "grad_norm": 1.9250558614730835,
      "learning_rate": 3.587443946188341e-05,
      "loss": 0.0468,
      "step": 1864
    },
    {
      "epoch": 0.0650783819385681,
      "grad_norm": 1.8375520706176758,
      "learning_rate": 3.562531141006477e-05,
      "loss": 0.0767,
      "step": 1865
    },
    {
      "epoch": 0.0651132765133341,
      "grad_norm": 1.7670334577560425,
      "learning_rate": 3.537618335824614e-05,
      "loss": 0.0605,
      "step": 1866
    },
    {
      "epoch": 0.06514817108810007,
      "grad_norm": 0.5639879107475281,
      "learning_rate": 3.5127055306427506e-05,
      "loss": 0.0187,
      "step": 1867
    },
    {
      "epoch": 0.06518306566286607,
      "grad_norm": 4.006464004516602,
      "learning_rate": 3.4877927254608875e-05,
      "loss": 0.168,
      "step": 1868
    },
    {
      "epoch": 0.06521796023763206,
      "grad_norm": 7.572173595428467,
      "learning_rate": 3.462879920279023e-05,
      "loss": 0.2225,
      "step": 1869
    },
    {
      "epoch": 0.06525285481239804,
      "grad_norm": 3.3882057666778564,
      "learning_rate": 3.4379671150971594e-05,
      "loss": 0.1288,
      "step": 1870
    },
    {
      "epoch": 0.06528774938716403,
      "grad_norm": 1.1016720533370972,
      "learning_rate": 3.4130543099152964e-05,
      "loss": 0.0131,
      "step": 1871
    },
    {
      "epoch": 0.06532264396193002,
      "grad_norm": 2.7554166316986084,
      "learning_rate": 3.388141504733433e-05,
      "loss": 0.1145,
      "step": 1872
    },
    {
      "epoch": 0.06535753853669601,
      "grad_norm": 1.3488649129867554,
      "learning_rate": 3.36322869955157e-05,
      "loss": 0.0641,
      "step": 1873
    },
    {
      "epoch": 0.06539243311146199,
      "grad_norm": 3.079066276550293,
      "learning_rate": 3.338315894369706e-05,
      "loss": 0.1287,
      "step": 1874
    },
    {
      "epoch": 0.06542732768622798,
      "grad_norm": 4.667779922485352,
      "learning_rate": 3.313403089187843e-05,
      "loss": 0.105,
      "step": 1875
    },
    {
      "epoch": 0.06546222226099398,
      "grad_norm": 0.8794269561767578,
      "learning_rate": 3.288490284005979e-05,
      "loss": 0.0232,
      "step": 1876
    },
    {
      "epoch": 0.06549711683575996,
      "grad_norm": 0.28933271765708923,
      "learning_rate": 3.263577478824116e-05,
      "loss": 0.0045,
      "step": 1877
    },
    {
      "epoch": 0.06553201141052595,
      "grad_norm": 3.178183078765869,
      "learning_rate": 3.238664673642252e-05,
      "loss": 0.0728,
      "step": 1878
    },
    {
      "epoch": 0.06556690598529194,
      "grad_norm": 0.6755455732345581,
      "learning_rate": 3.213751868460389e-05,
      "loss": 0.0124,
      "step": 1879
    },
    {
      "epoch": 0.06560180056005792,
      "grad_norm": 11.867128372192383,
      "learning_rate": 3.188839063278525e-05,
      "loss": 0.1345,
      "step": 1880
    },
    {
      "epoch": 0.06563669513482391,
      "grad_norm": 2.3380939960479736,
      "learning_rate": 3.163926258096662e-05,
      "loss": 0.1228,
      "step": 1881
    },
    {
      "epoch": 0.0656715897095899,
      "grad_norm": 6.64097785949707,
      "learning_rate": 3.139013452914798e-05,
      "loss": 0.3374,
      "step": 1882
    },
    {
      "epoch": 0.0657064842843559,
      "grad_norm": 1.8740988969802856,
      "learning_rate": 3.1141006477329346e-05,
      "loss": 0.0298,
      "step": 1883
    },
    {
      "epoch": 0.06574137885912187,
      "grad_norm": 0.7063553333282471,
      "learning_rate": 3.089187842551071e-05,
      "loss": 0.0267,
      "step": 1884
    },
    {
      "epoch": 0.06577627343388787,
      "grad_norm": 4.608401775360107,
      "learning_rate": 3.064275037369208e-05,
      "loss": 0.254,
      "step": 1885
    },
    {
      "epoch": 0.06581116800865386,
      "grad_norm": 4.85666036605835,
      "learning_rate": 3.0393622321873445e-05,
      "loss": 0.2073,
      "step": 1886
    },
    {
      "epoch": 0.06584606258341984,
      "grad_norm": 1.1455944776535034,
      "learning_rate": 3.014449427005481e-05,
      "loss": 0.0358,
      "step": 1887
    },
    {
      "epoch": 0.06588095715818583,
      "grad_norm": 4.880748271942139,
      "learning_rate": 2.989536621823617e-05,
      "loss": 0.051,
      "step": 1888
    },
    {
      "epoch": 0.06591585173295182,
      "grad_norm": 0.5597882866859436,
      "learning_rate": 2.9646238166417537e-05,
      "loss": 0.0131,
      "step": 1889
    },
    {
      "epoch": 0.06595074630771781,
      "grad_norm": 1.6801714897155762,
      "learning_rate": 2.9397110114598903e-05,
      "loss": 0.1296,
      "step": 1890
    },
    {
      "epoch": 0.06598564088248379,
      "grad_norm": 3.171036958694458,
      "learning_rate": 2.914798206278027e-05,
      "loss": 0.0905,
      "step": 1891
    },
    {
      "epoch": 0.06602053545724978,
      "grad_norm": 5.514359474182129,
      "learning_rate": 2.8898854010961636e-05,
      "loss": 0.1675,
      "step": 1892
    },
    {
      "epoch": 0.06605543003201578,
      "grad_norm": 3.297043561935425,
      "learning_rate": 2.8649725959143e-05,
      "loss": 0.0503,
      "step": 1893
    },
    {
      "epoch": 0.06609032460678176,
      "grad_norm": 0.4994792640209198,
      "learning_rate": 2.8400597907324365e-05,
      "loss": 0.0106,
      "step": 1894
    },
    {
      "epoch": 0.06612521918154775,
      "grad_norm": 4.9639763832092285,
      "learning_rate": 2.815146985550573e-05,
      "loss": 0.4841,
      "step": 1895
    },
    {
      "epoch": 0.06616011375631374,
      "grad_norm": 4.902300834655762,
      "learning_rate": 2.7902341803687098e-05,
      "loss": 0.1267,
      "step": 1896
    },
    {
      "epoch": 0.06619500833107972,
      "grad_norm": 4.370704650878906,
      "learning_rate": 2.765321375186846e-05,
      "loss": 0.2072,
      "step": 1897
    },
    {
      "epoch": 0.06622990290584571,
      "grad_norm": 2.9274020195007324,
      "learning_rate": 2.7404085700049827e-05,
      "loss": 0.0833,
      "step": 1898
    },
    {
      "epoch": 0.0662647974806117,
      "grad_norm": 17.500648498535156,
      "learning_rate": 2.7154957648231193e-05,
      "loss": 0.2047,
      "step": 1899
    },
    {
      "epoch": 0.0662996920553777,
      "grad_norm": 1.1346436738967896,
      "learning_rate": 2.690582959641256e-05,
      "loss": 0.0933,
      "step": 1900
    },
    {
      "epoch": 0.06633458663014367,
      "grad_norm": 1.7068393230438232,
      "learning_rate": 2.665670154459392e-05,
      "loss": 0.0301,
      "step": 1901
    },
    {
      "epoch": 0.06636948120490967,
      "grad_norm": 3.910886526107788,
      "learning_rate": 2.6407573492775285e-05,
      "loss": 0.0502,
      "step": 1902
    },
    {
      "epoch": 0.06640437577967566,
      "grad_norm": 2.2727646827697754,
      "learning_rate": 2.615844544095665e-05,
      "loss": 0.0501,
      "step": 1903
    },
    {
      "epoch": 0.06643927035444164,
      "grad_norm": 1.9656223058700562,
      "learning_rate": 2.5909317389138018e-05,
      "loss": 0.0494,
      "step": 1904
    },
    {
      "epoch": 0.06647416492920763,
      "grad_norm": 1.6859112977981567,
      "learning_rate": 2.566018933731938e-05,
      "loss": 0.0399,
      "step": 1905
    },
    {
      "epoch": 0.06650905950397362,
      "grad_norm": 1.7721439599990845,
      "learning_rate": 2.5411061285500747e-05,
      "loss": 0.0486,
      "step": 1906
    },
    {
      "epoch": 0.06654395407873961,
      "grad_norm": 0.7496963143348694,
      "learning_rate": 2.5161933233682113e-05,
      "loss": 0.0197,
      "step": 1907
    },
    {
      "epoch": 0.06657884865350559,
      "grad_norm": 1.1479734182357788,
      "learning_rate": 2.491280518186348e-05,
      "loss": 0.0332,
      "step": 1908
    },
    {
      "epoch": 0.06661374322827159,
      "grad_norm": 2.254333972930908,
      "learning_rate": 2.4663677130044842e-05,
      "loss": 0.0885,
      "step": 1909
    },
    {
      "epoch": 0.06664863780303758,
      "grad_norm": 4.902960777282715,
      "learning_rate": 2.441454907822621e-05,
      "loss": 0.1102,
      "step": 1910
    },
    {
      "epoch": 0.06668353237780356,
      "grad_norm": 6.013204097747803,
      "learning_rate": 2.4165421026407575e-05,
      "loss": 0.1546,
      "step": 1911
    },
    {
      "epoch": 0.06671842695256955,
      "grad_norm": 3.17154860496521,
      "learning_rate": 2.391629297458894e-05,
      "loss": 0.061,
      "step": 1912
    },
    {
      "epoch": 0.06675332152733554,
      "grad_norm": 3.4154248237609863,
      "learning_rate": 2.3667164922770308e-05,
      "loss": 0.1385,
      "step": 1913
    },
    {
      "epoch": 0.06678821610210152,
      "grad_norm": 2.858323574066162,
      "learning_rate": 2.3418036870951667e-05,
      "loss": 0.0731,
      "step": 1914
    },
    {
      "epoch": 0.06682311067686751,
      "grad_norm": 0.8471932411193848,
      "learning_rate": 2.3168908819133033e-05,
      "loss": 0.0235,
      "step": 1915
    },
    {
      "epoch": 0.0668580052516335,
      "grad_norm": 5.515161037445068,
      "learning_rate": 2.29197807673144e-05,
      "loss": 0.0745,
      "step": 1916
    },
    {
      "epoch": 0.0668928998263995,
      "grad_norm": 1.435409665107727,
      "learning_rate": 2.2670652715495766e-05,
      "loss": 0.0493,
      "step": 1917
    },
    {
      "epoch": 0.06692779440116547,
      "grad_norm": 0.9459073543548584,
      "learning_rate": 2.242152466367713e-05,
      "loss": 0.0301,
      "step": 1918
    },
    {
      "epoch": 0.06696268897593147,
      "grad_norm": 3.27052903175354,
      "learning_rate": 2.2172396611858495e-05,
      "loss": 0.0949,
      "step": 1919
    },
    {
      "epoch": 0.06699758355069746,
      "grad_norm": 2.931394338607788,
      "learning_rate": 2.192326856003986e-05,
      "loss": 0.0971,
      "step": 1920
    },
    {
      "epoch": 0.06703247812546344,
      "grad_norm": 2.8816535472869873,
      "learning_rate": 2.1674140508221228e-05,
      "loss": 0.1203,
      "step": 1921
    },
    {
      "epoch": 0.06706737270022943,
      "grad_norm": 3.4738898277282715,
      "learning_rate": 2.142501245640259e-05,
      "loss": 0.1159,
      "step": 1922
    },
    {
      "epoch": 0.06710226727499542,
      "grad_norm": 2.4596896171569824,
      "learning_rate": 2.1175884404583957e-05,
      "loss": 0.1091,
      "step": 1923
    },
    {
      "epoch": 0.06713716184976141,
      "grad_norm": 6.4530229568481445,
      "learning_rate": 2.0926756352765323e-05,
      "loss": 0.1732,
      "step": 1924
    },
    {
      "epoch": 0.0671720564245274,
      "grad_norm": 0.660881757736206,
      "learning_rate": 2.067762830094669e-05,
      "loss": 0.0131,
      "step": 1925
    },
    {
      "epoch": 0.06720695099929339,
      "grad_norm": 5.867231845855713,
      "learning_rate": 2.042850024912805e-05,
      "loss": 0.066,
      "step": 1926
    },
    {
      "epoch": 0.06724184557405938,
      "grad_norm": 2.620327949523926,
      "learning_rate": 2.0179372197309415e-05,
      "loss": 0.1329,
      "step": 1927
    },
    {
      "epoch": 0.06727674014882536,
      "grad_norm": 5.688217639923096,
      "learning_rate": 1.993024414549078e-05,
      "loss": 0.1182,
      "step": 1928
    },
    {
      "epoch": 0.06731163472359135,
      "grad_norm": 0.5955426096916199,
      "learning_rate": 1.9681116093672148e-05,
      "loss": 0.0138,
      "step": 1929
    },
    {
      "epoch": 0.06734652929835734,
      "grad_norm": 1.4542877674102783,
      "learning_rate": 1.943198804185351e-05,
      "loss": 0.0283,
      "step": 1930
    },
    {
      "epoch": 0.06738142387312332,
      "grad_norm": 2.937093734741211,
      "learning_rate": 1.9182859990034877e-05,
      "loss": 0.0696,
      "step": 1931
    },
    {
      "epoch": 0.06741631844788931,
      "grad_norm": 3.8939666748046875,
      "learning_rate": 1.8933731938216243e-05,
      "loss": 0.1465,
      "step": 1932
    },
    {
      "epoch": 0.0674512130226553,
      "grad_norm": 3.848921775817871,
      "learning_rate": 1.868460388639761e-05,
      "loss": 0.0546,
      "step": 1933
    },
    {
      "epoch": 0.0674861075974213,
      "grad_norm": 4.376051425933838,
      "learning_rate": 1.8435475834578976e-05,
      "loss": 0.1945,
      "step": 1934
    },
    {
      "epoch": 0.06752100217218727,
      "grad_norm": 6.011338233947754,
      "learning_rate": 1.818634778276034e-05,
      "loss": 0.1271,
      "step": 1935
    },
    {
      "epoch": 0.06755589674695327,
      "grad_norm": 4.933646202087402,
      "learning_rate": 1.7937219730941705e-05,
      "loss": 0.1061,
      "step": 1936
    },
    {
      "epoch": 0.06759079132171926,
      "grad_norm": 7.217484474182129,
      "learning_rate": 1.768809167912307e-05,
      "loss": 0.1674,
      "step": 1937
    },
    {
      "epoch": 0.06762568589648524,
      "grad_norm": 4.005887031555176,
      "learning_rate": 1.7438963627304438e-05,
      "loss": 0.1991,
      "step": 1938
    },
    {
      "epoch": 0.06766058047125123,
      "grad_norm": 7.560785293579102,
      "learning_rate": 1.7189835575485797e-05,
      "loss": 0.2289,
      "step": 1939
    },
    {
      "epoch": 0.06769547504601722,
      "grad_norm": 4.276863098144531,
      "learning_rate": 1.6940707523667164e-05,
      "loss": 0.1116,
      "step": 1940
    },
    {
      "epoch": 0.06773036962078322,
      "grad_norm": 1.8486652374267578,
      "learning_rate": 1.669157947184853e-05,
      "loss": 0.065,
      "step": 1941
    },
    {
      "epoch": 0.0677652641955492,
      "grad_norm": 1.660916805267334,
      "learning_rate": 1.6442451420029896e-05,
      "loss": 0.0379,
      "step": 1942
    },
    {
      "epoch": 0.06780015877031519,
      "grad_norm": 2.230806827545166,
      "learning_rate": 1.619332336821126e-05,
      "loss": 0.0296,
      "step": 1943
    },
    {
      "epoch": 0.06783505334508118,
      "grad_norm": 7.616592884063721,
      "learning_rate": 1.5944195316392625e-05,
      "loss": 0.2083,
      "step": 1944
    },
    {
      "epoch": 0.06786994791984716,
      "grad_norm": 3.9216043949127197,
      "learning_rate": 1.569506726457399e-05,
      "loss": 0.1798,
      "step": 1945
    },
    {
      "epoch": 0.06790484249461315,
      "grad_norm": 3.41851806640625,
      "learning_rate": 1.5445939212755354e-05,
      "loss": 0.1076,
      "step": 1946
    },
    {
      "epoch": 0.06793973706937914,
      "grad_norm": 1.9859263896942139,
      "learning_rate": 1.5196811160936722e-05,
      "loss": 0.0648,
      "step": 1947
    },
    {
      "epoch": 0.06797463164414512,
      "grad_norm": 2.6452901363372803,
      "learning_rate": 1.4947683109118085e-05,
      "loss": 0.0723,
      "step": 1948
    },
    {
      "epoch": 0.06800952621891111,
      "grad_norm": 2.183600664138794,
      "learning_rate": 1.4698555057299452e-05,
      "loss": 0.0324,
      "step": 1949
    },
    {
      "epoch": 0.0680444207936771,
      "grad_norm": 3.6942546367645264,
      "learning_rate": 1.4449427005480818e-05,
      "loss": 0.1281,
      "step": 1950
    },
    {
      "epoch": 0.0680793153684431,
      "grad_norm": 0.8612968921661377,
      "learning_rate": 1.4200298953662183e-05,
      "loss": 0.0319,
      "step": 1951
    },
    {
      "epoch": 0.06811420994320908,
      "grad_norm": 2.120898485183716,
      "learning_rate": 1.3951170901843549e-05,
      "loss": 0.0643,
      "step": 1952
    },
    {
      "epoch": 0.06814910451797507,
      "grad_norm": 2.2251405715942383,
      "learning_rate": 1.3702042850024913e-05,
      "loss": 0.1467,
      "step": 1953
    },
    {
      "epoch": 0.06818399909274106,
      "grad_norm": 1.6454814672470093,
      "learning_rate": 1.345291479820628e-05,
      "loss": 0.0574,
      "step": 1954
    },
    {
      "epoch": 0.06821889366750704,
      "grad_norm": 6.477297782897949,
      "learning_rate": 1.3203786746387643e-05,
      "loss": 0.1669,
      "step": 1955
    },
    {
      "epoch": 0.06825378824227303,
      "grad_norm": 3.169419527053833,
      "learning_rate": 1.2954658694569009e-05,
      "loss": 0.1476,
      "step": 1956
    },
    {
      "epoch": 0.06828868281703902,
      "grad_norm": 0.2506086230278015,
      "learning_rate": 1.2705530642750373e-05,
      "loss": 0.0077,
      "step": 1957
    },
    {
      "epoch": 0.06832357739180502,
      "grad_norm": 1.698081135749817,
      "learning_rate": 1.245640259093174e-05,
      "loss": 0.0539,
      "step": 1958
    },
    {
      "epoch": 0.068358471966571,
      "grad_norm": 4.649796485900879,
      "learning_rate": 1.2207274539113104e-05,
      "loss": 0.0794,
      "step": 1959
    },
    {
      "epoch": 0.06839336654133699,
      "grad_norm": 1.1804927587509155,
      "learning_rate": 1.195814648729447e-05,
      "loss": 0.0249,
      "step": 1960
    },
    {
      "epoch": 0.06842826111610298,
      "grad_norm": 3.2141199111938477,
      "learning_rate": 1.1709018435475834e-05,
      "loss": 0.0919,
      "step": 1961
    },
    {
      "epoch": 0.06846315569086896,
      "grad_norm": 2.9155118465423584,
      "learning_rate": 1.14598903836572e-05,
      "loss": 0.0441,
      "step": 1962
    },
    {
      "epoch": 0.06849805026563495,
      "grad_norm": 2.603426218032837,
      "learning_rate": 1.1210762331838564e-05,
      "loss": 0.0474,
      "step": 1963
    },
    {
      "epoch": 0.06853294484040094,
      "grad_norm": 4.818983554840088,
      "learning_rate": 1.096163428001993e-05,
      "loss": 0.1112,
      "step": 1964
    },
    {
      "epoch": 0.06856783941516692,
      "grad_norm": 6.712808609008789,
      "learning_rate": 1.0712506228201295e-05,
      "loss": 0.137,
      "step": 1965
    },
    {
      "epoch": 0.06860273398993291,
      "grad_norm": 4.736404895782471,
      "learning_rate": 1.0463378176382662e-05,
      "loss": 0.0997,
      "step": 1966
    },
    {
      "epoch": 0.0686376285646989,
      "grad_norm": 5.348162651062012,
      "learning_rate": 1.0214250124564025e-05,
      "loss": 0.1614,
      "step": 1967
    },
    {
      "epoch": 0.0686725231394649,
      "grad_norm": 5.155204772949219,
      "learning_rate": 9.96512207274539e-06,
      "loss": 0.0758,
      "step": 1968
    },
    {
      "epoch": 0.06870741771423088,
      "grad_norm": 2.2809762954711914,
      "learning_rate": 9.715994020926755e-06,
      "loss": 0.0613,
      "step": 1969
    },
    {
      "epoch": 0.06874231228899687,
      "grad_norm": 1.9914283752441406,
      "learning_rate": 9.466865969108122e-06,
      "loss": 0.087,
      "step": 1970
    },
    {
      "epoch": 0.06877720686376286,
      "grad_norm": 3.757549285888672,
      "learning_rate": 9.217737917289488e-06,
      "loss": 0.0853,
      "step": 1971
    },
    {
      "epoch": 0.06881210143852884,
      "grad_norm": 2.435265302658081,
      "learning_rate": 8.968609865470853e-06,
      "loss": 0.1529,
      "step": 1972
    },
    {
      "epoch": 0.06884699601329483,
      "grad_norm": 1.6236376762390137,
      "learning_rate": 8.719481813652219e-06,
      "loss": 0.0351,
      "step": 1973
    },
    {
      "epoch": 0.06888189058806082,
      "grad_norm": 3.9350476264953613,
      "learning_rate": 8.470353761833582e-06,
      "loss": 0.1247,
      "step": 1974
    },
    {
      "epoch": 0.06891678516282682,
      "grad_norm": 1.3771547079086304,
      "learning_rate": 8.221225710014948e-06,
      "loss": 0.0348,
      "step": 1975
    },
    {
      "epoch": 0.0689516797375928,
      "grad_norm": 6.639103412628174,
      "learning_rate": 7.972097658196313e-06,
      "loss": 0.1901,
      "step": 1976
    },
    {
      "epoch": 0.06898657431235879,
      "grad_norm": 3.819648265838623,
      "learning_rate": 7.722969606377677e-06,
      "loss": 0.2108,
      "step": 1977
    },
    {
      "epoch": 0.06902146888712478,
      "grad_norm": 3.928830146789551,
      "learning_rate": 7.473841554559043e-06,
      "loss": 0.1004,
      "step": 1978
    },
    {
      "epoch": 0.06905636346189076,
      "grad_norm": 3.906219959259033,
      "learning_rate": 7.224713502740409e-06,
      "loss": 0.1798,
      "step": 1979
    },
    {
      "epoch": 0.06909125803665675,
      "grad_norm": 3.584683656692505,
      "learning_rate": 6.975585450921774e-06,
      "loss": 0.1187,
      "step": 1980
    },
    {
      "epoch": 0.06912615261142274,
      "grad_norm": 0.5437661409378052,
      "learning_rate": 6.72645739910314e-06,
      "loss": 0.0258,
      "step": 1981
    },
    {
      "epoch": 0.06916104718618872,
      "grad_norm": 3.433171510696411,
      "learning_rate": 6.4773293472845045e-06,
      "loss": 0.0444,
      "step": 1982
    },
    {
      "epoch": 0.06919594176095471,
      "grad_norm": 4.863194465637207,
      "learning_rate": 6.22820129546587e-06,
      "loss": 0.0932,
      "step": 1983
    },
    {
      "epoch": 0.0692308363357207,
      "grad_norm": 1.8948540687561035,
      "learning_rate": 5.979073243647235e-06,
      "loss": 0.0629,
      "step": 1984
    },
    {
      "epoch": 0.0692657309104867,
      "grad_norm": 0.7390990257263184,
      "learning_rate": 5.7299451918286e-06,
      "loss": 0.0159,
      "step": 1985
    },
    {
      "epoch": 0.06930062548525268,
      "grad_norm": 3.1765265464782715,
      "learning_rate": 5.480817140009965e-06,
      "loss": 0.0591,
      "step": 1986
    },
    {
      "epoch": 0.06933552006001867,
      "grad_norm": 7.421543121337891,
      "learning_rate": 5.231689088191331e-06,
      "loss": 0.0435,
      "step": 1987
    },
    {
      "epoch": 0.06937041463478466,
      "grad_norm": 2.256025552749634,
      "learning_rate": 4.982561036372695e-06,
      "loss": 0.0306,
      "step": 1988
    },
    {
      "epoch": 0.06940530920955064,
      "grad_norm": 3.6455605030059814,
      "learning_rate": 4.733432984554061e-06,
      "loss": 0.1455,
      "step": 1989
    },
    {
      "epoch": 0.06944020378431663,
      "grad_norm": 3.3696959018707275,
      "learning_rate": 4.484304932735426e-06,
      "loss": 0.1752,
      "step": 1990
    },
    {
      "epoch": 0.06947509835908262,
      "grad_norm": 6.925027370452881,
      "learning_rate": 4.235176880916791e-06,
      "loss": 0.056,
      "step": 1991
    },
    {
      "epoch": 0.06950999293384862,
      "grad_norm": 12.857019424438477,
      "learning_rate": 3.986048829098156e-06,
      "loss": 0.2343,
      "step": 1992
    },
    {
      "epoch": 0.0695448875086146,
      "grad_norm": 0.47513893246650696,
      "learning_rate": 3.7369207772795213e-06,
      "loss": 0.0118,
      "step": 1993
    },
    {
      "epoch": 0.06957978208338059,
      "grad_norm": 7.610459327697754,
      "learning_rate": 3.487792725460887e-06,
      "loss": 0.357,
      "step": 1994
    },
    {
      "epoch": 0.06961467665814658,
      "grad_norm": 18.732032775878906,
      "learning_rate": 3.2386646736422522e-06,
      "loss": 0.3064,
      "step": 1995
    },
    {
      "epoch": 0.06964957123291256,
      "grad_norm": 5.690289497375488,
      "learning_rate": 2.9895366218236177e-06,
      "loss": 0.2097,
      "step": 1996
    },
    {
      "epoch": 0.06968446580767855,
      "grad_norm": 2.4391493797302246,
      "learning_rate": 2.7404085700049827e-06,
      "loss": 0.0596,
      "step": 1997
    },
    {
      "epoch": 0.06971936038244454,
      "grad_norm": 0.6748520731925964,
      "learning_rate": 2.4912805181863477e-06,
      "loss": 0.0207,
      "step": 1998
    },
    {
      "epoch": 0.06975425495721052,
      "grad_norm": 1.706207275390625,
      "learning_rate": 2.242152466367713e-06,
      "loss": 0.0554,
      "step": 1999
    },
    {
      "epoch": 0.06978914953197651,
      "grad_norm": 0.8973762392997742,
      "learning_rate": 1.993024414549078e-06,
      "loss": 0.0306,
      "step": 2000
    },
    {
      "epoch": 0.0698240441067425,
      "grad_norm": 1.1135072708129883,
      "learning_rate": 1.7438963627304436e-06,
      "loss": 0.0507,
      "step": 2001
    },
    {
      "epoch": 0.0698589386815085,
      "grad_norm": 0.786320686340332,
      "learning_rate": 1.4947683109118088e-06,
      "loss": 0.0307,
      "step": 2002
    },
    {
      "epoch": 0.06989383325627448,
      "grad_norm": 1.3095561265945435,
      "learning_rate": 1.2456402590931739e-06,
      "loss": 0.0183,
      "step": 2003
    },
    {
      "epoch": 0.06992872783104047,
      "grad_norm": 3.2395145893096924,
      "learning_rate": 9.96512207274539e-07,
      "loss": 0.1853,
      "step": 2004
    },
    {
      "epoch": 0.06996362240580646,
      "grad_norm": 2.2476885318756104,
      "learning_rate": 7.473841554559044e-07,
      "loss": 0.1101,
      "step": 2005
    },
    {
      "epoch": 0.06999851698057244,
      "grad_norm": 4.654491424560547,
      "learning_rate": 4.982561036372695e-07,
      "loss": 0.0975,
      "step": 2006
    },
    {
      "epoch": 0.07003341155533843,
      "grad_norm": 1.231461763381958,
      "learning_rate": 2.4912805181863477e-07,
      "loss": 0.0329,
      "step": 2007
    }
  ],
  "logging_steps": 1,
  "max_steps": 2007,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4003556236429824.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
